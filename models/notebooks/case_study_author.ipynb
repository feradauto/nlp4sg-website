{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49052ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7891bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': '1631a6e3b5235a4a2b0d71dca22a9614acca2ce3',\n",
       "  'title': 'Data-to-text Generation with Macro Planning',\n",
       "  'abstract': 'Abstract Recent approaches to data-to-text generation have adopted the very successful encoder-decoder architecture or variants thereof. These models generate text that is fluent (but often imprecise) and perform quite poorly at selecting appropriate content and ordering it coherently. To overcome some of these issues, we propose a neural model with a macro planning stage followed by a generation stage reminiscent of traditional methods which embrace separate modules for planning and surface realization. Macro plans represent high level organization of important content such as entities, events, and their interactions; they are learned from data and given as input to the generator. Extensive experiments on two data-to-text benchmarks (RotoWire and MLB) show that our approach outperforms competitive baselines in terms of automatic and human evaluation.',\n",
       "  'year': 2021},\n",
       " {'paperId': '19bd467b1c8de94b9bdaef1499788467937f594e',\n",
       "  'title': 'Meta-Learning for Domain Generalization in Semantic Parsing',\n",
       "  'abstract': 'The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing out-of-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard supervised learning. In this work, we use a meta-learning framework which targets zero-shot domain generalization for semantic parsing. We apply a model-agnostic training algorithm that simulates zero-shot parsing by constructing virtual train and test sets from disjoint domains. The learning objective capitalizes on the intuition that gradient steps that improve source-domain performance should also improve target-domain performance, thus encouraging a parser to generalize to unseen target domains. Experimental results on the (English) Spider and Chinese Spider datasets show that the meta-learning objective significantly boosts the performance of a baseline parser.',\n",
       "  'year': 2021},\n",
       " {'paperId': '5a02e44c9057bab706f62c970806cbe611808746',\n",
       "  'title': 'Universal Discourse Representation Structure Parsing',\n",
       "  'abstract': 'Abstract We consider the task of crosslingual semantic parsing in the style of Discourse Representation Theory (DRT) where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide learning in other languages. We introduce ùïåniversal Discourse Representation Theory (ùïåDRT), a variant of DRT that explicitly anchors semantic representations to tokens in the linguistic input. We develop a semantic parsing framework based on the Transformer architecture and utilize it to obtain semantic resources in multiple languages following two learning schemes. The many-to-one approach translates non-English text to English, and then runs a relatively accurate English parser on the translated text, while the one-to-many approach translates gold standard English to non-English text and trains multiple parsers (one per language) on the translations. Experimental results on the Parallel Meaning Bank show that our proposal outperforms strong baselines by a wide margin and can be used to construct (silver-standard) meaning banks for 99 languages.',\n",
       "  'year': 2021},\n",
       " {'paperId': '759521f22de9deb4760ff70440c55dcaf467df46',\n",
       "  'title': 'Informative and Controllable Opinion Summarization',\n",
       "  'abstract': 'Opinion summarization is the task of automatically generating summaries for a set of reviews about a specific target (e.g., a movie or a product). Since the number of reviews for each target can be prohibitively large, neural network-based methods follow a two-stage approach where an extractive step first pre-selects a subset of salient opinions and an abstractive step creates the summary while conditioning on the extracted subset. However, the extractive model leads to loss of information which may be useful depending on user needs. In this paper we propose a summarization framework that eliminates the need to rely only on pre-selected content and waste possibly useful information, especially when customizing summaries. The framework enables the use of all input reviews by first condensing them into multiple dense vectors which serve as input to an abstractive model. We showcase an effective instantiation of our framework which produces more informative summaries and also allows to take user preferences into account using our zero-shot customization technique. Experimental results demonstrate that our model improves the state of the art on the Rotten Tomatoes dataset and generates customized summaries effectively.',\n",
       "  'year': 2021},\n",
       " {'paperId': '834157c224b4bb3e90979d2d3d60b2fb24622657',\n",
       "  'title': 'Zero-Shot Cross-lingual Semantic Parsing',\n",
       "  'abstract': 'Recent work in crosslingual semantic parsing has successfully applied machine translation to localize accurate parsing to new languages. However, these advances assume access to high-quality machine translation systems, and tools such as word aligners, for all test languages. We remove these assumptions and study cross-lingual semantic parsing as a zeroshot problem without parallel data for 7 test languages (DE, ZH, FR, ES, PT, HI, TR). We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-Logical form paired data and unlabeled, monolingual utterances in each test language. We train an encoder to generate language-agnostic representations jointly optimized for generating logical forms or utterance reconstruction and against language discriminability. Our system frames zero-shot parsing as a latent-space alignment problem and finds that pre-trained models can be improved to generate logical forms with minimal cross-lingual transfer penalty. Experimental results on Overnight and a new executable version of MultiATIS++ find that our zero-shot approach performs above backtranslation baselines and, in some cases, approaches the supervised upper bound.',\n",
       "  'year': 2021},\n",
       " {'paperId': '92d879ed4be5438c2498d5269e356959da2030bd',\n",
       "  'title': 'Noisy Self-Knowledge Distillation for Text Summarization',\n",
       "  'abstract': 'In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximum-likelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results.',\n",
       "  'year': 2021},\n",
       " {'paperId': 'f825431db6e83c1ba3f485f2030682755325ccef',\n",
       "  'title': 'Learning from Executions for Semantic Parsing',\n",
       "  'abstract': 'Semantic parsing aims at translating natural language (NL) utterances onto machine-interpretable programs, which can be executed against a real-world environment. The expensive annotation of utterance-program pairs has long been acknowledged as a major bottleneck for the deployment of contemporary neural models to real-life applications. In this work, we focus on the task of semi-supervised learning where a limited amount of annotated data is available together with many unlabeled NL utterances. Based on the observation that programs which correspond to NL utterances should always be executable, we propose to encourage a parser to generate executable programs for unlabeled utterances. Due to the large search space of executable programs, conventional methods that use beam-search for approximation, such as self-training and top-k marginal likelihood training, do not perform as well. Instead, we propose a set of new training objectives that are derived by approaching the problem of learning from executions from the posterior regularization perspective. Our new objectives outperform conventional methods on Overnight and GeoQuery, bridging the gap between semi-supervised and supervised learning.',\n",
       "  'year': 2021},\n",
       " {'paperId': 'fb50f6f4f81f361bf1c6dbc93cc8fab5aee12fdf',\n",
       "  'title': 'Extractive Opinion Summarization in Quantized Transformer Spaces',\n",
       "  'abstract': 'Abstract We present the Quantized Transformer (QT), an unsupervised system for extractive opinion summarization. QT is inspired by Vector- Quantized Variational Autoencoders, which we repurpose for popularity-driven summarization. It uses a clustering interpretation of the quantized space and a novel extraction algorithm to discover popular opinions among hundreds of reviews, a significant step towards opinion summarization of practical scope. In addition, QT enables controllable summarization without further training, by utilizing properties of the quantized space to extract aspect-specific summaries. We also make publicly available Space, a large-scale evaluation benchmark for opinion summarizers, comprising general and aspect-specific summaries for 50 hotels. Experiments demonstrate the promise of our approach, which is validated by human studies where judges showed clear preference for our method over competitive baselines.',\n",
       "  'year': 2021},\n",
       " {'paperId': '979a4428bde364415d21c6231609eb4564df84a8',\n",
       "  'title': 'Screenplay Summarization Using Latent Narrative Structure',\n",
       "  'abstract': 'Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.',\n",
       "  'year': 2020},\n",
       " {'paperId': 'a04cae82222715d5997240cac51d011e03116c04',\n",
       "  'title': 'Alignment-free Cross-lingual Semantic Role Labeling',\n",
       "  'abstract': 'Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language. Recent approaches rely on word alignments, machine translation engines, or preprocessing tools such as parsers or taggers. We propose a cross-lingual SRL model which only requires annotations in a source language and access to raw text in the form of a parallel corpus. The backbone of our model is an LSTM-based semantic role labeler jointly trained with a semantic role compressor and multilingual word embeddings. The compressor collects useful information from the output of the semantic role labeler, filtering noisy and conflicting evidence. It lives in a multilingual embedding space and provides direct supervision for predicting semantic roles in the target language. Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.',\n",
       "  'year': 2020},\n",
       " {'paperId': 'b71c4aa7ad1b32412d8fe0952f1d7be2bf2ccf44',\n",
       "  'title': 'Zero-Shot Crosslingual Sentence Simplification',\n",
       "  'abstract': 'Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English. We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks. A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification). Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.',\n",
       "  'year': 2020},\n",
       " {'paperId': 'bd2a84a08f032179b60deb286543004c89e652fb',\n",
       "  'title': 'Bootstrapping a Crosslingual Semantic Parser',\n",
       "  'abstract': 'Recent progress in semantic parsing scarcely considers languages other than English but professional translation can be prohibitively expensive. We adapt a semantic parser trained on a single language, such as English, to new languages and multiple domains with minimal annotation. We query if machine translation is an adequate substitute for training data, and extend this to investigate bootstrapping using joint training with English, paraphrasing, and multilingual pre-trained models. We develop a Transformer-based parser combining paraphrases by ensembling attention over multiple encoders and present new versions of ATIS and Overnight in German and Chinese for evaluation. Experimental results indicate that MT can approximate training data in a new language for accurate parsing when augmented with paraphrasing through multiple MT engines. Considering when MT is inadequate, we also find that using our approach achieves parsing accuracy within 2% of complete translation using only 50% of training data.',\n",
       "  'year': 2020},\n",
       " {'paperId': 'cbdb0d682a59933fb144124f1bfaec0ee3f3b04c',\n",
       "  'title': 'Unsupervised Opinion Summarization with Noising and Denoising',\n",
       "  'abstract': 'The supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. In this paper we enable the use of supervised learning for the setting where there are only documents available (e.g., product or business reviews) without ground truth summaries. We create a synthetic dataset from a corpus of user reviews by sampling a review, pretending it is a summary, and generating noisy versions thereof which we treat as pseudo-review input. We introduce several linguistically motivated noise generation functions and a summarization model which learns to denoise the input and generate the original review. At test time, the model accepts genuine reviews and generates a summary containing salient opinions, treating those that do not reach consensus as noise. Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines.',\n",
       "  'year': 2020},\n",
       " {'paperId': 'cc0fc60e9b9d036acf53899c259574c8ce2aedfc',\n",
       "  'title': 'Compositional Generalization via Semantic Tagging',\n",
       "  'abstract': 'Although neural sequence-to-sequence models have been successfully applied to semantic parsing, they struggle to perform well on query-based data splits that require \\\\emph{composition generalization}, an ability of systematically generalizing to unseen composition of seen components. Motivated by the explicitly built-in compositionality in traditional statistical semantic parsing, we propose a new decoding framework that preserves the expressivity and generality of sequence-to-sequence models while featuring explicit lexicon-style alignments and disentangled information processing. Specifically, we decompose decoding into two phases where an input utterance is first tagged with semantic symbols representing the meanings of its individual words, and then a sequence-to-sequence model is used to predict the final meaning representation conditioning on the utterance and the predicted tag sequence. Experimental results on three semantic parsing datasets with query-based splits show that the proposed approach consistently improves compositional generalization of sequence-to-sequence models across different model architectures, domains and semantic formalisms.',\n",
       "  'year': 2020},\n",
       " {'paperId': 'cc3dc0fdb55a9556afa679f58adf0169aea4b2b1',\n",
       "  'title': 'Abstractive Query Focused Summarization with Query-Free Resources',\n",
       "  'abstract': 'ive Query Focused Summarization with Query-Free Resources Yumo Xu and Mirella Lapata Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yumo.xu@ed.ac.uk, mlap@inf.ed.ac.uk',\n",
       "  'year': 2020},\n",
       " {'paperId': 'cf1f69b38962f085a0628b46b44021dc86315fdc',\n",
       "  'title': 'Dscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing',\n",
       "  'abstract': 'Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length. Evaluating the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance. DRSs are typically visualized as boxes which are not straightforward to process automatically. Counter transforms DRSs to clauses and measures clause overlap by searching for variable mappings between two DRSs. However, this metric is computationally costly (with respect to memory and CPU time) and does not scale with longer texts. We introduce Dscorer, an efficient new metric which converts box-style DRSs to graphs and then measures the overlap of n-grams. Experiments show that Dscorer computes accuracy scores that are correlated with Counter at a fraction of the time.',\n",
       "  'year': 2020},\n",
       " {'paperId': 'da43b3d75529ce873049572fde5e8cbd74dbf436',\n",
       "  'title': 'Coarse-to-Fine Query Focused Multi-Document Summarization',\n",
       "  'abstract': 'We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries. We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant, likely to contain an answer, and central. The modules can be independently developed and leverage training data if available. We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary. Our framework is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets.',\n",
       "  'year': 2020},\n",
       " {'paperId': 'dec97a510bff3017aa03e1eb34d6c0a1745b1da4',\n",
       "  'title': 'Exploring Explainable Selection to Control Abstractive Generation',\n",
       "  'abstract': \"It is a big challenge to model long-range input for document summarization. In this paper, we target using a select and generate paradigm to enhance the capability of selecting explainable contents (i.e., interpret the selection given its semantics, novelty, relevance) and then guiding to control the abstract generation. Specifically, a newly designed pair-wise extractor is proposed to capture the sentence pair interactions and their centrality. Furthermore, the generator is hybrid with the selected content and is jointly integrated with a pointer distribution that is derived from a sentence deployment's attention. The abstract generation can be controlled by an explainable mask matrix that determines to what extent the content can be included in the summary. Encoders are adaptable with both Transformer-based and BERT-based configurations. Overall, both results based on ROUGE metrics and human evaluation gain outperformance over several state-of-the-art models on two benchmark CNN/DailyMail and NYT datasets.\",\n",
       "  'year': 2020},\n",
       " {'paperId': 'f01a79f03a4fa00d10dc6c8d72ddf50e2755c2e9',\n",
       "  'title': 'Few-Shot Learning for Abstractive Multi-Document Opinion Summarization',\n",
       "  'abstract': 'Opinion summarization is an automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to a high cost of summary production, datasets large enough for training supervised models are lacking. Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way. Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion. However, these models, not being exposed to the actual summaries, fail to capture their essential properties. In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation. We start by training a language model to generate a new product review given available reviews of the product. The model is aware of the properties: it proceeds with first generating property values and then producing a review conditioned on them. We do not use any summaries in this stage and the property values are derived from reviews with no manual effort. In the second stage, we fine-tune the module predicting the property values on a few available summaries. This lets us switch the generator to the summarization mode. Our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.',\n",
       "  'year': 2020},\n",
       " {'paperId': 'f0a852dddf5a548efe927afa5bacbbdbc55e874c',\n",
       "  'title': 'Query Focused Multi-Document Summarization with Distant Supervision',\n",
       "  'abstract': 'We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization (QFS). Due to the lack of training data, existing work relies heavily on retrieval-style methods for estimating the relevance between queries and text segments. In this work, we leverage distant supervision from question answering where various resources are available to more explicitly capture the relationship between queries and documents. We propose a coarse-to-fine modeling framework which introduces separate modules for estimating whether segments are relevant to the query, likely to contain an answer, and central. Under this framework, a trained evidence estimator further discerns which retrieved segments might answer the query for final selection in the summary. We demonstrate that our framework outperforms strong comparison systems on standard QFS benchmarks.',\n",
       "  'year': 2020},\n",
       " {'paperId': '051d868dcc32611133eaaac55cce82a6d4f08733',\n",
       "  'title': 'Single Document Summarization as Tree Induction',\n",
       "  'abstract': 'In this paper, we conceptualize single-document extractive summarization as a tree induction problem. In contrast to previous approaches which have relied on linguistically motivated document representations to generate summaries, our model induces a multi-root dependency tree while predicting the output summary. Each root node in the tree is a summary sentence, and the subtrees attached to it are sentences whose content relates to or explains the summary sentence. We design a new iterative refinement algorithm: it induces the trees through repeatedly refining the structures predicted by previous iterations. We demonstrate experimentally on two benchmark datasets that our summarizer performs competitively against state-of-the-art methods.',\n",
       "  'year': 2019},\n",
       " {'paperId': '0e9b6e650ab2232f1c9f8ede736ad46d5510bfb4',\n",
       "  'title': 'Learning Natural Language Interfaces with Neural Models',\n",
       "  'abstract': None,\n",
       "  'year': 2019},\n",
       " {'paperId': '185861590659e52cf585e685dc6baa5462194bd2',\n",
       "  'title': 'Unsupervised Multi-Document Opinion Summarization as Copycat-Review Generation',\n",
       "  'abstract': \"Summarization of opinions is the process of automatically creating text summaries that reflect subjective information expressed in input documents, such as product reviews. While most previous research in opinion summarization has focused on the extractive setting, i.e. selecting fragments of the input documents to produce a summary, we let the model generate novel sentences and hence produce fluent text. Supervised abstractive summarization methods typically rely on large quantities of document-summary pairs which are expensive to acquire. In contrast, we consider the unsupervised setting, in other words, we do not use any summaries in training. We define a generative model for a multi-product review collection. Intuitively, we want to design such a model that, when generating a new review given a set of other reviews of the product, we can control the `amount of novelty' going into the new review or, equivalently, vary the degree of deviation from the input reviews. At test time, when generating summaries, we force the novelty to be minimal, and produce a text reflecting consensus opinions. We capture this intuition by defining a hierarchical variational autoencoder model. Both individual reviews and products they correspond to are associated with stochastic latent codes, and the review generator ('decoder') has direct access to the text of input reviews through the pointer-generator mechanism. In experiments on Amazon and Yelp data, we show that in this model by setting at test time the review's latent code to its mean, we produce fluent and coherent summaries.\",\n",
       "  'year': 2019},\n",
       " {'paperId': '187e69900484453fda35d853cdc8c5a298ecbd24',\n",
       "  'title': 'Partners in Crime: Multi-view Sequential Inference for Movie Understanding',\n",
       "  'abstract': 'Multi-view learning algorithms are powerful representation learning tools, often exploited in the context of multimodal problems. However, for problems requiring inference at the token-level of a sequence (that is, a separate prediction must be made for every time step), it is often the case that single-view systems are used, or that more than one views are fused in a simple manner. We describe an incremental neural architecture paired with a novel training objective for incremental inference. The network operates on multi-view data. We demonstrate the effectiveness of our approach on the problem of predicting perpetrators in crime drama series, for which our model significantly outperforms previous work and strong baselines. Moreover, we introduce two tasks, crime case and speaker type tagging, that contribute to movie understanding and demonstrate the effectiveness of our model on them.1.',\n",
       "  'year': 2019},\n",
       " {'paperId': '1c0f70727e6ae4305e5e51ac191d4f09907aaf62',\n",
       "  'title': 'Discourse Representation Structure Parsing with Recurrent Neural Networks and the Transformer Model',\n",
       "  'abstract': 'We describe the systems we developed for Discourse Representation Structure (DRS) parsing as part of the IWCS-2019 Shared Task of DRS Parsing.1 Our systems are based on sequence-to-sequence modeling. To implement our model, we use the open-source neural machine translation system implemented in PyTorch, OpenNMT-py. We experimented with a variety of encoder-decoder models based on recurrent neural networks and the Transformer model. We conduct experiments on the standard benchmark of the Parallel Meaning Bank (PMB 2.2). Our best system achieves a score of 84.8% F1 in the DRS parsing shared task.',\n",
       "  'year': 2019},\n",
       " {'paperId': '250a8f218f05a295fe974a5fc80f9489aa6d58b6',\n",
       "  'title': 'Weakly Supervised Domain Detection',\n",
       "  'abstract': 'Abstract In this paper we introduce domain detection as a new natural language processing task. We argue that the ability to detect textual segments that are domain-heavy (i.e., sentences or phrases that are representative of and provide evidence for a given domain) could enhance the robustness and portability of various text classification applications. We propose an encoder-detector framework for domain detection and bootstrap classifiers with multiple instance learning. The model is hierarchically organized and suited to multilabel classification. We demonstrate that despite learning with minimal supervision, our model can be applied to text spans of different granularities, languages, and genres. We also showcase the potential of domain detection for text summarization.',\n",
       "  'year': 2019},\n",
       " {'paperId': '329dd1f97576416b60a19074c94726fc0faf8041',\n",
       "  'title': 'Movie Plot Analysis via Turning Point Identification',\n",
       "  'abstract': 'According to screenwriting theory, turning points (e.g., change of plans, major setback, climax) are crucial narrative moments within a screenplay: they define the plot structure, determine its progression and segment the screenplay into thematic units (e.g., setup, complications, aftermath). We propose the task of turning point identification in movies as a means of analyzing their narrative structure. We argue that turning points and the segmentation they provide can facilitate processing long, complex narratives, such as screenplays, for summarization and question answering. We introduce a dataset consisting of screenplays and plot synopses annotated with turning points and present an end-to-end neural network model that identifies turning points in plot synopses and projects them onto scenes in screenplays. Our model outperforms strong baselines based on state-of-the-art sentence representations and the expected position of turning points.',\n",
       "  'year': 2019},\n",
       " {'paperId': '3e2d30e8ae9ff584b5922310ba71bf277bcd8317',\n",
       "  'title': 'Generating Summaries with Topic Templates and Structured Convolutional Decoders',\n",
       "  'abstract': 'Existing neural generation approaches create multi-sentence text as a single sequence. In this paper we propose a structured convolutional decoder that is guided by the content structure of target summaries. We compare our model with existing sequential decoders on three data sets representing different domains. Automatic and human evaluation demonstrate that our summaries have better content coverage.',\n",
       "  'year': 2019},\n",
       " {'paperId': '416e3ffff2fe2d43343f6b721721a482829f882d',\n",
       "  'title': 'Data-to-text Generation with Entity Modeling',\n",
       "  'abstract': 'Recent approaches to data-to-text generation have shown great promise thanks to the use of large-scale datasets and the application of neural network architectures which are trained end-to-end. These models rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically, treating entities as nothing more than vocabulary tokens. In this work we propose an entity-centric neural architecture for data-to-text generation. Our model creates entity-specific representations which are dynamically updated. Text is generated conditioned on the data input and entity memory representations using hierarchical attention at each time step. We present experiments on the RotoWire benchmark and a (five times larger) new dataset on the baseball domain which we create. Our results show that the proposed model outperforms competitive baselines in automatic and human evaluation.',\n",
       "  'year': 2019},\n",
       " {'paperId': '5baf997ca06f2448f70183aa994390ccb6a6d370',\n",
       "  'title': 'Categorization in the Wild: Generalizing Cognitive Models to Naturalistic Data across Languages',\n",
       "  'abstract': 'Categories such as animal or furniture are acquired at an early age and play an important role in processing, organizing, and communicating world knowledge. Categories exist across cultures: they allow to efficiently represent the complexity of the world, and members of a community strongly agree on their nature, revealing a shared mental representation. Models of category learning and representation, however, are typically tested on data from small-scale experiments involving small sets of concepts with artificially restricted features; and experiments predominantly involve participants of selected cultural and socio-economical groups (very often involving western native speakers of English such as U.S. college students) . This work investigates whether models of categorization generalize (a) to rich and noisy data approximating the environment humans live in; and (b) across languages and cultures. We present a Bayesian cognitive model designed to jointly learn categories and their structured representation from natural language text which allows us to (a) evaluate performance on a large scale, and (b) apply our model to a diverse set of languages. We show that meaningful categories comprising hundreds of concepts and richly structured featural representations emerge across languages. Our work illustrates the potential of recent advances in computational modeling and large scale naturalistic datasets for cognitive science research.',\n",
       "  'year': 2019},\n",
       " {'paperId': '5ccfbddcfd8684a97fe1b693b8b510526936f553',\n",
       "  'title': 'Sentence Centrality Revisited for Unsupervised Summarization',\n",
       "  'abstract': 'Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.',\n",
       "  'year': 2019},\n",
       " {'paperId': '5e2a41122b0542dcdc2b45eb8d3a555a647e1bbd',\n",
       "  'title': 'Correct Program : Instantiation Execution Denotation : 0 Spurious Programs : Inconsistent Program',\n",
       "  'abstract': 'Semantic parsing aims to map natural language utterances onto machine interpretable meaning representations, aka programs whose execution against a real-world environment produces a denotation. Weakly-supervised semantic parsers are trained on utterancedenotation pairs treating programs as latent. The task is challenging due to the large search space and spuriousness of programs which may execute to the correct answer but do not generalize to unseen examples. Our goal is to instill an inductive bias in the parser to help it distinguish between spurious and correct programs. We capitalize on the intuition that correct programs would likely respect certain structural constraints were they to be aligned to the question (e.g., program fragments are unlikely to align to overlapping text spans) and propose to model alignments as structured latent variables. In order to make the latent-alignment framework tractable, we decompose the parsing task into (1) predicting a partial ‚Äúabstract program‚Äù and (2) refining it while modeling structured alignments with differential dynamic programming. We obtain state-of-the-art performance on the WIKITABLEQUESTIONS and WIKISQL datasets. When compared to a standard attention baseline, we observe that the proposed structuredalignment mechanism is highly beneficial.',\n",
       "  'year': 2019},\n",
       " {'paperId': '63748e59f4e106cbda6b65939b77589f40e48fcb',\n",
       "  'title': 'Text Summarization with Pretrained Encoders',\n",
       "  'abstract': 'Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings. Our code is available at this https URL',\n",
       "  'year': 2019},\n",
       " {'paperId': '76366a281cfc36f1cb28d7860c7bdd89d18e8495',\n",
       "  'title': 'Controllable Sentence Simplification: Employing Syntactic and Lexical Constraints',\n",
       "  'abstract': 'Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with sequence-to-sequence models which have been developed assuming homogeneous target audiences. In this paper we argue that different users have different simplification needs (e.g. dyslexics vs. non-native speakers), and propose CROSS, ContROllable Sentence Simplification model, which allows to control both the level of simplicity and the type of the simplification. We achieve this by enriching a Transformer-based architecture with syntactic and lexical constraints (which can be set or learned from data). Empirical results on two benchmark datasets show that constraints are key to successful simplification, offering flexible generation output.',\n",
       "  'year': 2019},\n",
       " {'paperId': '7cc730da554003dda77796d2cb4f06da5dfd5592',\n",
       "  'title': 'Hierarchical Transformers for Multi-Document Summarization',\n",
       "  'abstract': 'In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.',\n",
       "  'year': 2019},\n",
       " {'paperId': '8568d65e8c7808731a99c00d73b1bda3debea729',\n",
       "  'title': 'Semantic graph parsing with recurrent neural network DAG grammars',\n",
       "  'abstract': 'Semantic parses are directed acyclic graphs (DAGs), so semantic parsing should be modeled as graph prediction. But predicting graphs presents difficult technical challenges, so it is simpler and more common to predict the *linearized* graphs found in semantic parsing datasets using well-understood sequence models. The cost of this simplicity is that the predicted strings may not be well-formed graphs. We present recurrent neural network DAG grammars, a graph-aware sequence model that generates only well-formed graphs while sidestepping many difficulties in graph prediction. We test our model on the Parallel Meaning Bank‚Äîa multilingual semantic graphbank. Our approach yields competitive results in English and establishes the first results for German, Italian and Dutch.',\n",
       "  'year': 2019},\n",
       " {'paperId': '9bbdc2979544dd335c5012c1b2dc3f2a847fe817',\n",
       "  'title': \"University of Edinburgh's submission to the Document-level Generation and Translation Shared Task\",\n",
       "  'abstract': 'The University of Edinburgh participated in all six tracks: NLG, MT, and MT+NLG with both English and German as targeted languages. For the NLG track, we submitted a multilingual system based on the Content Selection and Planning model of Puduppully et al (2019). For the MT track, we submitted Transformer-based Neural Machine Translation models, where out-of-domain parallel data was augmented with in-domain data extracted from monolingual corpora. Our MT+NLG systems disregard the structured input data and instead rely exclusively on the source summaries.',\n",
       "  'year': 2019},\n",
       " {'paperId': 'a27cf1640713b45c92f155b0f54c97f09b658277',\n",
       "  'title': 'WITHDRAWN: Word n-gram attention models for sentence similarity and inference',\n",
       "  'abstract': None,\n",
       "  'year': 2019},\n",
       " {'paperId': 'a6af5e3766e510fb67fc4632b34bb9ef702ebdb1',\n",
       "  'title': 'Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs',\n",
       "  'abstract': 'Semantic parsing aims to map natural language utterances onto machine interpretable meaning representations, aka programs whose execution against a real-world environment produces a denotation. Weakly-supervised semantic parsers are trained on utterance-denotation pairs treating programs as latent. The task is challenging due to the large search space and spuriousness of programs which may execute to the correct answer but do not generalize to unseen examples. Our goal is to instill an inductive bias in the parser to help it distinguish between spurious and correct programs. We capitalize on the intuition that correct programs would likely respect certain structural constraints were they to be aligned to the question (e.g., program fragments are unlikely to align to overlapping text spans) and propose to model alignments as structured latent variables. In order to make the latent-alignment framework tractable, we decompose the parsing task into (1) predicting a partial ‚Äúabstract program‚Äù and (2) refining it while modeling structured alignments with differential dynamic programming. We obtain state-of-the-art performance on the WikiTableQuestions and WikiSQL datasets. When compared to a standard attention baseline, we observe that the proposed structured-alignment mechanism is highly beneficial.',\n",
       "  'year': 2019},\n",
       " {'paperId': 'b74208db3a241d3e7567e29bae3b81b1ad75cb91',\n",
       "  'title': 'Disambiguating Visual Verbs',\n",
       "  'abstract': 'In this article, we introduce a new task, visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful for a wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce a new dataset, which we call VerSe (short for Verb Sense) that augments existing multimodal datasets (COCO and TUHOI) with verb and sense labels. We explore supervised and unsupervised models for the sense disambiguation task using textual, visual, and multimodal embeddings. We also consider a scenario in which we must detect the verb depicted in an image prior to predicting its sense (i.e., there is no verbal information associated with the image). We find that textual embeddings perform well when gold-standard annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images. VerSe is publicly available at https://github.com/spandanagella/verse.',\n",
       "  'year': 2019},\n",
       " {'paperId': 'bd74bbf95713f84d7199c7ccee44a87b740a0654',\n",
       "  'title': 'Semi-Supervised Semantic Role Labeling with Cross-View Training',\n",
       "  'abstract': 'The successful application of neural networks to a variety of NLP tasks has provided strong impetus to develop end-to-end models for semantic role labeling which forego the need for extensive feature engineering. Recent approaches rely on high-quality annotations which are costly to obtain, and mostly unavailable in low resource scenarios (e.g., rare languages or domains). Our work aims to reduce the annotation effort involved via semi-supervised learning. We propose an end-to-end SRL model and demonstrate it can effectively leverage unlabeled data under the cross-view training modeling paradigm. Our LSTM-based semantic role labeler is jointly trained with a sentence learner, which performs POS tagging, dependency parsing, and predicate identification which we argue are critical to learning directly from unlabeled data without recourse to external pre-processing tools. Experimental results on the CoNLL-2009 benchmark dataset show that our model outperforms the state of the art in English, and consistently improves performance in other languages, including Chinese, German, and Spanish.',\n",
       "  'year': 2019},\n",
       " {'paperId': 'cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022',\n",
       "  'title': 'Text Generation from Knowledge Graphs with Graph Transformers',\n",
       "  'abstract': 'Generating texts which express complex ideas spanning multiple sentences requires a structured representation of their content (document plan), but these representations are prohibitively expensive to manually produce. In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph. Graphical knowledge representations are ubiquitous in computing, but pose a significant challenge for text generation techniques due to their non-hierarchical nature, collapsing of long-distance dependencies, and structural variety. We introduce a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints. Incorporated into an encoder-decoder setup, we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientific text. Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods.',\n",
       "  'year': 2019},\n",
       " {'paperId': 'db53e9926d7092d7c839c38123be85e84840192a',\n",
       "  'title': 'Discourse Representation Parsing for Sentences and Documents',\n",
       "  'abstract': 'We introduce a novel semantic parsing task based on Discourse Representation Theory (DRT; Kamp and Reyle 1993). Our model operates over Discourse Representation Tree Structures which we formally define for sentences and documents. We present a general framework for parsing discourse structures of arbitrary length and granularity. We achieve this with a neural model equipped with a supervised hierarchical attention mechanism and a linguistically-motivated copy strategy. Experimental results on sentence- and document-level benchmarks show that our model outperforms competitive baselines by a wide margin.',\n",
       "  'year': 2019},\n",
       " {'paperId': 'e8da733085746eb6344c73e506343e8db2b48545',\n",
       "  'title': 'Syntax-aware Semantic Role Labeling without Parsing',\n",
       "  'abstract': 'In this paper we focus on learning dependency aware representations for semantic role labeling without recourse to an external parser. The backbone of our model is an LSTM-based semantic role labeler jointly trained with two auxiliary tasks: predicting the dependency label of a word and whether there exists an arc linking it to the predicate. The auxiliary tasks provide syntactic information that is specific to semantic role labeling and are learned from training data (dependency annotations) without relying on existing dependency parsers, which can be noisy (e.g., on out-of-domain data or infrequent constructions). Experimental results on the CoNLL-2009 benchmark dataset show that our model outperforms the state of the art in English, and consistently improves performance in other languages, including Chinese, German, and Spanish.',\n",
       "  'year': 2019},\n",
       " {'paperId': 'ec7fad7ea827a11381bf1fc10b8c99e8af58f524',\n",
       "  'title': 'What is this Article about? Extreme Summarization with Topic-aware Convolutional Neural Networks',\n",
       "  'abstract': 'We introduce \"extreme summarization,\" a new\\xa0single-document summarization task which aims at creating a short,\\xa0one-sentence news summary answering the question \"What is the\\xa0article about?\". We argue that extreme summarization, by nature, is\\xa0not amenable to extractive strategies and requires an abstractive\\xa0modeling approach. In the hope of driving research on this task\\xa0further: (a) we collect a real-world, large scale dataset by\\xa0harvesting online articles from the British Broadcasting Corporation\\xa0(BBC); and (b) propose a novel abstractive model which is\\xa0conditioned on the article\\'s topics and based entirely on\\xa0convolutional neural networks. We demonstrate experimentally that\\xa0this architecture captures long-range dependencies in a document and\\xa0recognizes pertinent content, outperforming an oracle extractive\\xa0system and state-of-the-art abstractive approaches when evaluated automatically and by humans on the extreme summarization\\xa0dataset.',\n",
       "  'year': 2019},\n",
       " {'paperId': 'f24cb415f5364dd3875266ce8b85ca92bdb69ec0',\n",
       "  'title': 'Data-to-Text Generation with Content Selection and Planning',\n",
       "  'abstract': 'Recent advances in data-to-text generation have led to the use of large-scale datasets and neural network models which are trained end-to-end, without explicitly modeling what to say and in what order. In this work, we present a neural network architecture which incorporates content selection and planning without sacrificing end-to-end training. We decompose the generation task into two stages. Given a corpus of data records (paired with descriptive documents), we first generate a content plan highlighting which information should be mentioned and in which order and then generate the document while taking the content plan into account. Automatic and human-based evaluation experiments show that our model1 outperforms strong baselines improving the state-of-the-art on the recently released RotoWIRE dataset.',\n",
       "  'year': 2019},\n",
       " {'paperId': 'feef26f7932bf16f86a043aeb0651ed45e0ece09',\n",
       "  'title': 'Learning an Executable Neural Semantic Parser',\n",
       "  'abstract': 'This article describes a neural semantic parser that maps natural language utterances onto logical forms that can be executed against a task-specific environment, such as a knowledge base or a database, to produce a response. The parser generates tree-structured logical forms with a transition-based approach, combining a generic tree-generation algorithm with domain-general grammar defined by the logical language. The generation process is modeled by structured recurrent neural networks, which provide a rich encoding of the sentential context and generation history for making predictions. To tackle mismatches between natural language and logical form tokens, various attention mechanisms are explored. Finally, we consider different training settings for the neural semantic parser, including fully supervised training where annotated logical forms are given, weakly supervised training where denotations are provided, and distant supervision where only unlabeled sentences and a knowledge base are available. Experiments across a wide range of data sets demonstrate the effectiveness of our parser.',\n",
       "  'year': 2019},\n",
       " {'paperId': '0818b7b82f2a9a71a6efa79284e50e56cd8af68c',\n",
       "  'title': 'Bootstrapping Generators from Noisy Data',\n",
       "  'abstract': 'A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and associated texts. In this paper we aim to bootstrap generators from large scale datasets where the data (e.g., DBPedia facts) and related texts (e.g., Wikipedia abstracts) are loosely aligned. We tackle this challenging task by introducing a special-purpose content selection mechanism. We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention.',\n",
       "  'year': 2018},\n",
       " {'paperId': '0d01065fef78e00dc5948d8051987d00ad28f6d4',\n",
       "  'title': 'Discourse Representation Structure Parsing',\n",
       "  'abstract': 'We introduce an open-domain neural semantic parser which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). We propose a method which transforms Discourse Representation Structures (DRSs) to trees and develop a structure-aware model which decomposes the decoding process into three stages: basic DRS structure prediction, condition prediction (i.e., predicates and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin.',\n",
       "  'year': 2018},\n",
       " {'paperId': '105d015a0f8b652f26857fdde9c2f4abba6cc7ce',\n",
       "  'title': 'Confidence Modeling for Neural Semantic Parsing',\n",
       "  'abstract': 'In this work we focus on confidence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct. Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model, and verify or refine its input. Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores.',\n",
       "  'year': 2018},\n",
       " {'paperId': '16d0afaeb8419ec1c37c3473ab581df916148d72',\n",
       "  'title': 'Explorer Neural Latent Extractive Document Summarization',\n",
       "  'abstract': 'Extractive summarization models require sentence-level labels, which are usually created heuristically (e.g., with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes directly from gold summaries. Experiments on the CNN/Dailymail dataset show that our model improves over a strong extractive baseline trained on heuristically approximated labels and also performs competitively to several recent models.',\n",
       "  'year': 2018},\n",
       " {'paperId': '29219d826ead654f2b863de6eceb69811850b7d4',\n",
       "  'title': 'Learning Structured Text Representations',\n",
       "  'abstract': 'In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.',\n",
       "  'year': 2018},\n",
       " {'paperId': '305b2cf37e5dece81e95c92883d5a6e28ac93b22',\n",
       "  'title': 'Don‚Äôt Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization',\n",
       "  'abstract': 'We introduce ‚Äúextreme summarization‚Äù, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question ‚ÄúWhat is the article about?‚Äù. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article‚Äôs topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.',\n",
       "  'year': 2018},\n",
       " {'paperId': '31873a7fc8d97ad9eb71cb5135af8c9e839e65df',\n",
       "  'title': 'Sentence Compression for Arbitrary Languages via Multilingual Pivoting',\n",
       "  'abstract': 'In this paper we advocate the use of bilingual corpora which are abundantly available for training sentence compression models. Our approach borrows much of its machinery from neural machine translation and leverages bilingual pivoting: compressions are obtained by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length. Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data. We release. Moss, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres.',\n",
       "  'year': 2018},\n",
       " {'paperId': '3cef7f532bbbbf34852c9c456011fe921311566b',\n",
       "  'title': 'Structured Alignment Networks for Matching Sentences',\n",
       "  'abstract': 'Many tasks in natural language processing involve comparing two sentences to compute some notion of relevance, entailment, or similarity. Typically this comparison is done either at the word level or at the sentence level, with no attempt to leverage the inherent structure of the sentence. When sentence structure is used for comparison, it is obtained during a non-differentiable pre-processing step, leading to propagation of errors. We introduce a model of structured alignments between sentences, showing how to compare two sentences by matching their latent structures. Using a structured attention mechanism, our model matches candidate spans in the first sentence to candidate spans in the second sentence, simultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, natural entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena.',\n",
       "  'year': 2018},\n",
       " {'paperId': '3f12d57c3123956c52def72b97aaaf4204180be1',\n",
       "  'title': 'Neural Latent Extractive Document Summarization',\n",
       "  'abstract': 'Extractive summarization models need sentence level labels, which are usually created with rule-based methods since most summarization datasets only have document summary pairs. These labels might be suboptimal. We propose a latent variable extractive model, where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training, the loss can come directly from gold summaries. Experiments on CNN/Dailymail dataset show our latent extractive model outperforms a strong extractive baseline trained on rule-based labels and also performs competitively with several recent models.',\n",
       "  'year': 2018},\n",
       " {'paperId': '59562be2cf8e01e8b7bb7560cef56158ea171227',\n",
       "  'title': 'Ranking Sentences for Extractive Summarization with Reinforcement Learning',\n",
       "  'abstract': 'Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.',\n",
       "  'year': 2018},\n",
       " {'paperId': '5c2d880de3a9c2781303c6a8fb041c0165b0ad07',\n",
       "  'title': 'What‚Äôs This Movie About? A Joint Neural Network Architecture for Movie Content Analysis',\n",
       "  'abstract': 'This work takes a first step toward movie content analysis by tackling the novel task of movie overview generation. Overviews are natural language texts that give a first impression of a movie, describing aspects such as its genre, plot, mood, or artistic style. We create a dataset that consists of movie scripts, attribute-value pairs for the movies‚Äô aspects, as well as overviews, which we extract from an online database. We present a novel end-to-end model for overview generation, consisting of a multi-label encoder for identifying screenplay attributes, and an LSTM decoder to generate natural language sentences conditioned on the identified attributes. Automatic and human evaluation show that the encoder is able to reliably assign good labels for the movie‚Äôs attributes, and the overviews provide descriptions of the movie‚Äôs content which are informative and faithful.',\n",
       "  'year': 2018},\n",
       " {'paperId': '6b8ab30eefbbb5de171311086fcd5a4a00a6b4a3',\n",
       "  'title': 'Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised',\n",
       "  'abstract': 'We present a neural framework for opinion summarization from online product reviews which is knowledge-lean and only requires light supervision (e.g., in the form of product domain labels and user-provided ratings). Our method combines two weakly supervised components to identify salient opinions and form extractive summaries from multiple reviews: an aspect extractor trained under a multi-task objective, and a sentiment predictor based on multiple instance learning. We introduce an opinion summarization dataset that includes a training set of product reviews from six diverse domains and human-annotated development and test sets with gold standard aspect annotations, salience labels, and opinion summaries. Automatic evaluation shows significant improvements over baselines, and a large-scale study indicates that our opinion summaries are preferred by human judges according to multiple criteria.',\n",
       "  'year': 2018},\n",
       " {'paperId': '9283becd6596f9b4d0e0d753b7606edab5acfa8b',\n",
       "  'title': 'Weakly-Supervised Neural Semantic Parsing with a Generative Ranker',\n",
       "  'abstract': 'Weakly-supervised semantic parsers are trained on utterance-denotation pairs, treating logical forms as latent. The task is challenging due to the large search space and spuriousness of logical forms. In this paper we introduce a neural parser-ranker system for weakly-supervised semantic parsing. The parser generates candidate tree-structured logical forms from utterances using clues of denotations. These candidates are then ranked based on two criterion: their likelihood of executing to the correct denotation, and their agreement with the utterance semantics. We present a scheduled training procedure to balance the contribution of the two objectives. Furthermore, we propose to use a neurally encoded lexicon to inject prior domain knowledge to the model. Experiments on three Freebase datasets demonstrate the effectiveness of our semantic parser, achieving results within the state-of-the-art range.',\n",
       "  'year': 2018},\n",
       " {'paperId': 'a3b7d461b2e834ae64ec7747adc24cf4fdd660ae',\n",
       "  'title': 'Building a Neural Semantic Parser from a Domain Ontology',\n",
       "  'abstract': 'Semantic parsing is the task of converting natural language utterances into machine interpretable meaning representations which can be executed against a real-world environment such as a database. Scaling semantic parsing to arbitrary domains faces two interrelated challenges: obtaining broad coverage training data effectively and cheaply; and developing a model that generalizes to compositional utterances and complex intentions. We address these challenges with a framework which allows to elicit training data from a domain ontology and bootstrap a neural parser which recursively builds derivations of logical forms. In our framework meaning representations are described by sequences of natural language templates, where each template corresponds to a decomposed fragment of the underlying meaning representation. Although artificial, templates can be understood and paraphrased by humans to create natural utterances, resulting in parallel triples of utterances, meaning representations, and their decompositions. These allow us to train a neural semantic parser which learns to compose rules in deriving meaning representations. We crowdsource training data on six domains, covering both single-turn utterances which exhibit rich compositionality, and sequential utterances where a complex task is procedurally performed in steps. We then develop neural semantic parsers which perform such compositional tasks. In general, our approach allows to deploy neural semantic parsers quickly and cheaply from a given domain ontology.',\n",
       "  'year': 2018},\n",
       " {'paperId': 'a69f7e80bae350dd51518424abd2d63b73cb6ee6',\n",
       "  'title': 'Understanding visual scenes',\n",
       "  'abstract': 'Abstract A growing body of recent work focuses on the challenging problem of scene understanding using a variety of cross-modal methods which fuse techniques from image and text processing. In this paper, we develop representations for the semantics of scenes by explicitly encoding the objects detected in them and their spatial relations. We represent image content via two well-known types of tree representations, namely constituents and dependencies. Our representations are created deterministically, can be applied to any image dataset irrespective of the task at hand, and are amenable to standard NLP tools developed for tree-based structures. We show that we can apply syntax-based SMT and tree kernel methods in order to build models for image description generation and image-based retrieval. Experimental results on real-world images demonstrate the effectiveness of the framework.',\n",
       "  'year': 2018},\n",
       " {'paperId': 'ab60cb9bb176aa01c1116b7487395137c6700c5d',\n",
       "  'title': 'Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis',\n",
       "  'abstract': 'We consider the task of fine-grained sentiment analysis from the perspective of multiple instance learning (MIL). Our neural model is trained on document sentiment labels, and learns to predict the sentiment of text segments, i.e. sentences or elementary discourse units (EDUs), without segment-level supervision. We introduce an attention-based polarity scoring method for identifying positive and negative text snippets and a new dataset which we call SpoT (as shorthand for Segment-level POlariTy annotations) for evaluating MIL-style sentiment models like ours. Experimental results demonstrate superior performance against multiple baselines, whereas a judgement elicitation study shows that EDU-level opinion extraction produces more informative summaries than sentence-based alternatives.',\n",
       "  'year': 2018},\n",
       " {'paperId': 'e7bfdd47fbad3caefa4d2a120170d0ee7431a2a0',\n",
       "  'title': 'Document Modeling with External Attention for Sentence Extraction',\n",
       "  'abstract': 'Document modeling is essential to a variety of natural language understanding tasks. We propose to use external information to improve document modeling for problems that can be framed as sentence extraction. We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information. We evaluate our model on extractive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question). We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.',\n",
       "  'year': 2018},\n",
       " {'paperId': 'f6dd15b7e674755b01fe605efb42d03d920ebcde',\n",
       "  'title': 'Whodunnit? Crime Drama as a Case for Natural Language Understanding',\n",
       "  'abstract': 'In this paper we argue that crime drama exemplified in television programs such as CSI: Crime Scene Investigation is an ideal testbed for approximating real-world natural language understanding and the complex inferences associated with it. We propose to treat crime drama as a new inference task, capitalizing on the fact that each episode poses the same basic question (i.e., who committed the crime) and naturally provides the answer when the perpetrator is revealed. We develop a new dataset based on CSI episodes, formalize perpetrator identification as a sequence labeling problem, and develop an LSTM-based model which learns from multi-modal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input.',\n",
       "  'year': 2018},\n",
       " {'paperId': 'fd04e31c25451f9103a0ac2220ac8d7e7884c343',\n",
       "  'title': 'Coarse-to-Fine Decoding for Neural Semantic Parsing',\n",
       "  'abstract': 'Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.',\n",
       "  'year': 2018},\n",
       " {'paperId': '191c627fed56ebbbf7a04dbbfab67d3924690ebd',\n",
       "  'title': 'The European Chapter of the Association for Computational Linguistics (EACL 2017)',\n",
       "  'abstract': None,\n",
       "  'year': 2017},\n",
       " {'paperId': '1a050227dbb8de8b6345d451859e9e009b09fff3',\n",
       "  'title': 'Edinburgh Research Explorer Dependency Parsing as Head Selection',\n",
       "  'abstract': 'Conventional graph-based dependency parsers guarantee a tree structure both during training and inference. Instead, we formalize dependency parsing as the problem of independently selecting the head of each word in a sentence. Our model which we call DENSE (as shorthand for Dependency Neural Selection) produces a distribution over possible heads for each word using features obtained from a bidirectional recurrent neural network. Without enforcing structural constraints during training, DENSE generates (at inference time) trees for the overwhelming majority of sentences, while non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate DENSE on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of the approach, our parsers are on par with the state of the art.1',\n",
       "  'year': 2017},\n",
       " {'paperId': '216fa708ee2f2e24e53fa6e741c90a09fd08564d',\n",
       "  'title': 'Explorer Paraphrasing Revisited with Neural Machine Translation',\n",
       "  'abstract': 'Recognizing and generating paraphrases is an important component in many natural language processing applications. A wellestablished technique for automatically extracting paraphrases leverages bilingual corpora to find meaning-equivalent phrases in a single language by ‚Äúpivoting‚Äù over a shared translation in another language. In this paper we revisit bilingual pivoting in the context of neural machine translation and present a paraphrasing model based purely on neural networks. Our model represents paraphrases in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, or generates candidate paraphrases for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches.',\n",
       "  'year': 2017},\n",
       " {'paperId': '21e31a5d15e2294bd7028307f88c1aac5d25c295',\n",
       "  'title': 'Explorer Learning to Generate Product Reviews from Attributes',\n",
       "  'abstract': 'Automatically generating product reviews is a meaningful, yet not well-studied task in sentiment analysis. Traditional natural language generation methods rely extensively on hand-crafted rules and predefined templates. This paper presents an attention-enhanced attribute-to-sequence model to generate product reviews for given attribute information, such as user, product, and rating. The attribute encoder learns to represent input attributes as vectors. Then, the sequence decoder generates reviews by conditioning its output on these vectors. We also introduce an attention mechanism to jointly generate reviews and align words with input attributes. The proposed model is trained end-to-end to maximize the likelihood of target product reviews given the attributes. We build a publicly available dataset for the review generation task by leveraging the Amazon book reviews and their metadata. Experiments on the dataset show that our approach outperforms baseline methods and the attention mechanism significantly improves the performance of our model.',\n",
       "  'year': 2017},\n",
       " {'paperId': '2c58fe19e74a13d8fbb03d36e6263bcec47c90b7',\n",
       "  'title': 'Autofolding for Source Code Summarization',\n",
       "  'abstract': 'Developers spend much of their time reading and browsing source code, raising new opportunities for summarization methods. Indeed, modern code editors provide code folding, which allows one to selectively hide blocks of code. However this is impractical to use as folding decisions must be made manually or based on simple rules. We introduce the autofolding problem, which is to automatically create a code summary by folding less informative code regions. We present a novel solution by formulating the problem as a sequence of AST folding decisions, leveraging a scoped topic model for code tokens. On an annotated set of popular open source projects, we show that our summarizer outperforms simpler baselines, yielding a 28 percent error reduction. Furthermore, we find through a case study that our summarizer is strongly preferred by experienced developers. More broadly, we hope this work will aid program comprehension by turning code folding into a usable and valuable tool.',\n",
       "  'year': 2017},\n",
       " {'paperId': '309a8aef55ca8f89ef56973bb2c3b38d84a29113',\n",
       "  'title': 'Neural Extractive Summarization with Side Information',\n",
       "  'abstract': 'Most extractive summarization methods focus on the main body of the document from which sentences need to be extracted. However, the gist of the document may lie in side information, such as the title and image captions which are often available for newswire articles. We propose to explore side information in the context of single-document extractive summarization. We develop a framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor with attention over side information. We evaluate our model on a large scale news dataset. We show that extractive summarization with side information consistently outperforms its counterpart that does not use any side information, in terms of both informativeness and fluency.',\n",
       "  'year': 2017},\n",
       " {'paperId': '31f5864ada5fb08b69da74b6d5ad99e385dcc737',\n",
       "  'title': 'Sentence Simplification with Deep Reinforcement Learning',\n",
       "  'abstract': 'Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.',\n",
       "  'year': 2017},\n",
       " {'paperId': '54e71e43a657c4ed69b690688ff03b9f53a21a3f',\n",
       "  'title': 'Explorer Autofolding for Source Code Summarization',\n",
       "  'abstract': 'Developers spend much of their time reading and browsing source code, raising new opportunities for summarization methods. Indeed, modern code editors provide code folding, which allows one to selectively hide blocks of code. However this is impractical to use as folding decisions must be made manually or based on simple rules. We introduce the autofolding problem, which is to automatically create a code summary by folding less informative code regions. We present a novel solution by formulating the problem as a sequence of AST folding decisions, leveraging a scoped topic model for code tokens. On an annotated set of popular open source projects, we show that our summarizer outperforms simpler baselines, yielding a 28% error reduction. Furthermore, we find through a case study that our summarizer is strongly preferred by experienced developers. More broadly, we hope this work will aid program comprehension by turning code folding into a usable and valuable tool.',\n",
       "  'year': 2017},\n",
       " {'paperId': '5acb079d7187f4f4102ebb0dc6b38a89bcf5ce7e',\n",
       "  'title': 'Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers',\n",
       "  'abstract': None,\n",
       "  'year': 2017},\n",
       " {'paperId': '610f535b4bf5d87a5ae6bd83292d994bfa83feaf',\n",
       "  'title': 'Rewriting Improves Semantic Role Labeling ( Extended Abstract )',\n",
       "  'abstract': 'Large-scale annotated corpora are a prerequisite to developing high-performance NLP systems. Such corpora are expensive to produce, limited in size, often demanding linguistic expertise. In this paper we use text rewriting as a means of increasing the amount of labeled data available for model training. Our method uses automatically extracted rewrite rules from comparable corpora and bitexts to generate multiple versions of sentences annotated with gold standard labels. We apply this idea to semantic role labeling and show that a model trained on rewritten data outperforms the state of the art on the CoNLL-2009 benchmark dataset.',\n",
       "  'year': 2017},\n",
       " {'paperId': '8eee0c0a566c9d59e264dd4119225840caa307dc',\n",
       "  'title': 'Image Pivoting for Learning Multilingual Multimodal Representations',\n",
       "  'abstract': 'In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of image search and image understanding. Our model learns a common representation for images and their descriptions in two different languages (which need not be parallel) by considering the image as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance.',\n",
       "  'year': 2017},\n",
       " {'paperId': '93f967d32669a34c93fe13b904e572b285e18f05',\n",
       "  'title': 'Learning to Generate Product Reviews from Attributes',\n",
       "  'abstract': 'Automatically generating product reviews is a meaningful, yet not well-studied task in sentiment analysis. Traditional natural language generation methods rely extensively on hand-crafted rules and predefined templates. This paper presents an attention-enhanced attribute-to-sequence model to generate product reviews for given attribute information, such as user, product, and rating. The attribute encoder learns to represent input attributes as vectors. Then, the sequence decoder generates reviews by conditioning its output on these vectors. We also introduce an attention mechanism to jointly generate reviews and align words with input attributes. The proposed model is trained end-to-end to maximize the likelihood of target product reviews given the attributes. We build a publicly available dataset for the review generation task by leveraging the Amazon book reviews and their metadata. Experiments on the dataset show that our approach outperforms baseline methods and the attention mechanism significantly improves the performance of our model.',\n",
       "  'year': 2017},\n",
       " {'paperId': '96f9492c5d1ced53b470754dbcccba7f098236e4',\n",
       "  'title': 'Dependency Parsing as Head Selection',\n",
       "  'abstract': 'Conventional graph-based dependency parsers guarantee a tree structure both during training and inference. Instead, we formalize dependency parsing as the problem of independently selecting the head of each word in a sentence. Our model which we call DENSE (as shorthand for Dependency Neural Selection) produces a distribution over possible heads for each word using features obtained from a bidirectional recurrent neural network. Without enforcing structural constraints during training, DeNSe generates (at inference time) trees for the overwhelming majority of sentences, while non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate DeNSe on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of the approach, our parsers are on par with the state of the art.',\n",
       "  'year': 2017},\n",
       " {'paperId': 'a4a825d79e9e8c16716b2154fb47f76d7bb63a78',\n",
       "  'title': 'Text Rewriting Improves Semantic Role Labeling (Extended Abstract)',\n",
       "  'abstract': None,\n",
       "  'year': 2017},\n",
       " {'paperId': 'baa1079d1ee40754c44f5ee1498d6ba0c9c71b32',\n",
       "  'title': 'Paraphrasing Revisited with Neural Machine Translation',\n",
       "  'abstract': 'Recognizing and generating paraphrases is an important component in many natural language processing applications. A well-established technique for automatically extracting paraphrases leverages bilingual corpora to find meaning-equivalent phrases in a single language by ‚Äúpivoting‚Äù over a shared translation in another language. In this paper we revisit bilingual pivoting in the context of neural machine translation and present a paraphrasing model based purely on neural networks. Our model represents paraphrases in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, and generates candidate paraphrases for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches.',\n",
       "  'year': 2017},\n",
       " {'paperId': 'c3a45b0fc4154a5a47579c2b1d4b449c9e1aef88',\n",
       "  'title': 'Learning to Paraphrase for Question Answering',\n",
       "  'abstract': 'Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-to-end using question-answer pairs as a supervision signal. A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on QA over Freebase and answer sentence selection. Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.',\n",
       "  'year': 2017},\n",
       " {'paperId': 'ceb77ba5c149e5c664cdf0a4cd46df6a9a4f4896',\n",
       "  'title': 'Learning Contextually Informed Representations for Linear-Time Discourse Parsing',\n",
       "  'abstract': 'Recent advances in RST discourse parsing have focused on two modeling paradigms: (a) high order parsers which jointly predict the tree structure of the discourse and the relations it encodes; or (b) linear-time parsers which are efficient but mostly based on local features. In this work, we propose a linear-time parser with a novel way of representing discourse constituents based on neural networks which takes into account global contextual information and is able to capture long-distance dependencies. Experimental results show that our parser obtains state-of-the art performance on benchmark datasets, while being efficient (with time complexity linear in the number of sentences in the document) and requiring minimal feature engineering.',\n",
       "  'year': 2017},\n",
       " {'paperId': 'd4f8168242f688af29bcbbe1cc5aec7cd12a601c',\n",
       "  'title': 'Edinburgh Research Explorer Visually Grounded Meaning Representations',\n",
       "  'abstract': 'In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level representations from textual and visual input. The visual modality is encoded via vectors of attributes obtained automatically from images. We create a new large-scale taxonomy of 600 visual attributes representing more than 500 concepts and 700K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We evaluate our model on its ability to simulate word similarity judgments and concept categorization. On both tasks, our model yields a better fit to behavioral data compared to baselines and related models which either rely on a single modality or do not make use of attribute-based input.',\n",
       "  'year': 2017},\n",
       " {'paperId': 'da262d5809d61d8464d6db0eafd64643cd2ff352',\n",
       "  'title': 'Question Vectors Scores Answer Question q : who created microsoft ?',\n",
       "  'abstract': 'Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-toend using question-answer pairs as a supervision signal. A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on QA over Freebase and answer sentence selection. Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.',\n",
       "  'year': 2017},\n",
       " {'paperId': 'db5be062d8b26b2e636b57cc8ca9761b673ec0e6',\n",
       "  'title': 'Learning Structured Natural Language Representations for Semantic Parsing',\n",
       "  'abstract': 'We introduce a neural semantic parser that converts natural language utterances to intermediate representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We obtain competitive results on various datasets. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.',\n",
       "  'year': 2017},\n",
       " {'paperId': 'fae871b4892fb73ca3846b2bfbb4503e661f4fab',\n",
       "  'title': 'A Generative Parser with a Discriminative Recognition Algorithm',\n",
       "  'abstract': 'Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single-model language modeling score.',\n",
       "  'year': 2017},\n",
       " {'paperId': 'faee0c81a1170402b149500f1b91c51ccaf24027',\n",
       "  'title': 'Universal Semantic Parsing',\n",
       "  'abstract': 'Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions.',\n",
       "  'year': 2017},\n",
       " {'paperId': 'fbd429d1a5582ef4347abc7cd99eb6d5b740e70a',\n",
       "  'title': 'Explorer Image Pivoting for Learning Multilingual Multimodal Representations',\n",
       "  'abstract': 'In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of image search and image understanding. Our model learns a common representation for images and their descriptions in two different languages (which need not be parallel) by considering the image as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance.',\n",
       "  'year': 2017},\n",
       " {'paperId': 'fd909d6f6e3be5ee87a0c9c42767e0b7eaa2e968',\n",
       "  'title': 'Visually Grounded Meaning Representations',\n",
       "  'abstract': 'In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level representations from textual and visual input. The visual modality is encoded via vectors of attributes obtained automatically from images. We create a new large-scale taxonomy of 600\\xa0visual attributes representing more than 500 concepts and 700 K\\xa0images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We evaluate our model on its ability to simulate word similarity judgments and concept categorization. On both tasks, our model yields a better fit to behavioral data compared to baselines and related models which either rely on a single modality or do not make use of attribute-based input.',\n",
       "  'year': 2017},\n",
       " {'paperId': '13fe71da009484f240c46f14d9330e932f8de210',\n",
       "  'title': 'Long Short-Term Memory-Networks for Machine Reading',\n",
       "  'abstract': 'In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.',\n",
       "  'year': 2016},\n",
       " {'paperId': '1886ca51f556a61995ff0363d49aeedbb688befd',\n",
       "  'title': 'Proceedings of NAACL-HLT 2016',\n",
       "  'abstract': None,\n",
       "  'year': 2016},\n",
       " {'paperId': '29a294eaec7b485245aa21d994f7300f6b5da8fc',\n",
       "  'title': 'Neural Summarization by Extracting Sentences and Words',\n",
       "  'abstract': 'Traditional approaches to extractive summarization rely heavily on humanengineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs 1 . Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.',\n",
       "  'year': 2016},\n",
       " {'paperId': '3e3cf09d619cff79b8379b639cddfcd09451995b',\n",
       "  'title': 'Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings',\n",
       "  'abstract': 'We introduce a new task, visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful for a wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce VerSe, a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. We propose an unsupervised algorithm based on Lesk which performs visual sense disambiguation using textual, visual, or multimodal embeddings. We find that textual embeddings perform well when gold-standard textual annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images. We also verify our findings by using the textual and multimodal embeddings as features in a supervised setting and analyse the performance of visual sense disambiguation task. VerSe is made publicly available and can be downloaded at: this https URL',\n",
       "  'year': 2016},\n",
       " {'paperId': '558ac446dc26bee9789d660a251b75728cb6eeb2',\n",
       "  'title': 'Language to Logical Form with Neural Attention',\n",
       "  'abstract': 'Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.',\n",
       "  'year': 2016},\n",
       " {'paperId': '5f3d28b5c60386994f91f54cf61d1c8568ed328e',\n",
       "  'title': 'The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies',\n",
       "  'abstract': None,\n",
       "  'year': 2016},\n",
       " {'paperId': '70d3d2e0a8f34d6c3cb7890e249e2ed6a574ce50',\n",
       "  'title': 'Neural Semantic Role Labeling with Dependency Path Embeddings',\n",
       "  'abstract': 'This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as subsequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method.',\n",
       "  'year': 2016},\n",
       " {'paperId': '93e64b4ab413381dbb63246757fdb42694052066',\n",
       "  'title': 'Visually Grounded Meaning Representations.',\n",
       "  'abstract': 'In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level representations from textual and visual input. The visual modality is encoded via vectors of attributes obtained automatically from images. We create a new large-scale taxonomy of 600 visual attributes representing more than 500 concepts and 700K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We evaluate our model on its ability to simulate word similarity judgments and concept categorization. On both tasks, our model yields a better fit to behavioral data compared to baselines and related models which either rely on a single modality or do not make use of attribute-based input.',\n",
       "  'year': 2016},\n",
       " {'paperId': 'b0cc8ac7624472905e45f0301dfcaab405299035',\n",
       "  'title': 'TASSAL: Autofolding for Source Code Summarization',\n",
       "  'abstract': 'We present a novel tool, TASSAL, that automatically creates a summary of each source file in a project by folding its least salient code regions. The intended use-case for our tool is the first-look problem: to help developers who are unfamiliar with a new codebase and are attempting to understand it. TASSAL is intended to aid developers in this task by folding away less informative regions of code and allowing them to focus their efforts on the most informative ones. While modern code editors do provide \\\\emph{code folding} to selectively hide blocks of code, it is impractical to use as folding decisions must be made manually or based on simple rules. We find through a case study that TASSAL is strongly preferred by experienced developers over simple folding baselines, demonstrating its usefulness. In short, we strongly believe TASSAL can aid program comprehension by turning code folding into a usable and valuable tool. A video highlighting the main features of TASSAL can be found at https://youtu.be/_yu7JZgiBA4.',\n",
       "  'year': 2016},\n",
       " {'paperId': 'b20eaf26e7cc4ebfbb70c74fca5a30c9f0c3eade',\n",
       "  'title': 'Incremental Bayesian Category Learning From Natural Language',\n",
       "  'abstract': 'Models of category learning have been extensively studied in cognitive science and primarily tested on perceptual abstractions or artificial stimuli. In this paper, we focus on categories acquired from natural language stimuli, that is, words (e.g., chair is a member of the furniture category). We present a Bayesian model that, unlike previous work, learns both categories and their features in a single process. We model category induction as two interrelated subproblems: (a) the acquisition of features that discriminate among categories, and (b) the grouping of concepts into categories based on those features. Our model learns categories incrementally using particle filters, a sequential Monte Carlo method commonly used for approximate probabilistic inference that sequentially integrates newly observed data and can be viewed as a plausible mechanism for human learning. Experimental results show that our incremental learner obtains meaningful categories which yield a closer fit to behavioral data compared to related models while at the same time acquiring features which characterize the learned categories. (An earlier version of this work was published in Frermann and Lapata .).',\n",
       "  'year': 2016},\n",
       " {'paperId': 'cf22b97f93e761993292960fde8bdfa1d7497c48',\n",
       "  'title': 'Explorer Visually Grounded Meaning Representations',\n",
       "  'abstract': 'In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level representations from textual and visual input. The visual modality is encoded via vectors of attributes obtained automatically from images. We create a new large-scale taxonomy of 600 visual attributes representing more than 500 concepts and 700K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We evaluate our model on its ability to simulate word similarity judgments and concept categorization. On both tasks, our model yields a better fit to behavioral data compared to baselines and related models which either rely on a single modality or do not make use of attribute-based input.',\n",
       "  'year': 2016},\n",
       " {'paperId': 'cf7acb07d5d7bda08a67e10d09ec2ed1fb3c58cf',\n",
       "  'title': 'A Bayesian Model of Diachronic Meaning Change',\n",
       "  'abstract': 'Word meanings change over time and an automated procedure for extracting this information from text would be useful for historical exploratory studies, information retrieval or question answering. We present a dynamic Bayesian model of diachronic meaning change, which infers temporal word representations as a set of senses and their prevalence. Unlike previous work, we explicitly model language change as a smooth, gradual process. We experimentally show that this modeling decision is beneficial: our model performs competitively on meaning change detection tasks whilst inducing discernible word senses and their development over time. Application of our model to the SemEval-2015 temporal classification benchmark datasets further reveals that it performs on par with highly optimized task-specific systems.',\n",
       "  'year': 2016},\n",
       " {'paperId': 'e5f0762b9cfd07f88608f5502ed4467a8b5546cb',\n",
       "  'title': 'Top-down Tree Long Short-Term Memory Networks',\n",
       "  'abstract': 'Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance.',\n",
       "  'year': 2016},\n",
       " {'paperId': 'f0e7b91c470039fd4a1a7c7fb74815c83c03d985',\n",
       "  'title': '54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers',\n",
       "  'abstract': None,\n",
       "  'year': 2016},\n",
       " {'paperId': 'f4fa16b9e15580db747110655a1e1df67ce888db',\n",
       "  'title': 'Transforming Dependency Structures to Logical Forms for Semantic Parsing',\n",
       "  'abstract': 'The strongly typed syntax of grammar formalisms such as CCG, TAG, LFG and HPSG offers a synchronous framework for deriving syntactic structures and semantic logical forms. In contrast‚Äîpartly due to the lack of a strong type system‚Äîdependency structures are easy to annotate and have become a widely used form of syntactic analysis for many languages. However, the lack of a type system makes a formal mechanism for deriving logical forms from dependency structures challenging. We address this by introducing a robust system based on the lambda calculus for deriving neo-Davidsonian logical forms from dependency trees. These logical forms are then used for semantic parsing of natural language to Freebase. Experiments on the Free917 and Web-Questions datasets show that our representation is superior to the original dependency trees and that it outperforms a CCG-based representation on this task. Compared to prior work, we obtain the strongest result to date on Free917 and competitive results on WebQuestions.',\n",
       "  'year': 2016},\n",
       " {'paperId': '230b3e73b581704986a404510bcc3c91826f4e03',\n",
       "  'title': 'Movie Script Summarization as Graph-based Scene Extraction',\n",
       "  'abstract': 'In this paper we study the task of movie script summarization, which we argue could enhance script browsing, give readers a rough idea of the script‚Äôs plotline, and speed up reading time. We formalize the process of generating a shorter version of a screenplay as the task of finding an optimal chain of scenes. We develop a graph-based model that selects a chain by jointly optimizing its logical progression, diversity, and importance. Human evaluation based on a question-answering task shows that our model produces summaries which are more informative compared to competitive baselines.',\n",
       "  'year': 2015},\n",
       " {'paperId': '26800f68335b1199aa7b88e5c2d796e26292cf2d',\n",
       "  'title': 'Context-aware Frame-Semantic Role Labeling',\n",
       "  'abstract': 'Frame semantic representations have been useful in several applications ranging from text-to-scene generation, to question answering and social network analysis. Predicting such representations from raw text is, however, a challenging task and corresponding models are typically only trained on a small set of sentence-level annotations. In this paper, we present a semantic role labeling system that takes into account sentence and discourse context. We introduce several new features which we motivate based on linguistic insights and experimentally demonstrate that they lead to significant improvements over the current state-of-the-art in FrameNet-based semantic role labeling.',\n",
       "  'year': 2015},\n",
       " {'paperId': '4c34e7af8283a94655e85cfde4d55dc52239e33d',\n",
       "  'title': 'Learning to Interpret and Describe Abstract Scenes',\n",
       "  'abstract': 'Given a (static) scene, a human can effortlessly describe what is going on (who is doing what to whom, how, and why). The process requires knowledge about the world, how it is perceived, and described. In this paper we study the problem of interpreting and verbalizing visual information using abstract scenes created from collections of clip art images. We propose a model inspired by machine translation operating over a large parallel corpus of visual relations and linguistic descriptions. We demonstrate that this approach produces human-like scene descriptions which are both fluent and relevant, outperforming a number of competitive alternatives based on templates, sentence-based retrieval, and a multimodal neural language model.',\n",
       "  'year': 2015},\n",
       " {'paperId': '52884a0c7913be319c1a2395f009cea47b03f128',\n",
       "  'title': 'Explorer Learning Grounded Meaning Representations with Autoencoders',\n",
       "  'abstract': 'In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input. The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models.',\n",
       "  'year': 2015},\n",
       " {'paperId': '5e28b8b7935dfa317f8a805f93f9e8b1e870ec8d',\n",
       "  'title': 'Explorer Incremental Bayesian Category Learning from Natural Language',\n",
       "  'abstract': None,\n",
       "  'year': 2015},\n",
       " {'paperId': '731c001e956f67a8a67c65eb143c239f7b3c4082',\n",
       "  'title': 'A Bayesian Model for Joint Learning of Categories and their Features',\n",
       "  'abstract': 'Categories such as ANIMAL or FURNITURE are acquired at an early age and play an important role in processing, organizing, and conveying world knowledge. Theories of categorization largely agree that categories are characterized by features such as function or appearance and that feature and category acquisition go hand-in-hand, however previous work has considered these problems in isolation. We present the first model that jointly learns categories and their features. The set of features is shared across categories, and strength of association is inferred in a Bayesian framework. We approximate the learning environment with natural language text which allows us to evaluate performance on a large scale. Compared to highly engineered pattern-based approaches, our model is cognitively motivated, knowledge-lean, and learns categories and features which are perceived by humans as more meaningful.',\n",
       "  'year': 2015},\n",
       " {'paperId': '8b0c024500db697b6ccace87a376b36835e424ec',\n",
       "  'title': 'Distributed Representations for Unsupervised Semantic Role Labeling',\n",
       "  'abstract': 'We present a new approach for unsupervised semantic role labeling that leverages distributed representations. We induce embeddings to represent a predicate, its arguments and their complex interdependence. Argument embeddings are learned from surrounding contexts involving the predicate and neighboring arguments, while predicate embeddings are learned from argument contexts. The induced representations are clustered into roles using a linear programming formulation of hierarchical clustering, where we can model task-specific knowledge. Experiments show improved performance over previous unsupervised semantic role labeling approaches and other distributed word representation models.',\n",
       "  'year': 2015},\n",
       " {'paperId': '9c81188d46522a71a77c48686b7a9b0d97350cf1',\n",
       "  'title': 'Tree Recurrent Neural Networks with Application to Language Modeling',\n",
       "  'abstract': 'In this paper we develop a recurrent neural network (TreeRNN), which is designed to predict a tree rather than a linear sequence as is the case in conventional recurrent neural networks. Our model defines the probability of a sentence by estimating the generation probability of its dependency tree. We construct the tree incrementally by generating the left and right dependents of a node whose probability is computed using recurrent neural networks with shared hidden layers. Application of our model to two language modeling tasks shows that it outperforms or performs on par with related models.',\n",
       "  'year': 2015},\n",
       " {'paperId': 'abce179c4b1f664f34341b29631c91654558e8cf',\n",
       "  'title': 'Explorer Incremental Bayesian Learning of Semantic Categories',\n",
       "  'abstract': 'Models of category learning have been extensively studied in cognitive science and primarily tested on perceptual abstractions or artificial stimuli. In this paper we focus on categories acquired from natural language stimuli, that is words (e.g., chair is a member of the FURNITURE category). We present a Bayesian model which, unlike previous work, learns both categories and their features in a single process. Our model employs particle filters, a sequential Monte Carlo method commonly used for approximate probabilistic inference in an incremental setting. Comparison against a state-of-the-art graph-based approach reveals that our model learns qualitatively better categories and demonstrates cognitive plausibility during learning.',\n",
       "  'year': 2015},\n",
       " {'paperId': 'bb40295efc4fb8377166fba863860422853e908e',\n",
       "  'title': 'Explorer Multiple aspect summarization using integer linear programming',\n",
       "  'abstract': 'Multi-document summarization involves many aspects of content selection and surface realization. The summaries must be informative, succinct, grammatical, and obey stylistic writing conventions. We present a method where such individual aspects are learned separately from data (without any hand-engineering) but optimized jointly using an integer linear programme. The ILP framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting, soft and hard constraints. Experimental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries.',\n",
       "  'year': 2015},\n",
       " {'paperId': 'cd8308ab5f8cd7808311a5981ea7e29c7db57480',\n",
       "  'title': 'Which Step Do I Take First? Troubleshooting with Bayesian Models',\n",
       "  'abstract': 'Online discussion forums and community question-answering websites provide one of the primary avenues for online users to share information. In this paper, we propose text mining techniques which aid users navigate troubleshooting-oriented data such as questions asked on forums and their suggested solutions. We introduce Bayesian generative models of the troubleshooting data and apply them to two interrelated tasks: (a) predicting the complexity of the solutions (e.g., plugging a keyboard in the computer is easier compared to installing a special driver) and (b) presenting them in a ranked order from least to most complex. Experimental results show that our models are on par with human performance on these tasks, while outperforming baselines based on solution length or readability.',\n",
       "  'year': 2015},\n",
       " {'paperId': 'cfcb00438afdfe7572e27a3485137226f985665c',\n",
       "  'title': 'Explorer An Information Retrieval Approach to Sense Ranking',\n",
       "  'abstract': 'In word sense disambiguation, choosing the most frequent sense for an ambiguous word is a powerful heuristic. However, its usefulness is restricted by the availability of sense-annotated data. In this paper, we propose an information retrieval-based method for sense ranking that does not require annotated data. The method queries an information retrieval engine to estimate the degree of association between a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance. We also show that the estimated sense frequencies correlate reliably with native speakers‚Äô intuitions.',\n",
       "  'year': 2015},\n",
       " {'paperId': 'f5fd38858d79a71bc5b48a889c02a66f8648767d',\n",
       "  'title': 'Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies',\n",
       "  'abstract': None,\n",
       "  'year': 2015},\n",
       " {'paperId': '0e8ab0fd2c619e32d7b6608f3f0d80fe417e088d',\n",
       "  'title': 'Large-scale Semantic Parsing without Question-Answer Pairs',\n",
       "  'abstract': 'In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the Free917 and WebQuestions benchmark datasets show our semantic parser improves over the state of the art.',\n",
       "  'year': 2014},\n",
       " {'paperId': '186afb5b9fead328a5b833bc62ddb52ccfb1c955',\n",
       "  'title': 'Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL',\n",
       "  'abstract': None,\n",
       "  'year': 2014},\n",
       " {'paperId': '229ec55602143271867682d181ec35f2e43e06e8',\n",
       "  'title': 'Chinese Poetry Generation with Recurrent Neural Networks',\n",
       "  'abstract': 'We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form. Our generator jointly performs content selection (‚Äúwhat to say‚Äù) and surface realization (‚Äúhow to say‚Äù) by learning representations of individual characters, and their combinations into one or more lines as well as how these mutually reinforce and constrain each other. Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams. Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods.',\n",
       "  'year': 2014},\n",
       " {'paperId': '25a93f3c2833be94486be29d21e8e750918e0da2',\n",
       "  'title': 'Incremental Semantic Role Labeling with Tree Adjoining Grammar',\n",
       "  'abstract': 'We introduce the task of incremental semantic role labeling (iSRL), in which semantic roles are assigned to incomplete input (sentence prefixes). iSRL is the semantic equivalent of incremental parsing, and is useful for language modeling, sentence completion, machine translation, and psycholinguistic modeling. We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon, a role propagation algorithm, and a cascade of classifiers. Our approach achieves an SRL Fscore of 78.38% on the standard CoNLL 2009 dataset. It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment, as well as a baseline based on Nivre‚Äôs incremental dependency parser.',\n",
       "  'year': 2014},\n",
       " {'paperId': '260b989f7ca90360a7ad2fbddf448140db9286db',\n",
       "  'title': 'Text Rewriting Improves Semantic Role Labeling',\n",
       "  'abstract': 'Large-scale annotated corpora are a prerequisite to developing high-performance NLP systems. Such corpora are expensive to produce, limited in size, often demanding linguistic expertise. In this paper we use text rewriting as a means of increasing the amount of labeled data available for model training. Our method uses automatically extracted rewrite rules from comparable corpora and bitexts to generate multiple versions of sentences annotated with gold standard labels. We apply this idea to semantic role labeling and show that a model trained on rewritten data outperforms the state of the art on the CoNLL-2009 benchmark dataset.',\n",
       "  'year': 2014},\n",
       " {'paperId': '3ffacd8459502b58eb266227be1c60ee16bde433',\n",
       "  'title': 'Unsupervised Interpretation of Eventive Propositions',\n",
       "  'abstract': 'This work addresses the challenge of automatically unfold transfers of meaning in eventive propositions. For example, if we want to interpret \"throw pass\" in the context of sports, we need to find the object \"ball\" that transferred some semantic properties to \"pass\" to make it acceptable as argument for \"throw\". We propose a probabilistic model for interpreting an eventive proposition by recovering two additional coupled propositions related to the one under interpretation. We gather the statistics after building a Proposition Store from a document collection, and explore different configurations to couple propositions based on WordNet relations. These coupled propositions compose an actual interpretation of the original proposition with a precision of 0.57, but only for an 18% of samples. If we evaluate whether the interpretation is just useful or not for recovering background knowledge required for interpretation, then results rise up to 0.71 of precision and recall.',\n",
       "  'year': 2014},\n",
       " {'paperId': '520aaf994273f7218952c98518b186cdfbe68fe7',\n",
       "  'title': 'Similarity-Driven Semantic Role Induction via Graph Partitioning',\n",
       "  'abstract': 'As in many natural language processing tasks, data-driven models based on supervised learning have become the method of choice for semantic role labeling. These models are guaranteed to perform well when given sufficient amount of labeled training data. Producing this data is costly and time-consuming, however, thus raising the question of whether unsupervised methods offer a viable alternative. The working hypothesis of this article is that semantic roles can be induced without human supervision from a corpus of syntactically parsed sentences based on three linguistic principles: (1) arguments in the same syntactic position (within a specific linking) bear the same semantic role, (2) arguments within a clause bear a unique role, and (3) clusters representing the same semantic role should be more or less lexically and distributionally equivalent. We present a method that implements these principles and formalizes the task as a graph partitioning problem, whereby argument instances of a verb are represented as vertices in a graph whose edges express similarities between these instances. The graph consists of multiple edge layers, each one capturing a different aspect of argument-instance similarity, and we develop extensions of standard clustering algorithms for partitioning such multi-layer graphs. Experiments for English and German demonstrate that our approach is able to induce semantic role clusters that are consistently better than a strong baseline and are competitive with the state of the art.',\n",
       "  'year': 2014},\n",
       " {'paperId': '52a27c157f1ec724491f41dc2e107b4f4e0418ac',\n",
       "  'title': 'Incremental Bayesian Learning of Semantic Categories',\n",
       "  'abstract': 'Models of category learning have been extensively studied in cognitive science and primarily tested on perceptual abstractions or artificial stimuli. In this paper we focus on categories acquired from natural language stimuli, that is words (e.g., chair is a member of the FURNITURE category). We present a Bayesian model which, unlike previous work, learns both categories and their features in a single process. Our model employs particle filters, a sequential Monte Carlo method commonly used for approximate probabilistic inference in an incremental setting. Comparison against a state-of-the-art graph-based approach reveals that our model learns qualitatively better categories and demonstrates cognitive plausibility during learning.',\n",
       "  'year': 2014},\n",
       " {'paperId': 'a3540081f3aa95646171d875f8ede7d93f45cf9b',\n",
       "  'title': 'Learning Grounded Meaning Representations with Autoencoders',\n",
       "  'abstract': 'In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input. The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models.',\n",
       "  'year': 2014},\n",
       " {'paperId': '11ddcfde20ee7e661056419cb719780ceccec727',\n",
       "  'title': 'Unsupervised Relation Extraction with General Domain Knowledge',\n",
       "  'abstract': 'In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., \" X was born in Y \" and \" X is from Y \") into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categoriza-tion (RDC) task show that our model outper-forms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.',\n",
       "  'year': 2013},\n",
       " {'paperId': '132172ef168154d8f1214fdd12813debf22f82b5',\n",
       "  'title': 'Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)',\n",
       "  'abstract': None,\n",
       "  'year': 2013},\n",
       " {'paperId': '194e9a6f02fd5f39226dc9848213479fec5f1821',\n",
       "  'title': 'Automatic Caption Generation for News Images',\n",
       "  'abstract': 'This paper is concerned with the task of automatically generating captions for images, which is important for many image-related applications. Examples include video and image retrieval as well as the development of tools that aid visually impaired individuals to access pictorial information. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned and colocated with thematically related documents. Our model learns to create captions from a database of news articles, the pictures embedded in them, and their captions, and consists of two stages. Content selection identifies what the image and accompanying article are about, whereas surface realization determines how to verbalize the chosen content. We approximate content selection with a probabilistic image annotation model that suggests keywords for an image. The model postulates that images and their textual descriptions are generated by a shared set of latent variables (topics) and is trained on a weakly labeled dataset (which treats the captions and associated news articles as image labels). Inspired by recent work in summarization, we propose extractive and abstractive surface realization models. Experimental results show that it is viable to generate captions that are pertinent to the specific content of an image and its associated article, while permitting creativity in the description. Indeed, the output of our abstractive model compares favorably to handwritten captions and is often superior to extractive methods.',\n",
       "  'year': 2013},\n",
       " {'paperId': '229b5f92d5d07a957421f54829e91d9ea481e553',\n",
       "  'title': 'Inducing Document Plans for Concept-to-Text Generation',\n",
       "  'abstract': 'In a language generation system, a content planner selects which elements must be included in the output text and the ordering between them. Recent empirical approaches perform content selection without any ordering and have thus no means to ensure that the output is coherent. In this paper we focus on the problem of generating text from a database and present a trainable end-to-end generation system that includes both content selection and ordering. Content plans are represented intuitively by a set of grammar rules that operate on the document level and are acquired automatically from training data. We develop two approaches: the first one is inspired from Rhetorical Structure Theory and represents the document as a tree of discourse relations between database records; the second one requires little linguistic sophistication and uses tree structures to represent global patterns of database record sequences within a document. Experimental evaluation on two domains yields considerable improvements over the state of the art for both approaches.',\n",
       "  'year': 2013},\n",
       " {'paperId': '33704e9942d3c5461dee5fd33cc051e493e58284',\n",
       "  'title': 'IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, Beijing, China, August 3-9, 2013',\n",
       "  'abstract': None,\n",
       "  'year': 2013},\n",
       " {'paperId': '41de65f718207cc5f98b561a62b16d5e818cb98c',\n",
       "  'title': 'A Global Model for Concept-to-Text Generation',\n",
       "  'abstract': 'Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. We present a joint model that captures content selection (\"what to say\") and surface realization (\"how to say\") in an unsupervised domain-independent fashion. Rather than breaking up the generation process into a sequence of local decisions, we define a probabilistic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). We recast generation as the task of finding the best derivation tree for a set of database records and describe an algorithm for decoding in this framework that allows to intersect the grammar with additional information capturing fluency and syntactic well-formedness constraints. Experimental evaluation on several domains achieves results competitive with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.',\n",
       "  'year': 2013},\n",
       " {'paperId': '4a6de89efc5da79c4aabccdb4737ebeedbea7ab2',\n",
       "  'title': 'Unsupervised Relation Extraction with General Domain Knowledge',\n",
       "  'abstract': 'Information extraction (IE) is becoming increasingly useful as a form of shallow semantic analysis. Learning relational facts from text is one of the core tasks of IE and has applications in a variety of fields including summarization, question answering, and information retrieval. Previous work has traditionally relied on extensive human involvement (e.g., hand-annotated training instances, manual pattern extraction rules, hand-picked seeds). Standard supervised techniques can yield high performance when large amounts of hand-labeled data are available for a fixed inventory of relation types, however, extraction systems do not easily generalize beyond their training domains and often must be re-engineered for each application. In this talk I will present an unsupervised approach to relational information extraction which could lead to significant resource savings and more portable extraction systems that require less engineering effort. The proposed model partitions tuples representing an observed syntactic relationship between two named entities (e.g., ‚ÄúX was born in Y‚Äù and ‚ÄúX is from Y‚Äù) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules. Specifically and automatically combine with we combine a topic model developed for the relation extraction task with automatically extracted domain relevant rules, and present an algorithm that estimates the parameters of this model. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.',\n",
       "  'year': 2013},\n",
       " {'paperId': '4adca62f888226d3a16654ca499bf2a7d3d11b71',\n",
       "  'title': 'Models of Semantic Representation with Visual Attributes',\n",
       "  'abstract': 'We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.',\n",
       "  'year': 2013},\n",
       " {'paperId': '7604a1aa300ffeb5e26d719536c428b7c76472eb',\n",
       "  'title': 'i, Poet: Chinese Poetry Composition through a Summarization Framework under Constrained Optimization',\n",
       "  'abstract': None,\n",
       "  'year': 2013},\n",
       " {'paperId': '9ce4c4c801a4cbbe98a19ecf355429d7c7938c89',\n",
       "  'title': 'Sixth International Joint Conference on Natural Language Processing',\n",
       "  'abstract': 'This paper deals with the fast bootstrapping of Grapheme-to-Phoneme (G2P) conversion system, which is a key module for both automatic speech recognition (ASR), and text-to-speech synthesis (TTS). The idea is to exploit language contact between a local dominant language (Malay) and a very under-resourced language (Iban spoken in Sarawak and in several parts of the Borneo Island) for which no resource nor knowledge is really available. More precisely, a pre-existing Malay G2P is used to produce phoneme sequences of Iban words. The phonemes are then manually post-edited (corrected) by an Iban native. This resource, which has been produced in a semi-supervised fashion, is later used to train the first G2P system for Iban language. As a by-product of this methodology, the analysis of the ‚Äúpronunciation distance‚Äù between Malay and Iban enlighten the phonological and orthographic relations between these two languages. The experiments conducted show that a rather efficient Iban G2P system can be obtained after only two hours of post-edition (correction) of the output of Malay G2P applied to Iban words.',\n",
       "  'year': 2013},\n",
       " {'paperId': 'a994ed8bac53bf7cf70d428d71cf1e2940fb4351',\n",
       "  'title': 'An abstractive approach to sentence compression',\n",
       "  'abstract': 'In this article we generalize the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present an experimental study showing that humans can naturally create abstractive sentences using a variety of rewrite operations, not just deletion. We next create a new corpus that is suited to the abstractive compression task and formulate a discriminative tree-to-tree transduction model that can account for structural and lexical mismatches. The model incorporates a grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression-specific loss functions.',\n",
       "  'year': 2013},\n",
       " {'paperId': 'ae791eeb50b7f737b573acc59e0be44fcfd56873',\n",
       "  'title': 'Semantic v.s. Positions: Utilizing Balanced Proximity in Language Model Smoothing for Information Retrieval',\n",
       "  'abstract': 'Work on information retrieval has shown that language model smoothing leads to more accurate estimation of document models and hence is crucial for achieving good retrieval performance. Several smoothing methods have been proposed in the literature, using either semantic or positional information. In this paper, we propose a unified proximity-based framework to smooth language models, leveraging semantic and positional information simultaneously in combination. The key idea is to project terms to positions where they originally do not exist (i.e., zero count), which is actually a word count propagation process. We achieve this projection through two proximity-based density functions indicating semantic association and positional adjacency. We balance the effects of semantic and positional smoothing, and score a document based on the smoothed language model. Experiments on four standard TREC test collections show that our smoothing model is effective for information retrieval and generally performs better than the state of the art.',\n",
       "  'year': 2013},\n",
       " {'paperId': 'b73eaf50663dbb1d54554935c2f7629e088dcc29',\n",
       "  'title': 'Explorer Models of Semantic Representation with Visual Attributes',\n",
       "  'abstract': 'We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.',\n",
       "  'year': 2013},\n",
       " {'paperId': 'c8fc59c884480ef94d961f7c971a86adbbdd6a4a',\n",
       "  'title': 'A Quantum-Theoretic Approach to Distributional Semantics',\n",
       "  'abstract': 'In this paper we explore the potential of quantum theory as a formal framework for capturing lexical meaning. We present a novel semantic space model that is syntactically aware, takes word order into account, and features key quantum aspects such as superposition and entanglement. We define a dependency-based Hilbert space and show how to represent the meaning of words by density matrices that encode dependency neighborhoods. Experiments on word similarity and association reveal that our model achieves results competitive with a variety of classical models.',\n",
       "  'year': 2013},\n",
       " {'paperId': 'd6e9cb9c0e62ce52c6cb3117396e88d532a4dfaa',\n",
       "  'title': 'Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA',\n",
       "  'abstract': None,\n",
       "  'year': 2013},\n",
       " {'paperId': 'dcbfae75824809636f854a21eaaf71dde1863586',\n",
       "  'title': 'Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL',\n",
       "  'abstract': None,\n",
       "  'year': 2013},\n",
       " {'paperId': 'e14b7e6ea0455671774891f9d90874d94085a01a',\n",
       "  'title': 'i, Poet: Automatic Chinese Poetry Composition through a Generative Summarization Framework under Constrained Optimization',\n",
       "  'abstract': 'Part of the long lasting cultural heritage of China is the classical ancient Chinese poems which follow strict formats and complicated linguistic rules. Automatic Chinese poetry composition by programs is considered as a challenging problem in computational linguistics and requires high Artificial Intelligence assistance, and has not been well addressed. In this paper, we formulate the poetry composition task as an optimization problem based on a generative summarization framework under several constraints. Given the user specified writing intents, the system retrieves candidate terms out of a large poem corpus, and then orders these terms to fit into poetry formats, satisfying tonal and rhythm requirements. The optimization process under constraints is conducted via iterative term substitutions till convergence, and outputs the subset with the highest utility as the generated poem. For experiments, we perform generation on large datasets of 61,960 classic poems from Tang and Song Dynasty of China. A comprehensive evaluation, using both human judgments and ROUGE scores, has demonstrated the effectiveness of our proposed approach.',\n",
       "  'year': 2013},\n",
       " {'paperId': '14acb33b78280a37268d472e7fc9dde68d218207',\n",
       "  'title': 'Tweet Recommendation with Graph Co-Ranking',\n",
       "  'abstract': 'As one of the most popular micro-blogging services, Twitter attracts millions of users, producing millions of tweets daily. Shared information through this service spreads faster than would have been possible with traditional sources, however the proliferation of user-generation content poses challenges to browsing and finding valuable information. In this paper we propose a graph-theoretic model for tweet recommendation that presents users with items they may have an interest in. Our model ranks tweets and their authors simultaneously using several networks: the social network connecting the users, the network connecting the tweets, and a third network that ties the two together. Tweet and author entities are ranked following a co-ranking algorithm based on the intuition that that there is a mutually reinforcing relationship between tweets and their authors that could be reflected in the rankings. We show that this framework can be parametrized to take into account user preferences, the popularity of tweets and their authors, and diversity. Experimental evaluation on a large dataset shows that our model outperforms competitive approaches by a large margin.',\n",
       "  'year': 2012},\n",
       " {'paperId': '16e07e07c7ca1d8dcbbd90d6ee0768221ae3a034',\n",
       "  'title': 'Semi-Supervised Semantic Role Labeling via Structural Alignment',\n",
       "  'abstract': 'Large-scale annotated corpora are a prerequisite to developing high-performance semantic role labeling systems. Unfortunately, such corpora are expensive to produce, limited in size, and may not be representative. Our work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning. The key idea of our approach is to find novel instances for classifier training based on their similarity to manually labeled seed instances. The underlying assumption is that sentences that are similar in their lexical material and syntactic structure are likely to share a frame semantic analysis. We formalize the detection of similar sentences and the projection of role annotations as a graph alignment problem, which we solve exactly using integer linear programming. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.',\n",
       "  'year': 2012},\n",
       " {'paperId': '6d9dc8cdfd8fc7d5fa54bc42aa4e97191a0832e3',\n",
       "  'title': 'Multiple Aspect Summarization Using Integer Linear Programming',\n",
       "  'abstract': 'Multi-document summarization involves many aspects of content selection and surface realization. The summaries must be informative, succinct, grammatical, and obey stylistic writing conventions. We present a method where such individual aspects are learned separately from data (without any hand-engineering) but optimized jointly using an integer linear programme. The ILP framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting, soft and hard constraints. Experimental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries.',\n",
       "  'year': 2012},\n",
       " {'paperId': '727ffaa7345748c62da164c15247bda205385856',\n",
       "  'title': 'Unsupervised Concept-to-text Generation with Hypergraphs',\n",
       "  'abstract': 'Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. We present a joint model that captures content selection (\"what to say\") and surface realization (\"how to say\") in an unsupervised domain-independent fashion. Rather than breaking up the generation process into a sequence of local decisions, we define a probabilistic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). We represent our grammar compactly as a weighted hypergraph and recast generation as the task of finding the best derivation tree for a given input. Experimental evaluation on several domains achieves competitive results with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.',\n",
       "  'year': 2012},\n",
       " {'paperId': '7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917',\n",
       "  'title': 'A Comparison of Vector-based Representations for Semantic Composition',\n",
       "  'abstract': 'In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.',\n",
       "  'year': 2012},\n",
       " {'paperId': '8a644eff44bb362b990f54f1981b3763660ecda6',\n",
       "  'title': 'Visualizing timelines: evolutionary summarization via iterative reinforcement between text and image streams',\n",
       "  'abstract': 'We present a novel graph-based framework for timeline summarization, the task of creating different summaries for different timestamps but for the same topic. Our work extends timeline summarization to a multimodal setting and creates timelines that are both textual and visual. Our approach exploits the fact that news documents are often accompanied by pictures and the two share some common content. Our model optimizes local summary creation and global timeline generation jointly following an iterative approach based on mutual reinforcement and co-ranking. In our algorithm, individual summaries are generated by taking into account the mutual dependencies between sentences and images, and are iteratively refined by considering how they contribute to the global timeline and its coherence. Experiments on real-world datasets show that the timelines produced by our model outperform several competitive baselines both in terms of ROUGE and when assessed by human evaluators.',\n",
       "  'year': 2012},\n",
       " {'paperId': 'a80e849db427ae4b2d78edd28e9b8db2fc16fa5a',\n",
       "  'title': 'The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, July 8-14, 2012, Jeju Island, Korea - Volume 1: Long Papers',\n",
       "  'abstract': None,\n",
       "  'year': 2012},\n",
       " {'paperId': 'c99b1ae451bdc9472f677b1973b6614d725ec342',\n",
       "  'title': 'Taxonomy Induction Using Hierarchical Random Graphs',\n",
       "  'abstract': 'This paper presents a novel approach for inducing lexical taxonomies automatically from text. We recast the learning problem as that of inferring a hierarchy from a graph whose nodes represent taxonomic terms and edges their degree of relatedness. Our model takes this graph representation as input and fits a taxonomy to it via combination of a maximum likelihood approach with a Monte Carlo Sampling algorithm. Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. We use our model to infer a taxonomy over 541 nouns and show that it outperforms popular flat and hierarchical clustering algorithms.',\n",
       "  'year': 2012},\n",
       " {'paperId': 'd4149dbef949644dad4833012e2def98529c0241',\n",
       "  'title': 'Grounded Models of Semantic Representation',\n",
       "  'abstract': 'A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.',\n",
       "  'year': 2012},\n",
       " {'paperId': 'e7ac9bb57cfc04058f6b66ee7aeafd7d21a1c70c',\n",
       "  'title': '2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies',\n",
       "  'abstract': None,\n",
       "  'year': 2012},\n",
       " {'paperId': 'f4a0efe1e69581e9f3bdcdbc6f29b9563fb5189e',\n",
       "  'title': 'Concept-to-text Generation via Discriminative Reranking',\n",
       "  'abstract': 'This paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. A key insight in our approach is to reduce the tasks of content selection (\"what to say\") and surface realization (\"how to say\") into a common parsing problem. We define a probabilistic context-free grammar that describes the structure of the input (a corpus of database records and text describing some of them) and represent it compactly as a weighted hypergraph. The hypergraph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. We propose a novel decoding algorithm for finding the best scoring derivation and generating in this setting. Experimental evaluation on the Atis domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study.',\n",
       "  'year': 2012},\n",
       " {'paperId': '082ec771aa3a9f4641a5cefe382c2a61aee540f1',\n",
       "  'title': 'Proceedings of the 33nd Annual Conference of the Cognitive Science Society',\n",
       "  'abstract': None,\n",
       "  'year': 2011},\n",
       " {'paperId': '188c73b075b665fe96d10a23f7045ce2dc714b33',\n",
       "  'title': 'Semi-supervised Semantic Role Labeling via Graph Alignment',\n",
       "  'abstract': 'Semantic roles, which constitute a shallow form of meaning representation, have attracted increasing interest in recent years. Various applications have been shown to benefit from this level of semantic analysis, and a large number of publications has addressed the problem of semantic role labeling, i.e., the task of automatically identifying semantic roles in arbitrary sentences. A major limiting factor for these approaches, however, is the need for large manually labeled semantic resources to train semantic role labeling systems in the supervised learning paradigm. Consequently, the application of such systems is still limited to the small number of languages and domains for which sufficiently large semantic resources are available. This thesis addresses the knowledge acquisition problem of semantic role labeling, i.e., the substantial annotation effort required for the creation of semantic resources that can be used to train state-of-the-art semantic role labeling systems. Our main contribution is to formulate a semi-supervised approach to semantic role labeling, which requires only a small manually labeled corpus of role-annotated sentences. This initial seed corpus is augmented with annotation instances generated automatically from a large unlabeled corpus. The augmented corpus is used as training data for a supervised role labeler, to improve labeling performance over what can be attained when training on the manually labeled sentences alone. Our approach therefore reduces the annotation effort required to attain satisfactory performance and thus alleviates the knowledge acquistion problem, especially for languages and domains where the cost of annotating large semantic resources is prohibitive. The key idea of our semi-supervised approach is to measure the similarity between labeled sentences from the manually annotated resource and sentences from a large unlabeled corpus. Similarity is conceptualized in terms of optimal graph alignments, which are employed to project annotations from labeled to unlabeled sentences. To select a set of novel training instances, similarity is operationalized as a measure of confidence, allowing us to limit the adverse effect of erroneous annotations. The optimization problem is formulated as an integer linear program and solved efficiently. The thesis broadly consists of two parts. In the theoretical part, our semi-supervised approach to semantic role labeling is described in detail.',\n",
       "  'year': 2011},\n",
       " {'paperId': '29ebd555754494357a7c0deea4ec078a21a5222e',\n",
       "  'title': 'Incremental Models of Natural Language Category Acquisition',\n",
       "  'abstract': 'Incremental Models of Natural Language Category Acquisition Trevor Fountain (t.fountain@sms.ed.ac.uk) Mirella Lapata (mlap@inf.ed.ac.uk) Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB, UK Abstract In this work, we concentrate on the task of acquiring nat- ural language (semantic) categories and examine how the statistics of the linguistic environment as approximated by large corpora influences category learning. Evidently, cate- gories are learned not only from exposure to the linguistic environment but also from our interaction with the physical world. Perhaps unsurprisingly, words that refer to concrete entities and actions are among the first words being learned as these are directly observable in the environment (Bornstein et al. 2004). Experimental evidence also shows that children respond to categories on the basis of visual features, e.g., they generalize object names to new objects often on the basis of similarity in shape and texture (Landau et al. 1998, Jones et al. 1991). Nevertheless, we focus on the acquisition of se- mantic categories from large text corpora based on the hy- pothesis that simple co-occurrence statistics can be used to capture word meaning quantitatively. The corpus-based ap- proach is attractive for modeling the development of linguis- tic categories. If simple distributional information really does form the basis of a word‚Äôs cognitive representation, this im- plies that learners are sensitive to the structure of the envi- ronment during language development. As experience with a word accumulates, more information about its contexts of use becomes encoded, with a corresponding increase in the abil- ity of the language learner to use the word appropriately and make inferences about novel words of the same category. The process of learning semantic categories is necessar- ily incremental. Human language acquisition is bounded by memory and procecessing limitations, and it is implausible that children process large amounts of linguistic input at once and induce an optimal set of categories. An incremental model learns as it is applied, meaning it does not require sep- arate training and testing phases. Behavioral evidence (Born- stein and Mash 2010) suggests that this scenario more closely mirrors the process by which infants acquire categories. Hav- ing this in mind, we formulate two incremental categorization models, each differing in the way they represent categories. Both models follow the exemplar tradition ‚Äî categories are denoted by a list of stored exemplars and inclusion of an un- known item in a category is determined by some notion of similarity between the item and the category exemplars. Pre- vious work (Voorspoels et al. 2008, Storms et al. 2000, Foun- tain and Lapata 2010) indicates that exemplar models perform consistently better across a broad range of natural language Learning categories from examples is a fundamental problem faced by the human cognitive system, and a long-standing topic of investigation in psychology. In this work we focus on the acquisition of natural language categories and exam- ine how the statistics of the linguistic environment influence category formation. We present two incremental models of category acquisition ‚Äî one probabilistic, one graph-based ‚Äî which encode different assumptions about how concepts are represented (i.e., as a set of topics or nodes in a graph). Eval- uation against gold-standard clusters and human performance in a category acquisition task suggests that the graph-based ap- proach is better suited at modeling the acquisition of natural language categories. Introduction The task of categorization, in which people cluster stimuli into categories and then use those categories to make in- ferences about novel stimuli, has long been a core problem within cognitive science. Understanding the mechanisms in- volved in categorization is essential, as the ability to gener- alize from experience underlies a variety of common mental tasks, including perception, learning, and the use of language. As a result, category learning has been one of the most ex- tensively studied aspects in human cognition, with compu- tational models that range from strict prototypes (categories are represented by a single idealized member which embod- ies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously en- countered members; e.g., Nosofsky 1988) or combinations of the two (e.g., Griffiths et al. 2007a). Historically, the stimuli involved in such studies tend to be either concrete objects with an unbounded number of features (e.g., physical objects; Bornstein and Mash 2010) or highly abstract, with a small number of manually specified features (e.g, binary strings, colored shapes; Medin and Schaffer 1978, Kruschke 1993). Furthermore, most existing models focus on adult categorization, i.e., it is assumed that a large number of categories have already been learned. A notable exception is Anderson‚Äôs (1991) rational model of categorization (see also Griffiths et al. 2007a) where it is assumed that the learner starts without any predefined categories and stimuli are clus- tered into groups as they come along. When a new stimulus is observed, it can either be assigned to one of the pre-existing clusters, or to a new cluster of its own.',\n",
       "  'year': 2011},\n",
       " {'paperId': '426025b638d6573a243ba4773c5b9e2aba18d7d9',\n",
       "  'title': 'Unsupervised Semantic Role Induction with Graph Partitioning',\n",
       "  'abstract': 'In this paper we present a method for unsupervised semantic role induction which we formalize as a graph partitioning problem. Argument instances of a verb are represented as vertices in a graph whose edge weights quantify their role-semantic similarity. Graph partitioning is realized with an algorithm that iteratively assigns vertices to clusters based on the cluster assignments of neighboring vertices. Our method is algorithmically and conceptually simple, especially with respect to how problem-specific knowledge is incorporated into the model. Experimental results on the CoNLL 2008 benchmark dataset demonstrate that our model is competitive with other unsupervised approaches in terms of F1 whilst attaining significantly higher cluster purity.',\n",
       "  'year': 2011},\n",
       " {'paperId': '4ac54dc1d1d29955e26df3ed3a7106c0b18dcf69',\n",
       "  'title': 'The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA - Student Session',\n",
       "  'abstract': None,\n",
       "  'year': 2011},\n",
       " {'paperId': '5a8c0c6247225d5201c2eb75a36df963d3471eb5',\n",
       "  'title': 'The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA - System Demonstrations',\n",
       "  'abstract': None,\n",
       "  'year': 2011},\n",
       " {'paperId': '724eeeed58dfa329078bc7fc2201aad7b82cd5e3',\n",
       "  'title': 'Table of Contents (linear) Maps of the Impossible: Capturing Semantic Anomalies in Distributional Space Shared Task System Description: Frustratingly Hard Compositionality Prediction Shared Task System Description: Measuring the Compositionality of Bigrams Using Statistical Method- Ologies Detecting',\n",
       "  'abstract': 'Introduction Any NLP system that does semantic processing relies on the assumption of semantic compositionality: the meaning of a phrase is determined by the meanings of its parts and their combination. For this, it is necessary to have automatic methods that are capable to reproduce the compositionality of language. Recent years have shown the renaissance of interest in distributional semantics. While distributional methods in semantics have proven to be very efficient in tackling a wide range of tasks in natural and many others, they are still strongly limited by being inherently word-based. The main hurdle for vector space models to further progress is the ability to handle compositionality. The workshop is of potential interest to the researchers working on distributional semantics and compositionality as well as for those interested in extracting non-compositional phrases from large corpora by applying distributional methods that assign a graded compositionality score to a phrase. This score denotes the extent to which the compositionality assumption holds for a given expression. The latter can be used, for example, to decide whether the phrase should be treated as a single unit in applications or included in a dictionary. We have emphasized that the focus is on automatically acquiring semantic compositionality, thereby explicitly avoiding approaches that employ prefabricated lists of non-compositional phrases. The workshop consists of a main session and a shared task. To the best of our knowledge, this has been the first attempt in the community to offer a dataset and a shared task that allows to explicitly evaluate the models of graded compositionality for phrases per se that occur in three types of grammatical relations: adjective-noun pairs, subject-verb and verb-object pairs in English and German. For the main session, one long and two short papers have been accepted for publication. Further, seven teams with 19 systems have taken part in the shared task. We consider this a success, taking into consideration that the task is new and difficult. The description of the task and the results of evaluation are part of these proceedings. In short, approaches ranging from pure statistical association measures to various variations of word space models have been applied to solve the DiSCo task. Six system description papers have been accepted for publication. Both regular and system description papers have been carefully reviewed by the program committee. We would like to thank the committee for insightful and timely reviews (in spite of the ‚Ä¶',\n",
       "  'year': 2011},\n",
       " {'paperId': 'cadf27667b0b37baaf96dd3b05d7559df2945cdd',\n",
       "  'title': 'Proceedings of the National Conference on Artificial Intelligence',\n",
       "  'abstract': None,\n",
       "  'year': 2011},\n",
       " {'paperId': 'da47c8256aca536ffd7e59212f7c7a3361c9ff73',\n",
       "  'title': 'Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming',\n",
       "  'abstract': 'Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. This paper presents a data-driven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simplifications that significantly reduce the reading difficulty of the input, while maintaining grammaticality and preserving its meaning.',\n",
       "  'year': 2011},\n",
       " {'paperId': 'e4c71fd504fd6657fc444e82e481b22f952bcaab',\n",
       "  'title': 'WikiSimple: Automatic Simplification of Wikipedia Articles',\n",
       "  'abstract': 'Text simplification aims to rewrite text into simpler versions and thus make information accessible to a broader audience (e.g., non-native speakers, children, and individuals with language impairments). In this paper, we propose a model that simplifies documents automatically while selecting their most important content and rewriting them in a simpler style. We learn content selection rules from same-topic Wikipedia articles written in the main encyclopedia and its Simple English variant. We also use the revision histories of Simple Wikipedia articles to learn a quasi-synchronous grammar of simplification rewrite rules. Based on an integer linear programming formulation, we develop a joint model where preferences based on content and style are optimized simultaneously. Experiments on simplifying main Wikipedia articles show that our method significantly reduces the reading difficulty, while still capturing the important content.',\n",
       "  'year': 2011},\n",
       " {'paperId': 'ed73c8b844f9bc19170fd904c30c402fe215b9b1',\n",
       "  'title': 'Unsupervised Semantic Role Induction via Split-Merge Clustering',\n",
       "  'abstract': 'In this paper we describe an unsupervised method for semantic role induction which holds promise for relieving the data acquisition bottleneck associated with supervised role labelers. We present an algorithm that iteratively splits and merges clusters representing semantic roles, thereby leading from an initial clustering to a final clustering of better quality. The method is simple, surprisingly effective, and allows to integrate linguistic knowledge transparently. By combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system. Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin.',\n",
       "  'year': 2011},\n",
       " {'paperId': 'fdf0b3f10770b48330939d1714e31b0c0ebee94d',\n",
       "  'title': 'Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing',\n",
       "  'abstract': None,\n",
       "  'year': 2011},\n",
       " {'paperId': '2d84082b1b86be38decd388f955cd9a884e0311f',\n",
       "  'title': 'Visual Information in Semantic Representation',\n",
       "  'abstract': 'The question of how meaning might be acquired by young children and represented by adult speakers of a language is one of the most debated topics in cognitive science. Existing semantic representation models are primarily amodal based on information provided by the linguistic input despite ample evidence indicating that the cognitive system is also sensitive to perceptual information. In this work we exploit the vast resource of images and associated documents available on the web and develop a model of multimodal meaning representation which is based on the linguistic and visual context. Experimental results show that a closer correspondence to human data can be obtained by taking the visual modality into account.',\n",
       "  'year': 2010},\n",
       " {'paperId': '405bd9bd7d17ae18a69d34bb4057ff5e498bb439',\n",
       "  'title': 'Proceedings of the Fourteenth Conference on Computational Natural Language Learning',\n",
       "  'abstract': 'The 2010 Conference on Computational Natural Language Learning is the fourteenth in the series of annual meetings organized by SIGNLL, the ACL special interest group on natural language learning. CONLL-2010 will be held in Uppsala, Sweden, 15-16 July 2010, in conjunction with ACL. \\n \\nFor our special focus this year in the main session of CoNLL, we invited papers relating to grammar induction, from a machine learning, natural language engineering and cognitive perspective. We received 99 submissions on these and other relevant topics, of which 18 were eventually withdrawn. Of the remaining 81 papers, 12 were selected to appear in the conference programme as oral presentations, and 13 were chosen as posters. All accepted papers appear here in the proceedings. Following the ACL 2010 policy we allowed an extra page in the camera ready paper for authors to incorporate reviewer comments, so each accepted paper was allowed to have nine pages plus any number of pages containing only bibliographic references. \\n \\nAs in previous years, CoNLL-2010 has a shared task, Learning to detect hedges and their scope in natural language text. The Shared Task papers are collected into an accompanying volume of CoNLL-2010.',\n",
       "  'year': 2010},\n",
       " {'paperId': '501363eb1c55d30e6151d99269afc5aa31f8a0c8',\n",
       "  'title': 'Plot Induction and Evolutionary Search for Story Generation',\n",
       "  'abstract': \"In this paper we develop a story generator that leverages knowledge inherent in corpora without requiring extensive manual involvement. A key feature in our approach is the reliance on a story planner which we acquire automatically by recording events, their participants, and their precedence relationships in a training corpus. Contrary to previous work our system does not follow a generate-and-rank architecture. Instead, we employ evolutionary search techniques to explore the space of possible stories which we argue are well suited to the story generation task. Experiments on generating simple children's stories show that our system outperforms previous data-driven approaches.\",\n",
       "  'year': 2010},\n",
       " {'paperId': '5974441a0bebfb45579491a9a28bca4fff6bc256',\n",
       "  'title': 'Measuring Distributional Similarity in Context',\n",
       "  'abstract': 'The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks ranging from the acquisition of synonyms and paraphrases to word sense disambiguation and textual entailment. Vector-based models are typically directed at representing words in isolation and thus best suited for measuring similarity out of context. In his paper we propose a probabilistic framework for measuring similarity in context. Central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models.',\n",
       "  'year': 2010},\n",
       " {'paperId': '5c62d0bfbaa80855196ca558c1d187221e258375',\n",
       "  'title': 'Incremental Models of Natural Language Categorization',\n",
       "  'abstract': None,\n",
       "  'year': 2010},\n",
       " {'paperId': '60be767a255fd13f73ed4e64d9901b30cf6081e8',\n",
       "  'title': 'Topic Models for Image Annotation and Text Illustration',\n",
       "  'abstract': 'Image annotation, the task of automatically generating description words for a picture, is a key component in various image search and retrieval applications. Creating image databases for model development is, however, costly and time consuming, since the keywords must be hand-coded and the process repeated for new collections. In this work we exploit the vast resource of images and documents available on the web for developing image annotation models without any human involvement. We describe a probabilistic model based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics. We show that this model outperforms previously proposed approaches when applied to image annotation and the related task of text illustration despite the noisy nature of our dataset.',\n",
       "  'year': 2010},\n",
       " {'paperId': '6a3a52ac09f18b36cee752665d82ef3bc868a229',\n",
       "  'title': 'Reviewers for Volume 36',\n",
       "  'abstract': 'This journal has a knowledgeable and hard working editorial board, listed on the inside front cover of each issue, but for most submissions we also enlist the aid of specialist reviewers outside the editorial board. The Editor of Computational Linguistics would like to express his gratitude to the external reviewers listed below, who anonymously reviewed papers for the journal during the preparation of this volume (Volume 36). Their generosity, judicious judgment, and prompt response substantially helped us to publish a journal that both is timely and maintains exacting scientific standards; it is a genuine pleasure to thank them collectively for their dedicated service.',\n",
       "  'year': 2010},\n",
       " {'paperId': '6c95908d08323c5acfc5bfdf7399f31563ade4f5',\n",
       "  'title': 'Discourse Constraints for Document Compression',\n",
       "  'abstract': 'Sentence compression holds promise for many applications ranging from summarization to subtitle generation. The task is typically performed on isolated sentences without taking the surrounding context into account, even though most applications would operate over entire documents. In this article we present a discourse-informed model which is capable of producing document compressions that are coherent and informative. Our model is inspired by theories of local coherence and formulated within the framework of integer linear programming. Experimental results show significant improvements over a state-of-the-art discourse agnostic approach.',\n",
       "  'year': 2010},\n",
       " {'paperId': '7453a974d355883f342aaa6e29eb86a13edcedb9',\n",
       "  'title': 'Automatic Generation of Story Highlights',\n",
       "  'abstract': 'In this paper we present a joint content selection and compression model for single-document summarization. The model operates over a phrase-based representation of the source document which we obtain by merging information from PCFG parse trees and dependency graphs. Using an integer linear programming formulation, the model learns to select and combine phrases subject to length, coverage and grammar constraints. We evaluate the approach on the task of generating \"story highlights\"---a small number of brief, self-contained sentences that allow readers to quickly gather information on news stories. Experimental results show that the model\\'s output is comparable to human-written highlights in terms of both grammaticality and content.',\n",
       "  'year': 2010},\n",
       " {'paperId': '745d86adca56ec50761591733e157f84cfb19671',\n",
       "  'title': 'Composition in Distributional Models of Semantics',\n",
       "  'abstract': 'Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words. This article proposes a framework for representing the meaning of word combinations in vector space. Central to our approach is vector composition, which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models that we evaluate empirically on a phrase similarity task.',\n",
       "  'year': 2010},\n",
       " {'paperId': '7b77c3bc55e5f448b557a041d3b1b029dc9d930f',\n",
       "  'title': 'Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 2-4, 2010, Los Angeles, California, USA',\n",
       "  'abstract': None,\n",
       "  'year': 2010},\n",
       " {'paperId': '93a8f38237e7326aca0f596264e45e0360d81e9a',\n",
       "  'title': 'Explorer Visual Information in Semantic Representation',\n",
       "  'abstract': 'The question of how meaning might be acquired by young children and represented by adult speakers of a language is one of the most debated topics in cognitive science. Existing semantic representation models are primarily amodal based on information provided by the linguistic input despite ample evidence indicating that the cognitive system is also sensitive to perceptual information. In this work we exploit the vast resource of images and associated documents available on the web and develop a model of multimodal meaning representation which is based on the linguistic and visual context. Experimental results show that a closer correspondence to human data can be obtained by taking the visual modality into account.',\n",
       "  'year': 2010},\n",
       " {'paperId': '9824df37db10479e1784bacafb3b34db760318a4',\n",
       "  'title': 'Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure',\n",
       "  'abstract': 'The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.',\n",
       "  'year': 2010},\n",
       " {'paperId': 'b6103f9f466e23c4897f47b4cc427fae29d6333d',\n",
       "  'title': 'Title Generation with Quasi-Synchronous Grammar',\n",
       "  'abstract': 'The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as paraphrases and compressions. Based on an integer linear programming formulation, the model learns to generate summaries that satisfy both types of preferences, while ensuring that length, topic coverage and grammar constraints are met. Experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modifications.',\n",
       "  'year': 2010},\n",
       " {'paperId': 'c8b7e13a5d0c13dfde17c16f9cad2d50b442dba1',\n",
       "  'title': 'How Many Words Is a Picture Worth? Automatic Caption Generation for News Images',\n",
       "  'abstract': 'In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.',\n",
       "  'year': 2010},\n",
       " {'paperId': 'd38f560e324b58da2fc556818050eb506bbd9c8d',\n",
       "  'title': 'COLING 2010, 23rd International Conference on Computational Linguistics, Posters Volume, 23-27 August 2010, Beijing, China',\n",
       "  'abstract': None,\n",
       "  'year': 2010},\n",
       " {'paperId': 'de8fabadadb6a1ea15815322642457878897b241',\n",
       "  'title': 'Image and Natural Language Processing for Multimedia Information Retrieval',\n",
       "  'abstract': 'Image annotation, the task of automatically generating description words for a picture, is a key component in various image search and retrieval applications. Creating image databases for model development is, however, costly and time consuming, since the keywords must be hand-coded and the process repeated for new collections. In this work we exploit the vast resource of images and documents available on the web for developing image annotation models without any human involvement. We describe a probabilistic framework based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics. Applications of this framework to image annotation and retrieval show performance gains over previously proposed approaches, despite the noisy nature of our dataset. We also discuss how the proposed model can be used for story picturing, i.e., to find images that appropriately illustrate a text and demonstrate its utility when interfaced with an image caption generator.',\n",
       "  'year': 2010},\n",
       " {'paperId': 'e6066a0e23f2fde6908b6fffcbd5e598cb443f7d',\n",
       "  'title': 'An Experimental Study of Graph Connectivity for Unsupervised Word Sense Disambiguation',\n",
       "  'abstract': 'Word sense disambiguation (WSD), the task of identifying the intended meanings (senses) of words in context, has been a long-standing research objective for natural language processing. In this paper, we are concerned with graph-based algorithms for large-scale WSD. Under this framework, finding the right sense for a given word amounts to identifying the most ¬øimportant¬ø node among the set of graph nodes representing its senses. We introduce a graph-based WSD algorithm which has few parameters and does not require sense-annotated data for training. Using this algorithm, we investigate several measures of graph connectivity with the aim of identifying those best suited for WSD. We also examine how the chosen lexicon and its connectivity influences WSD performance. We report results on standard data sets and show that our graph-based approach performs comparably to the state of the art.',\n",
       "  'year': 2010},\n",
       " {'paperId': 'eb987adbfdac2002633c8a0e8808565b027c091a',\n",
       "  'title': 'Meaning Representation in Natural Language Categorization',\n",
       "  'abstract': 'Meaning Representation in Natural Language Categorization Trevor Fountain (t.fountain@sms.ed.ac.uk) and Mirella Lapata (mlap@inf.ed.ac.uk) School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB, UK Abstract A large number of formal models of categorization have been proposed in recent years. Many of these are tested on artificial categories or perceptual stimuli. In this paper we focus on cat- egorization models for natural language concepts and specif- ically address the question of how these may be represented. Many psychological theories of semantic cognition assume that concepts are defined by features which are commonly elicited from humans. Norming studies yield detailed knowl- edge about meaning representations, however they are small- scale (features are obtained for a few hundred words), and ad- mittedly of limited use for a general model of natural language categorization. As an alternative we investigate whether cate- gory meanings may be represented quantitatively in terms of simple co-occurrence statistics extracted from large text col- lections. Experimental comparisons of feature-based catego- rization models against models based on data-driven represen- tations indicate that the latter represent a viable alternative to the feature norms typically used. Introduction Considerable psychological research has shown that people reason about novel objects they encounter by identifying the category to which these objects belong and extrapolating from their past experiences with other members of that cat- egory. This task of categorization, or grouping objects into meaningful categories, is a classic problem in the field of cog- nitive science, one with a history of study dating back to Aris- totle. This is hardly surprising, as the ability to reason about categories is central to a multitude of other tasks, including perception, learning, and the use of language. Numerous theories exist as to how humans categorize ob- jects. These theories themselves tend to belong to one of three schools of thought. In the classical (or Aristotelian) view cat- egories are defined by a list of ‚Äúnecessary and sufficient‚Äù features. For example, the defining features for the concept BACHELOR might be male, single, and adult. Unfortunately, this approach is unable to account for most ordinary usage of categories, as many real-world objects have a somewhat fuzzy definition and don‚Äôt fit neatly into well-defined cate- gories (Smith and Medin, 1981). Prototype theory (Rosch, 1973) presents an alternative for- mulation of this idea, in which categories are defined by an idealized prototypical member possessing the features which are critical to the category. Objects are deemed to be members of the category if they exhibit enough of these features; for example, the characteristic features of FRUIT might include contains seeds, grows above ground, and is edible. Roughly speaking, prototype theory differs from the classical theory in that members of the category are not required to possess all of the features specified in the prototype. Although prototype theory provides a superior and work- able alternative to the classical theory it has been challenged by the exemplar approach (Medin and Schaffer, 1978). In this view, categories are defined not by a single representation but rather by a list of previously encountered members. Instead of maintaining a single prototype for FRUIT that lists the fea- tures typical of fruits, an exemplar model simply stores those instances of fruit to which it has been exposed (e.g., apples, oranges, pears). A new object is grouped into the category if it is sufficiently similar to one or more of the FRUIT instances stored in memory. In the past much experimental work has tested the predic- tions of prototype- and exemplar-based theories in laboratory studies involving categorization and category learning. These experiments tend to use perceptual stimuli and artificial cat- egories (e.g., strings of digit sequences such as 100000 or 0111111). Analogously, much modeling work has focused on the questions of how categories and stimuli can be rep- resented (Griffiths et al., 2007a; Sanborn et al., 2006) and how best to formalize similarity. The latter plays an impor- tant role in both prototype and exemplar models as correct generalization to new objects depends on identifying previ- ously encountered items correctly. In this paper we focus on the less studied problem of cat- egorization of natural language concepts. In contrast to the numerous studies using perceptual stimuli or artificial cate- gories, there is surprisingly little work on how natural lan- guage categories are learned or used by adult speakers. A few notable exceptions are Heit and Barsalou (1996) who attempt to experimentally test an exemplar model within the context of natural language concepts, Storms et al. (2000) who eval- uate the differences in performance between exemplar and prototype models on a number of natural categorization tasks, and Voorspoels et al. (2008) who model typicality ratings for natural language concepts. A common assumption underly- ing this work is that the meaning of the concepts involved in categorization can be represented by a set of features (also referred to as properties or attributes). Indeed, featural representations have played a central role in psychological theories of semantic cognition and knowl- edge organization and many studies have been conducted to elicit detailed knowledge of features. In a typical procedure, participants are given a series of object names and for each object they are asked to name all the properties they can think of that are characteristic of the object. Although fea- ture norms are often interpreted as a useful proxy of the struc- ture of semantic representations, a number of difficulties arise',\n",
       "  'year': 2010},\n",
       " {'paperId': 'ef0c96de44933f40aabbb921264796187fec87bd',\n",
       "  'title': 'Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP 2010, 9-11 October 2010, MIT Stata Center, Massachusetts, USA, A meeting of SIGDAT, a Special Interest Group of the ACL',\n",
       "  'abstract': None,\n",
       "  'year': 2010},\n",
       " {'paperId': 'efec2d8dd197dccf6916aa0b69556a66bbb6bac8',\n",
       "  'title': 'Topic Models for Meaning Similarity in Context',\n",
       "  'abstract': 'Recent work on distributional methods for similarity focuses on using the context in which a target word occurs to derive context-sensitive similarity computations. In this paper we present a method for computing similarity which builds vector representations for words in context by modeling senses as latent variables in a large corpus. We apply this to the Lexical Substitution Task and we show that our model significantly outperforms typical distributional methods.',\n",
       "  'year': 2010},\n",
       " {'paperId': 'fa7c9aa119913bb7cafb61c2d55fdc117c2bb4fa',\n",
       "  'title': 'Unsupervised Induction of Semantic Roles',\n",
       "  'abstract': 'Datasets annotated with semantic roles are an important prerequisite to developing high-performance role labeling systems. Unfortunately, the reliance on manual annotations, which are both difficult and highly expensive to produce, presents a major obstacle to the widespread application of these systems across different languages and text genres. In this paper we describe a method for inducing the semantic roles of verbal arguments directly from unannotated text. We formulate the role induction problem as one of detecting alternations and finding a canonical syntactic form for them. Both steps are implemented in a novel probabilistic model, a latent-variable variant of the logistic classifier. Our method increases the purity of the induced role clusters by a wide margin over a strong baseline.',\n",
       "  'year': 2010},\n",
       " {'paperId': '0ad53086d1ae47ceacc0ad0287f5fef14d3fc4bf',\n",
       "  'title': 'Learning to Tell Tales: A Data-driven Approach to Story Generation',\n",
       "  'abstract': \"Computational story telling has sparked great interest in artificial intelligence, partly because of its relevance to educational and gaming applications. Traditionally, story generators rely on a large repository of background knowledge containing information about the story plot and its characters. This information is detailed and usually hand crafted. In this paper we propose a data-driven approach for generating short children's stories that does not require extensive manual involvement. We create an end-to-end system that realizes the various components of the generation pipeline stochastically. Our system follows a generate-and-and-rank approach where the space of multiple candidate stories is pruned by considering whether they are plausible, interesting, and coherent.\",\n",
       "  'year': 2009},\n",
       " {'paperId': '3e4ddaef990a072da03864ff6750dba7a229e620',\n",
       "  'title': 'Invited talk: Constraint-based Sentence Compression: An Integer Programming Approach',\n",
       "  'abstract': 'In this talk we introduce the sentence compression task, which can be viewed as producing a summary of a single sentence. An ideal compression algorithm should produce a shorter version of an original sentence that retains the most important information while remaining grammatical. The task has an immediate impact on several applications ranging from document summarisation to audio scanning devices for the blind and caption generation. Previous approaches have primarily relied on parallel corpora to determine what is important in a sentence. These include data intensive methods inspired from machine translation using the noisy-channel model and from parsing by treating compression as a series of tree rewriting operations. Our work views sentence compression as an optimisation problem. We develop an integer programming formulation and infer globally optimal compressions in the face of linguistically motivated constraints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or large-scale. The proposed approach yields results comparable and in some cases superior to state-of-the-art. Proceedings of the 19th Meeting of Computational Linguistics in the Netherlands Edited by: Barbara Plank, Erik Tjong Kim Sang and Tim Van de Cruys. Copyright c ¬©2009 by the individual authors.',\n",
       "  'year': 2009},\n",
       " {'paperId': '603336f4b24ade85cba363b4815916ac0611fb20',\n",
       "  'title': 'Bayesian Word Sense Induction',\n",
       "  'abstract': \"Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word's contexts into different classes, each representing a word sense. Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical co-occurrences and to systematically assess their utility on the sense induction task. The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.\",\n",
       "  'year': 2009},\n",
       " {'paperId': '60f1cf5b596183dec1825088ae1925cb1e8d0d94',\n",
       "  'title': 'Cross-lingual Annotation Projection for Semantic Roles',\n",
       "  'abstract': 'This article considers the task of automatically inducing role-semantic annotations in the FrameNet paradigm for new languages. We propose a general framework that is based on annotation projection, phrased as a graph optimization problem. It is relatively inexpensive and has the potential to reduce the human effort involved in creating role-semantic resources. Within this framework, we present projection models that exploit lexical and syntactic information. We provide an experimental evaluation on an English-German parallel corpus which demonstrates the feasibility of inducing high-precision German semantic role annotation both for manually and automatically annotated English data.',\n",
       "  'year': 2009},\n",
       " {'paperId': '6f9a0d28ff735723530521de606dac59bfb5ded3',\n",
       "  'title': 'Explorer Semi-Supervised Semantic Role Labeling',\n",
       "  'abstract': 'Large scale annotated corpora are prerequisite to developing high-performance semantic role labeling systems. Unfortunately, such corpora are expensive to produce, limited in size, and may not be representative. Our work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning. Our algorithm augments a small number of manually labeled instances with unlabeled examples whose roles are inferred automatically via annotation projection. We formulate the projection task as a generalization of the linear assignment problem. We seek to find a role assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.',\n",
       "  'year': 2009},\n",
       " {'paperId': 'a8d5adf80fc33c2450e6eccc465ad2134e0beb8a',\n",
       "  'title': 'Explorer Bayesian Word Sense Induction',\n",
       "  'abstract': 'Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word‚Äôs contexts into different classes, each representing a word sense. Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task. The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.',\n",
       "  'year': 2009},\n",
       " {'paperId': 'bae866593a978d3f871d1bb2b2ba57ede8fe3dd5',\n",
       "  'title': 'Proceedings of the 12th Conference of the European Chapter of the ACL',\n",
       "  'abstract': None,\n",
       "  'year': 2009},\n",
       " {'paperId': 'c73e0c7b718788414d7349cb6283d10adf344aaa',\n",
       "  'title': 'Semi-Supervised Semantic Role Labeling',\n",
       "  'abstract': 'Large scale annotated corpora are prerequisite to developing high-performance semantic role labeling systems. Unfortunately, such corpora are expensive to produce, limited in size, and may not be representative. Our work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning. Our algorithm augments a small number of manually labeled instances with unlabeled examples whose roles are inferred automatically via annotation projection. We formulate the projection task as a generalization of the linear assignment problem. We seek to find a role assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.',\n",
       "  'year': 2009},\n",
       " {'paperId': 'd8346039cef65f121f3fa628869ecda5f79ddfee',\n",
       "  'title': 'Sentence Compression as Tree Transduction',\n",
       "  'abstract': 'This paper presents a tree-to-tree transduction method for sentence compression. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring significant improvements over a state-of-the-art model.',\n",
       "  'year': 2009},\n",
       " {'paperId': 'e420f393bc23839a65a1a32778026bb6eae25fa2',\n",
       "  'title': 'Language Models Based on Semantic Composition',\n",
       "  'abstract': 'In this paper we propose a novel statistical language model to capture long-range semantic dependencies. Specifically, we apply the concept of semantic composition to the problem of constructing predictive history representations for upcoming words. We also examine the influence of the underlying semantic space on the composition task by comparing spatial semantic representations against topic-based ones. The composition models yield reductions in perplexity when combined with a standard n-gram language model over the n-gram model alone. We also obtain perplexity reductions when integrating our models with a structured language model.',\n",
       "  'year': 2009},\n",
       " {'paperId': 'f16b11b71cbda7229ee84706b1ae140e9ac8f0b0',\n",
       "  'title': 'Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP 2009, 6-7 August 2009, Singapore, A meeting of SIGDAT, a Special Interest Group of the ACL',\n",
       "  'abstract': None,\n",
       "  'year': 2009},\n",
       " {'paperId': 'f46ecf9530e1a135c9e03e4f4dd72d826dedb05f',\n",
       "  'title': 'Graph Alignment for Semi-Supervised Semantic Role Labeling',\n",
       "  'abstract': 'Unknown lexical items present a major obstacle to the development of broad-coverage semantic role labeling systems. We address this problem with a semi-supervised learning approach which acquires training instances for unseen verbs from an unlabeled corpus. Our method relies on the hypothesis that unknown lexical items will be structurally and semantically similar to known items for which annotations are available. Accordingly, we represent known and unknown sentences as graphs, formalize the search for the most similar verb as a graph alignment problem and solve the optimization using integer linear programming. Experimental results show that role labeling performance for unknown lexical items improves with training data produced automatically by our method.',\n",
       "  'year': 2009},\n",
       " {'paperId': '0014ceb65f27ecf92ed60066afa13e71d2c900cd',\n",
       "  'title': 'Good Neighbors Make Good Senses: Exploiting Distributional Similarity for Unsupervised WSD',\n",
       "  'abstract': 'We present an automatic method for senselabeling of text in an unsupervised manner. The method makes use of distributionally similar words to derive an automatically labeled training set, which is then used to train a standard supervised classifier for distinguishing word senses. Experimental results on the Senseval-2 and Senseval-3 datasets show that our approach yields significant improvements over state-of-the-art unsupervised methods, and is competitive with supervised ones, while eliminating the annotation cost.',\n",
       "  'year': 2008},\n",
       " {'paperId': '02b3e1a74d6f09a467823ade1460fb950b7f609b',\n",
       "  'title': 'Proceedings of the Conference on Empirical Methods in Natural Language Processing',\n",
       "  'abstract': \"Welcome to the 2008 Conference on Empirical Methods in Natural Language Processing! The conference is organized under the auspices of SIGDAT, the ACL Special Interest Group for linguistic data and corpus-based approaches to natural language processing. It is co-located this year with AMTA 2008 and the International Workshop on Spoken Language Translation, in Honolulu, Hawaii. \\n \\nEMNLP received 385 submissions. We were able to accept 116 papers in total (an acceptance rate of 30%). 81 of the papers (21%) were accepted for oral presentation, and 35 (9%) for poster presentation. Two poster papers were subsequently withdrawn after acceptance. The papers were selected by a program committee of 15 area chairs, from Asia, Europe, and North America, assisted by a panel of 339 reviewers. This year EMNLP introduced an author response period. Authors were able to read and respond to the reviews of their paper before the program committee made a final decision. They were asked to correct factual errors in the reviews and answer questions raised in the reviewer comments. The intention was to help produce more accurate reviews. In some cases, reviewers changed their scores in view of the authors' response and the area chairs read all responses carefully prior to making recommendations for acceptance.\",\n",
       "  'year': 2008},\n",
       " {'paperId': '1be546bb78e087334a4dcf365874021d8e134104',\n",
       "  'title': 'Reviewers for Volume 34',\n",
       "  'abstract': 'This journal has a knowledgeable and hard working editorial board, listed on the inside front cover of each issue, but for most submissions we also enlist the aid of specialist reviewers outside the editorial board. The Editor of Computational Linguistics would like to express his gratitude to the external reviewers listed below, who anonymously reviewed papers for the journal during the preparation of this volume (Volume 34). Their generosity, judicious review, and prompt response substantially helped us to publish a journal that both is timely and maintains exacting scientific standards; it is a genuine pleasure to thank them collectively for their dedicated service.',\n",
       "  'year': 2008},\n",
       " {'paperId': '1ec86811a79fb02a1c551b8f418314a00f5f5a99',\n",
       "  'title': 'Global inference for sentence compression : an integer linear programming approach',\n",
       "  'abstract': 'Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models.',\n",
       "  'year': 2008},\n",
       " {'paperId': '2ec0657cf7b8c641ad61530c181f15d68ce7ff6a',\n",
       "  'title': 'Explorer Modeling Local Coherence : An Entity-Based Approach',\n",
       "  'abstract': 'General rights Copyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s) and / or other copyright owners and it is a condition of accessing these publications that users recognise and abide by the legal requirements associated with these rights. Take down policy The University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorer content complies with UK legislation. If you believe that the public display of this file breaches copyright please contact openaccess@ed.ac.uk providing details, and we will remove access to the work immediately and investigate your claim. This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.',\n",
       "  'year': 2008},\n",
       " {'paperId': '31d648cfc29ffad86b3d50f08c22ecc6d4edfd0e',\n",
       "  'title': 'Constructing Corpora for the Development and Evaluation of Paraphrase Systems',\n",
       "  'abstract': 'Automatic paraphrasing is an important component in many natural language processing tasks. In this article we present a new parallel corpus with paraphrase annotations. We adopt a definition of paraphrase based on word alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure.',\n",
       "  'year': 2008},\n",
       " {'paperId': '5e3a06a47ef1757cee4952b6c7510804ff6dfb87',\n",
       "  'title': 'ParaMetric: An Automatic Evaluation Metric for Paraphrasing',\n",
       "  'abstract': 'We present ParaMetric, an automatic evaluation metric for data-driven approaches to paraphrasing. ParaMetric provides an objective measure of quality using a collection of multiple translations whose paraphrases have been manually annotated. ParaMetric calculates precision and recall scores by comparing the paraphrases discovered by automatic paraphrasing techniques against gold standard alignments of words and phrases within equivalent sentences. We report scores for several established paraphrasing techniques.',\n",
       "  'year': 2008},\n",
       " {'paperId': '8f276cc334b0bf48bbcd667cc48b069eb8fcdadf',\n",
       "  'title': 'Evidence for serial coercion: A time course analysis using the visual-world paradigm',\n",
       "  'abstract': \"Metonymic verbs like start or enjoy often occur with artifact-denoting complements (e.g., The artist started the picture) although semantically they require event-denoting complements (e.g., The artist started painting the picture). In case of artifact-denoting objects, the complement is assumed to be type shifted (or coerced) into an event to conform to the verb's semantic restrictions. Psycholinguistic research has provided evidence for this kind of enriched composition: readers experience processing difficulty when faced with metonymic constructions compared to non-metonymic controls. However, slower reading times for metonymic constructions could also be due to competition between multiple interpretations that are being entertained in parallel whenever a metonymic verb is encountered. Using the visual-world paradigm, we devised an experiment which enabled us to determine the time course of metonymic interpretation in relation to non-metonymic controls. The experiment provided evidence in favor of a non-competitive, serial coercion process.\",\n",
       "  'year': 2008},\n",
       " {'paperId': '9c818fe59a76b242dcca62579bd353fe9cf01c0d',\n",
       "  'title': 'Sentence Compression Beyond Word Deletion',\n",
       "  'abstract': 'In this paper we generalise the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present a new corpus that is suited to our task and a discriminative tree-to-tree transduction model that can naturally account for structural and lexical mismatches. The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions.',\n",
       "  'year': 2008},\n",
       " {'paperId': '9d74eca00d4d0c4aa7c7369ae37d67498b37bf2f',\n",
       "  'title': 'Modeling Local Coherence: An Entity-Based Approach',\n",
       "  'abstract': 'This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.',\n",
       "  'year': 2008},\n",
       " {'paperId': '9f3d0fa6302e85c86faaf9ec5e9e320f64c56f42',\n",
       "  'title': 'Natural Language Processing and the Web',\n",
       "  'abstract': 'This special issue focuses on applications that innovatively use the Web and Web-scale document collections to create useful resources or applications that let end users navigate the Web more easily. This article is part of a special issue on Natural Language Processing and the Web.',\n",
       "  'year': 2008},\n",
       " {'paperId': 'b5d67d1dc671bce42a9daac0c3605adb3fcfc697',\n",
       "  'title': 'Vector-based Models of Semantic Composition',\n",
       "  'abstract': 'This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.',\n",
       "  'year': 2008},\n",
       " {'paperId': 'babe7ff44322a308e980f466422f99449a37e0de',\n",
       "  'title': 'Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)',\n",
       "  'abstract': None,\n",
       "  'year': 2008},\n",
       " {'paperId': 'dc5b083275ee111dc5e276bd5a9178de8c781c3e',\n",
       "  'title': 'Proceedings of ACL-08: HLT',\n",
       "  'abstract': None,\n",
       "  'year': 2008},\n",
       " {'paperId': 'f3e1bdc6f3bda32b24c695b6d792c2246a179712',\n",
       "  'title': 'Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing',\n",
       "  'abstract': None,\n",
       "  'year': 2008},\n",
       " {'paperId': 'f6ffc5d4e234c0f4f37b00492ae33cb5dfe65765',\n",
       "  'title': 'Automatic Image Annotation Using Auxiliary Text Information',\n",
       "  'abstract': 'The availability of databases of images labeled with keywords is necessary for developing and evaluating image annotation models. Dataset collection is however a costly and time consuming task. In this paper we exploit the vast resource of images available on the web. We create a database of pictures that are naturally embedded into news articles and propose to use their captions as a proxy for annotation keywords. Experimental results show that an image annotation model can be developed on this dataset alone without the overhead of manual annotation. We also demonstrate that the news article associated with the picture can be used to boost image annotation performance.',\n",
       "  'year': 2008},\n",
       " {'paperId': '02e5f147a3d49100bc9f32baccc36ae115435fc6',\n",
       "  'title': 'Reviewers for Volume 33',\n",
       "  'abstract': 'This journal has a knowledgeable and hard working editorial board, listed on the inside front cover of each issue, but for most submissions we also enlist the aid of specialist reviewers outside the editorial board. The Editor of Computational Linguistics would like to express his gratitude to the external reviewers listed here, who anonymously reviewed papers for the journal during the preparation of this volume (Volume 33). Their generosity, judicious judgment, and prompt response substantially helped us to publish a journal that both is timely and maintains exacting scientific standards; it is a genuine pleasure to thank them collectively for their dedicated service.',\n",
       "  'year': 2007},\n",
       " {'paperId': '0dad0da221dea30c3a0e90c45a0699aeb850af49',\n",
       "  'title': 'Using Semantic Roles to Improve Question Answering',\n",
       "  'abstract': 'Shallow semantic parsing, the automatic identification and labeling of sentential constituents, has recently received much attention. Our work examines whether semantic role information is beneficial to question answering. We introduce a general framework for answer extraction which exploits semantic role annotations in the FrameNet paradigm. We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate improvements over state-of-the-art models.',\n",
       "  'year': 2007},\n",
       " {'paperId': '0fff1523828095c89e02be8340dee70de6312514',\n",
       "  'title': 'Graph Connectivity Measures for Unsupervised Word Sense Disambiguation',\n",
       "  'abstract': 'Word sense disambiguation (WSD) has been a long-standing research objective for natural language processing. In this paper we are concerned with developing graph-based unsupervised algorithms for alleviating the data requirements for large scale WSD. Under this framework, finding the right sense for a given word amounts to identifying the most \"important\" node among the set of graph nodes representing its senses. We propose a variety of measures that analyze the connectivity of graph structures, thereby identifying the most relevant word senses. We assess their performance on standard datasets, and show that the best measures perform comparably to state-of-the-art.',\n",
       "  'year': 2007},\n",
       " {'paperId': '1e09b2e87b1d22f375ecd4b571138d5317d5302b',\n",
       "  'title': 'Proceedings of the 20th International Joint Conference on Artifical Intelligence',\n",
       "  'abstract': None,\n",
       "  'year': 2007},\n",
       " {'paperId': '25956623c2ad64895fa31294a094b20188121837',\n",
       "  'title': 'Large Margin Synchronous Generation and its Application to Sentence Compression',\n",
       "  'abstract': 'This paper presents a tree-to-tree transduction method for text rewriting. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring significant improvements over a state-of-the-art model.',\n",
       "  'year': 2007},\n",
       " {'paperId': '58456578b9256e24e5a78edd9fe90cea1ac1d806',\n",
       "  'title': 'Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora',\n",
       "  'abstract': 'Current phrase-based SMT systems perform poorly when using small training sets. This is a consequence of unreliable translation estimates and low coverage over source and target phrases. This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase. Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system.',\n",
       "  'year': 2007},\n",
       " {'paperId': '68cb5b186233a1cb87c4017b5dc35d0346c33495',\n",
       "  'title': 'ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague, Czech Republic',\n",
       "  'abstract': None,\n",
       "  'year': 2007},\n",
       " {'paperId': '7441116c5b5a745708a9d7c5aa0ecf04e0c76c93',\n",
       "  'title': 'Dependency-Based Construction of Semantic Space Models',\n",
       "  'abstract': 'Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account. We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art.',\n",
       "  'year': 2007},\n",
       " {'paperId': '91fea240923c8c7a1dfe5116b0cd420b0c4e5afb',\n",
       "  'title': 'Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning',\n",
       "  'abstract': None,\n",
       "  'year': 2007},\n",
       " {'paperId': 'a11c16410d332005449c4fa281abd2ff538dccfa',\n",
       "  'title': 'Modelling Compression with Discourse Constraints',\n",
       "  'abstract': 'Sentence compression holds promise for many applications ranging from summarisation to subtitle generation and subtitle generation. The task is typically performed on isolated sentences without taking the surrounding context into account, even though most applications would operate over entire documents. In this paper we present a discourse informed model which is capable of producing document compressions that are coherent and informative. Our model is inspired by theories of local coherence and formulated within the framework of Integer Linear Programming. Experimental results show significant improvements over a stateof-the-art discourse agnostic approach.',\n",
       "  'year': 2007},\n",
       " {'paperId': 'dffd7b5346727cef0cb346c2fcc2c4564b1d5ee7',\n",
       "  'title': 'Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, April 22-27, 2007, Rochester, New York, USA',\n",
       "  'abstract': None,\n",
       "  'year': 2007},\n",
       " {'paperId': 'efa28c8638b7c0a973e8902eb943e91855bab9d5',\n",
       "  'title': 'An Information Retrieval Approach to Sense Ranking',\n",
       "  'abstract': 'In word sense disambiguation, choosing the most frequent sense for an ambiguous word is a powerful heuristic. However, its usefulness is restricted by the availability of sense-annotated data. In this paper, we propose an information retrieval-based method for sense ranking that does not require annotated data. The method queries an information retrieval engine to estimate the degree of association between a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance. We also show that the estimated sense frequencies correlate reliably with native speakers‚Äô intuitions.',\n",
       "  'year': 2007},\n",
       " {'paperId': '17708302b7e611608da42feda0f35fb8b92a9ee4',\n",
       "  'title': 'Aggregation via Set Partitioning for Natural Language Generation',\n",
       "  'abstract': 'The role of aggregation in natural language generation is to combine two or more linguistic structures into a single sentence. The task is crucial for generating concise and readable texts. We present an efficient algorithm for automatically learning aggregation rules from a text and its related database. The algorithm treats aggregation as a set partitioning problem and uses a global inference procedure to find an optimal solution. Our experiments show that this approach yields substantial improvements over a clustering-based model which relies exclusively on local information.',\n",
       "  'year': 2006},\n",
       " {'paperId': '37c058e3075c9c8979d0fb75df18d74e2bc611c8',\n",
       "  'title': 'ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, Sydney, Australia, 17-21 July 2006',\n",
       "  'abstract': None,\n",
       "  'year': 2006},\n",
       " {'paperId': '4052dd751012b1bf3771cf4bbd2cc535cbd3fe00',\n",
       "  'title': 'Optimal Constituent Alignment with Edge Covers for Semantic Projection',\n",
       "  'abstract': 'Given a parallel corpus, semantic projection attempts to transfer semantic role annotations from one language to another, typically by exploiting word alignments. In this paper, we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task. Our extensions are twofold: (a) we model constituent alignment as minimum weight edge covers in a bipartite graph, which allows us to find a globally optimal solution efficiently; (b) we propose tree pruning as a promising strategy for reducing alignment noise. Experimental results on an English-German parallel corpus demonstrate improvements over state-of-the-art models.',\n",
       "  'year': 2006},\n",
       " {'paperId': '8ba76acd2304c1618971b5ee84a6608b1c9ffa0f',\n",
       "  'title': 'Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics',\n",
       "  'abstract': None,\n",
       "  'year': 2006},\n",
       " {'paperId': '9d99b79d53ad422eb190f4c646428382db0c59a6',\n",
       "  'title': 'Automatic Evaluation of Information Ordering: Kendall‚Äôs Tau',\n",
       "  'abstract': \"This article considers the automatic evaluation of information ordering, a task underlying many text-based applications such as concept-to-text generation and multidocument summarization. We propose an evaluation method based on Kendall's , a metric of rank correlation. The method is inexpensive, robust, and representation independent. We show that Kendall's correlates reliably with human ratings and reading times.\",\n",
       "  'year': 2006},\n",
       " {'paperId': 'b06cc9e40a6e98db86275f1852eba447b9a7ec25',\n",
       "  'title': 'Learning Sentence-internal Temporal Relations',\n",
       "  'abstract': 'In this paper we propose a data intensive approach for inferring sentence-internal temporal relations. Temporal inference is relevant for practical NLP applications which either extract or synthesize temporal information (e.g., summarisation, question answering). Our method bypasses the need for manual coding by exploiting the presence of markers like after, which overtly signal a temporal relation. We first show that models trained on main and subordinate clauses connected with a temporal marker achieve good performance on a pseudo-disambiguation task simulating temporal inference (during testing the temporal marker is treated as unseen and the models must select the right marker from a set of possible candidates). Secondly, we assess whether the proposed approach holds promise for the semi-automatic creation of temporal annotations. Specifically, we use a model trained on noisy and approximate data (i.e., main and subordinate clauses) to predict intra-sentential relations present in TimeBank, a corpus annotated rich temporal information. Our experiments compare and contrast several probabilistic models differing in their feature space, linguistic assumptions and data requirements. We evaluate performance against gold standard corpora and also against human subjects.',\n",
       "  'year': 2006},\n",
       " {'paperId': 'b3b978262a55368a61fceb044794c598c9b16f6f',\n",
       "  'title': 'Ensemble Methods for Unsupervised WSD',\n",
       "  'abstract': 'Combination methods are an effective way of improving system performance. This paper examines the benefits of system combination for unsupervised WSD. We investigate several voting- and arbiter-based combination strategies over a diverse pool of unsupervised WSD systems. Our combination methods rely on predominant senses which are derived automatically from raw text. Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield significantly better results when compared with state-of-the-art.',\n",
       "  'year': 2006},\n",
       " {'paperId': 'c7b0f1259bca0bf7dd9b9552346efc1c5ed13e1a',\n",
       "  'title': 'Broad coverage paragraph segmentation across languages and domains',\n",
       "  'abstract': 'This article considers the problem of automatic paragraph segmentation. The task is relevant for speech-to-text applications whose output transcipts do not usually contain punctuation or paragraph indentation and are naturally difficult to read and process. Text-to-text generation applications (e.g., summarization) could also benefit from an automatic paragaraph segementation mechanism which indicates topic shifts and provides visual targets to the reader. We present a paragraph segmentation model which exploits a variety of knowledge sources (including textual cues, syntactic and discourse-related information) and evaluate its performance in different languages and domains. Our experiments demonstrate that the proposed approach significantly outperforms our baselines and in many cases comes to within a few percent of human performance. Finally, we integrate our method with a single document summarizer and show that it is useful for structuring the output of automatically generated text.',\n",
       "  'year': 2006},\n",
       " {'paperId': 'd9da9be266b1739dea7691302dc7eb4d33677802',\n",
       "  'title': 'Models for Sentence Compression: A Comparison across Domains, Training Requirements and Evaluation Measures',\n",
       "  'abstract': 'Sentence compression is the task of producing a summary at the sentence level. This paper focuses on three aspects of this task which have not received detailed treatment in the literature: training requirements, scalability, and automatic evaluation. We provide a novel comparison between a supervised constituent-based and an weakly supervised word-based compression algorithm and examine how these models port to different domains (written vs. spoken text). To achieve this, a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used. Finally, we assess whether automatic evaluation measures can be used to determine compression quality.',\n",
       "  'year': 2006},\n",
       " {'paperId': 'fff138b125483ae824a2819dadea49fbff351de6',\n",
       "  'title': 'Constraint-Based Sentence Compression: An Integer Programming Approach',\n",
       "  'abstract': 'The ability to compress sentences while preserving their grammaticality and most of their meaning has recently received much attention. Our work views sentence compression as an optimisation problem. We develop an integer programming formulation and infer globally optimal compressions in the face of linguistically motivated constraints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or large-scale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art.',\n",
       "  'year': 2006},\n",
       " {'paperId': '07320df5382cc09ff5ef4cc05e079b23e7020f25',\n",
       "  'title': \"Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)\",\n",
       "  'abstract': None,\n",
       "  'year': 2005},\n",
       " {'paperId': '1bdc66bf35f7443cea550eb82a691966761f1111',\n",
       "  'title': 'Web-based models for natural language processing',\n",
       "  'abstract': 'Previous work demonstrated that Web counts can be used to approximate bigram counts, suggesting that Web-based frequencies should be useful for a wide variety of Natural Language Processing (NLP) tasks. However, only a limited number of tasks have so far been tested using Web-scale data sets. The present article overcomes this limitation by systematically investigating the performance of Web-based models for several NLP tasks, covering both syntax and semantics, both generation and analysis, and a wider range of n-grams and parts of speech than have been previously explored. For the majority of our tasks, we find that simple, unsupervised models perform better when n-gram counts are obtained from the Web rather than from a large corpus. In some cases, performance can be improved further by using backoff or interpolation techniques that combine Web counts and corpus counts. However, unsupervised Web-based models generally fail to outperform supervised state-of-the-art models trained on smaller corpora. We argue that Web-based models should therefore be used as a baseline for, rather than an alternative to, standard supervised models.',\n",
       "  'year': 2005},\n",
       " {'paperId': '424c1bd4a03f5d65711f311b63a8deccb3c6d92f',\n",
       "  'title': 'Chunking and its Application to Sentence Compression',\n",
       "  'abstract': 'In this paper we consider the problem of analysing sentence-level discourse structure. We introduce discourse chunking (i.e., the identification of intra-sentential nucleus and satellite spans) as an alternative to full-scale discourse parsing. Our experiments show that the proposed modelling approach yields results comparable to state-of-the-art while exploiting knowledge-lean features and small amounts of discourse annotations. We also demonstrate how discourse chunking can be successfully applied to a sentence compression task.',\n",
       "  'year': 2005},\n",
       " {'paperId': '4b131a9c1ef6a3ea6c410110a15dd673a16ed3f8',\n",
       "  'title': 'Automatic Evaluation of Text Coherence: Models and Representations',\n",
       "  'abstract': 'This paper investigates the automatic evaluation of text coherence for machine-generated texts. We introduce a fully-automatic, linguistically rich model of local coherence that correlates with human judgments. Our modeling approach relies on shallow text properties and is relatively inexpensive. We present experimental results that assess the predictive power of various discourse representations proposed in the linguistic literature. Our results demonstrate that certain models capture complementary aspects of coherence and thus can be combined to improve performance.',\n",
       "  'year': 2005},\n",
       " {'paperId': '5feb2f09f3fba65a6fb7dacdd6eb0eaf757194f2',\n",
       "  'title': 'Modeling Local Coherence: An Entity-Based Approach',\n",
       "  'abstract': 'This paper considers the problem of automatic assessment of local coherence. We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text. We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function. Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model.',\n",
       "  'year': 2005},\n",
       " {'paperId': '8822f768f9cdbff2c060e278dba1d744e80a6efb',\n",
       "  'title': 'A comparison of parsing technologies for the biomedical domain',\n",
       "  'abstract': 'This paper reports on a number of experiments which are designed to investigate the extent to which current NLP resources are able to syntactically and semantically analyse biomedical text. We address two tasks: (a) parsing a real corpus with a hand-built wide-coverage grammar, producing both syntactic analyses and logical forms and (b) automatically computing the interpretation of compound nouns where the head is a nominalisation (e.g. hospital arrival means an arrival at hospital, while patient arrival means an arrival of a patient). For the former task we demonstrate that flexible and yet constrained pre-processing techniques are crucial to success: these enable us to use part-of-speech tags to overcome inadequate lexical coverage, and to package up complex technical expressions prior to parsing so that they are blocked from creating misleading amounts of syntactic complexity. We argue that the XML-processing paradigm is ideally suited for automatically preparing the corpus for parsing. For the latter task, we compute interpretations of the compounds by exploiting surface cues and meaning paraphrases, which in turn are extracted from the parsed corpus. This provides an empirical setting in which we can compare the utility of a comparatively deep parser vs. a shallow one, exploring the trade-off between resolving attachment ambiguities on the one hand and generating errors in the parses on the other. We demonstrate that a model of the meaning of compound nominalisations is achievable with the aid of current broad-coverage parsers.',\n",
       "  'year': 2005},\n",
       " {'paperId': 'b2c1efcaac5e99083639994d0daa4ae05b3839d8',\n",
       "  'title': 'Collective Content Selection for Concept-to-Text Generation',\n",
       "  'abstract': 'A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efficient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods.',\n",
       "  'year': 2005},\n",
       " {'paperId': 'bd5c7ba2ac23384f94a0d33906c91c14beb6caf8',\n",
       "  'title': 'Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP)',\n",
       "  'abstract': None,\n",
       "  'year': 2005},\n",
       " {'paperId': 'c626ce70d215121c8f6b94eba18b2e4c0986590f',\n",
       "  'title': 'AMLaP-2005 11th Annual Conference on Architectures and Mechanisms for Language Processing',\n",
       "  'abstract': None,\n",
       "  'year': 2005},\n",
       " {'paperId': 'c89bbeec03e8fdc95bdc35ebda3b63e93f447f39',\n",
       "  'title': 'Knowledge-Lean Approaches to Metonymy Recognition',\n",
       "  'abstract': 'Current approaches to metonymy recognition are mainly supervised, relying heavily on the manual annotation of training and test data. This forms a considerable hindrance to their application on a wider scale. This dissertation therefore aims to relieve the knowledge acquisition bottleneck with respect to metonymy recognition by examining knowledge-lean approaches that reduce this need for human effort. This investigation involves the study of three algorithms that constitute an entire spectrum of machine learning approaches ‚Äî unsupervised, supervised and semi-supervised ones. Chapter 2 will discuss an unsupervised approach to metonymy recognition, and will show that promising results can be reached when the data are automatically annotated with grammatical information. Although the robustness of these systems is limited, they can serve as a pre-processing step for the selection of useful training data, thereby reducing the workload for human annotators. Chapter 3 will investigate memory-based learning, a ‚Äúlazy‚Äù supervised algorithm. This algorithm, which relies on an extremely simple learning stage, is able to replicate the results of more complex systems. Yet, it will also become clear that the performance of this algorithm, like that of others in the literature, depends heavily on grammatical annotation. Finally, chapter 4 will present a semi-supervised algorithm that produces very promising results with only ten labelled training instances. In addition, it will be shown that less than half of the training data from chapter 3 can lead to the same performance as the entire set. Semantic information in particular will prove very useful in this respect. In short, this dissertation presents experimental results which indicate that the knowledge acquisition bottleneck in metonymy recognition can be relieved with unsupervised and semi-supervised methods. These approaches may make the extension of current algorithms to a wide-scale metonymy resolution system a much more feasible task.',\n",
       "  'year': 2005},\n",
       " {'paperId': 'cab88777bbde2e884726cf72ff7b3f9126efc8ff',\n",
       "  'title': 'Cross-linguistic Projection of Role-Semantic Information',\n",
       "  'abstract': 'This paper considers the problem of automatically inducing role-semantic annotations in the FrameNet paradigm for new languages. We introduce a general framework for semantic projection which exploits parallel texts, is relatively inexpensive and can potentially reduce the amount of effort involved in creating semantic resources. We propose projection models that exploit lexical and syntactic information. Experimental results on an English-German parallel corpus demonstrate the advantages of this approach.',\n",
       "  'year': 2005},\n",
       " {'paperId': 'd7c3435dfafa3f7fdc546de6dbd53dab74a604a4',\n",
       "  'title': 'Reviewers for Volume 31',\n",
       "  'abstract': 'This journal has a knowledgeable and hardworking editorial board, whose members are listed on the inside front cover of each issue, but for most submissions we also enlist the aid of specialist reviewers outside the editorial board. The Editor of Computational Linguistics would like to express his gratitude to the external reviewers listed below who anonymously reviewed papers for the journal during the preparation of this volume (Volume 31). Their generosity, judicious judgment, and prompt response substantially helped us to publish a journal that both is timely and maintains exacting scientific standards; it is a genuine pleasure to thank them collectively for their dedicated service.',\n",
       "  'year': 2005},\n",
       " {'paperId': 'dd6cdc89c4adedf5b9a3ac659f1b2bca5cc0a855',\n",
       "  'title': 'Cross-Lingual Bootstrapping of Semantic Lexicons: The Case of FrameNet',\n",
       "  'abstract': 'This paper considers the problem of unsupervised semantic lexicon acquisition. We introduce a fully automatic approach which exploits parallel corpora, relies on shallow text properties, and is relatively inexpensive. Given the English FrameNet lexicon, our method exploits word alignments to generate frame candidate lists for new languages, which are subsequently pruned automatically using a small set of linguistically motivated filters. Evaluation shows that our approach can produce high-precision multilingual FrameNet lexicons without recourse to bilingual dictionaries or deep syntactic and semantic analysis.',\n",
       "  'year': 2005},\n",
       " {'paperId': 'ede7b99dfac6ed8b2a0edfa1998eeee312b17e2d',\n",
       "  'title': \"IJCAI'05 Proceedings of the 19th international joint conference on Artificial intelligence\",\n",
       "  'abstract': None,\n",
       "  'year': 2005},\n",
       " {'paperId': 'f1d3b870e7210624428cd0d7e78cc52e666c4b4d',\n",
       "  'title': 'Discourse Chunking and its Application to Sentence Compression',\n",
       "  'abstract': 'In this paper we consider the problem of analysing sentence-level discourse structure. We introduce discourse chunking (i.e., the identification of intra-sentential nucleus and satellite spans) as an alternative to full-scale discourse parsing. Our experiments show that the proposed modelling approach yields results comparable to state-of-the-art while exploiting knowledge-lean features and small amounts of discourse annotations. We also demonstrate how discourse chunking can be successfully applied to a sentence compression task.',\n",
       "  'year': 2005},\n",
       " {'paperId': '2ab11d1c723f21dc1206db79549fcd995eacce81',\n",
       "  'title': 'Automatic Paragraph Identification: A Study across Languages and Domains',\n",
       "  'abstract': 'In this paper we investigate whether paragraphs can be identified automatically in different languages and domains. We propose a machine learning approach which exploits textual and discourse cues and we assess how well humans perform on this task. Our best models achieve an accuracy that is significantly higher than the best baseline and, for most data sets, comes to within 6% of human performance.',\n",
       "  'year': 2004},\n",
       " {'paperId': '41f2ff87fb5346027b13a90b5ce6df35d9022d3e',\n",
       "  'title': 'Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004',\n",
       "  'abstract': None,\n",
       "  'year': 2004},\n",
       " {'paperId': '7526083344329891af16ef7b9ed072bacb47bc28',\n",
       "  'title': 'The Cost of Enriched Composition: Eye-Movement Evidence from German',\n",
       "  'abstract': 'Reading research in English has shown that the processing of logical metonymy as in ‚ÄúThe student began the book‚Äù is costly compared with, e.g., ‚ÄúThe student read the book‚Äù (McElree et al., 2001; Traxler, Pickering, & McElree, 2002). An explanation for this is that the interpretation of ‚Äúbegan the book‚Äù requires type shifting of the object noun (‚Äúbook‚Äù) into an event representation (e.g. ‚Äúbegan reading the book‚Äù), a mechanism also known as enriched composition (cf. Pustejovsky, 1995). The present experiments were designed to answer two important questions:',\n",
       "  'year': 2004},\n",
       " {'paperId': 'a1ac4aa31e9c8ea28bf7aa32e086ce83182d1afd',\n",
       "  'title': 'Inferring Sentence-internal Temporal Relations',\n",
       "  'abstract': 'In this paper we propose a data intensive approach for inferring sentence-internal temporal relations, which relies on a simple probabilistic model and assumes no manual coding. We explore various combinations of features, and evaluate performance against a goldstandard corpus and human subjects performing the same task. The best model achieves 70.7% accuracy in inferring the temporal relation between two clauses and 97.4% accuracy in ordering them, assuming that the temporal relation is known.',\n",
       "  'year': 2004},\n",
       " {'paperId': 'b0c4c4a7a215b910b6e23bad1da07bc1a63f3868',\n",
       "  'title': 'Verb Class Disambiguation Using Informative Priors',\n",
       "  'abstract': \"Levin's (1993) study of verb classes is a widely used resource for lexical semantics. In her framework, some verbs, such as give, exhibit no class ambiguity. But other verbs, such as write, have several alternative classes. We extend Levin's inventory to a simple statistical model of verb class ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus. We additionally show that these preferences are useful as priors for a verb sense disambiguator.\",\n",
       "  'year': 2004},\n",
       " {'paperId': 'b8aecb833b6f094e862461b412e6e3b1d6cf0a68',\n",
       "  'title': 'The Web as a Baseline: Evaluating the Performance of Unsupervised Web-based Models for a Range of NLP Tasks',\n",
       "  'abstract': 'Previous work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks. So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets. The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams. For the majority of tasks, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus. However, in most cases, web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora. We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models.',\n",
       "  'year': 2004},\n",
       " {'paperId': 'c384f0688c216742b8de84d08003b66707717786',\n",
       "  'title': 'Proceedings of EMNLP 2004',\n",
       "  'abstract': None,\n",
       "  'year': 2004},\n",
       " {'paperId': '325585c4ab24370a91b589ed17ce19f280110955',\n",
       "  'title': 'Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, 7-12 July 2003, Sapporo Convention Center, Sapporo, Japan',\n",
       "  'abstract': None,\n",
       "  'year': 2003},\n",
       " {'paperId': '5dfed29550d75cca99019aa52d40038dcb23b3cb',\n",
       "  'title': 'Using the Web to Obtain Frequencies for Unseen Bigrams',\n",
       "  'abstract': 'This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.',\n",
       "  'year': 2003},\n",
       " {'paperId': '60591fff5844f1a653bdfaf22e0de3f159872d9c',\n",
       "  'title': 'Intra-sentential context effects on the interpretation of logical metonymy',\n",
       "  'abstract': 'Verbs such as enjoy in the student enjoyed the book exhibit logical metonymy: enjoy is interpreted as enjoy reading. Theoretical work (Pustejovsky, 1991, 1995) predicts that this interpretation can be influenced by intra-sentential context, e.g., by the subject of enjoy .I n this article, we test this prediction using a completion experiment and find that the interpretation of a metonymic verb is influenced by the semantic role of its subject. We present a Bayesian model that accounts for the interpretation of logical metonymy and achieves a good fit on our experimental data. We show that the parameters of the model can be estimated from completion data or from corpus data.',\n",
       "  'year': 2003},\n",
       " {'paperId': '670a5ab26e55391e8309cb596817723874170b2f',\n",
       "  'title': 'Plagiarism Declaration',\n",
       "  'abstract': 'Death distribution methods, particularly the Generalized Growth Balance (GGB) and the Synthetic Extinct Generations (SEG) methods, have been observed to lead to the most accurate estimates when estimating mortality [1]. The more general version of the SEG method corrects for differential coverage of censuses directly by adding a constant (6) to the age-specific growth rates such that the correction leads to a horizontal series of age specific estimates of completeness. This research attempts to obtain the best variation of this version of the SEG method from a range of choices for an open interval age as well as well as methods of estimating life expectancy. completeness and 6. This task is accomplished by starting with a base population with known mortality then applying random errors in completeness. age misstatement and net migration to it to generate numerous datasets consisting of simulated census counts and simulated vital registration deaths by age. Variations of the SEG method are then applied to the simulated datasets to correct for the underestimation of mortality caused by the data errors. The best variations are found by statistical analysis of the difference between the true mortality and the estimated mortality for each variation and dataset generated. Using the Coale and Demeny model life tables to estimate life expectancy. selecting the a that results in a minimum variance in the age specific estimates of completeness. estimating completeness using the median value of the age specific. estimates Of completeness for ages 15 and older and using the 85+ age group for the open interval is observed to be the variation of the SEG method that leads to the most accurate estimates of mortality. U ive rsi ty of Ca pe To wn Acknowledgements I would like to thank all people who have helped and inspired me during my masters study. I especially want to thank my advisor. Professor Rob Dorrington, for his guidance during my research and study at the University of Cape Town. His advice, support and direction in this research has been invaluable and I especially appreciate his being accessible and willing to help with this research. spurring me on even when work commitments were hindering my progress. Associate Professor Tom Moultrie deserves a special thanks as my student advisor and course convener. I would like to thank him for the help for the duration of the degree and for hiring me as a tutor for Basic Demography and Biostatistics for demographers. My deepest gratitude goes to my physical family for their unflagging love and support throughout my life: this dissertation is simply impossible without them. I am indebted to my mother. Mrs Helen Msemburi, for her care and love and to my three brothers: Leo, Tom and Andrew. Particularly Thomas who has supported and encouraged me every step of the way and has exemplified diligence and strength. I am also thankful to my spiritual family, His People (Baxter). Thank you to all my pastors and friends there. I extend my heartfelt appreciation to my dear lady Hilda. my closest friend and my treasure in this world. Her love and support have been an anchor in the most trying storms. Love you lots my girl, both now and always. The generous support from the Mellon fund is greatly appreciated. Without their support, my ambition to pursue this M Phil would not have been realized. Last but not least, thanks be to God for my life through all tests and victories in the past six years. My Father, my Savior, my Lord. You have made my life more bountiful. May your name be exalted. honored. and glorified. iii Un ive rsi ty of Ca pe To wn',\n",
       "  'year': 2003},\n",
       " {'paperId': '8e3a93ab58a97c9cad6caf46baceb44f95ad91b8',\n",
       "  'title': 'Evaluating and Combining Approaches to Selectional Preference Acquisition',\n",
       "  'abstract': 'Previous work on the induction of selectional preferences has been mainly carried out for English and has concentrated almost exclusively on verbs and their direct objects. In this paper, we focus on class-based models of selectional preferences for German verbs and take into account not only direct objects, but also subjects and prepositional complements. We evaluate model performance against human judgments and show that there is no single method that overall performs best. We explore a variety of parametrizations for our models and demonstrate that model combination enhances agreement with human ratings.',\n",
       "  'year': 2003},\n",
       " {'paperId': 'b1a3b5a20e77d8ac94967dc48173c48af3012eaf',\n",
       "  'title': 'Constructing Semantic Space Models from Parsed Corpora',\n",
       "  'abstract': 'Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations.',\n",
       "  'year': 2003},\n",
       " {'paperId': 'ec6c09bbf13a08e8fcc901e334e2aae20821727b',\n",
       "  'title': 'Probabilistic Text Structuring: Experiments with Sentence Ordering',\n",
       "  'abstract': \"Ordering information is a critical task for natural language generation applications. In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation. We describe a model that learns constraints on sentence order from a corpus of domain-specific texts and an algorithm that yields the most likely order among several alternatives. We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model's task. We also assess the appropriateness of such a model for multidocument summarization.\",\n",
       "  'year': 2003},\n",
       " {'paperId': 'f932caac89709a716a7d3e6632caf9f34d709518',\n",
       "  'title': 'A Probabilistic Account of Logical Metonymy',\n",
       "  'abstract': \"In this article we investigate logical metonymy, that is, constructions in which the argument of a word in syntax appears to be different from that argument in logical form (e.g., enjoy the book means enjoy reading the book, and easy problem means a problem that is easy to solve). The systematic variation in the interpretation of such constructions suggests a rich and complex theory of composition on the syntax/semantics interface. Linguistic accounts of logical metonymy typically fail to describe exhaustively all the possible interpretations, or they don't rank those interpretations in terms of their likelihood. In view of this, we acquire the meanings of metonymic verbs and adjectives from a large corpus and propose a probabilistic model that provides a ranking on the set of possible interpretations. We identify the interpretations automatically by exploiting the consistent correspondences between surface syntactic cues and meaning. We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model's ranking of meanings correlates reliably with human intuitions.\",\n",
       "  'year': 2003},\n",
       " {'paperId': 'f95ef8650e2b54f02511f60a4bfe866e1a75b760',\n",
       "  'title': 'Detecting Novel Compounds: The Role of Distributional Evidence',\n",
       "  'abstract': 'Research on the discovery of terms from corpora has focused on word sequences whose recurrent occurrence in a corpus is indicative of their terminological status, and has not addressed the issue of discovering terms when data is sparse. This becomes apparent in the case of noun compounding, which is extremely productive: more than half of the candidate compounds extracted from a corpus are attested only once. We show how evidence about established (i.e., frequent) compounds can be used to estimate features that can discriminate rare valid compounds from rare nonce terms in addition to a variety of linguistic features than can be easily gleaned from corpora without relying on parsed text.',\n",
       "  'year': 2003},\n",
       " {'paperId': '2b6180538fe84c16ec5c473ef6fd3e672ddc5174',\n",
       "  'title': 'A Comparison of Parsing Te hnologies for theBiomedi al',\n",
       "  'abstract': None,\n",
       "  'year': 2002},\n",
       " {'paperId': '746ed2e83b25702e9da1a7735fb3f6f227f28652',\n",
       "  'title': 'XML-based NLP Tools for Analysing and Annotating Medical Language',\n",
       "  'abstract': \"We describe the use of a suite of highly flexible XML-based NLP tools in a project for processing and interpreting text in the medical domain. The main aim of the paper is to demonstrate the central role that XML mark-up and XML NLP tools have played in the analysis process and to describe the resultant annotated corpus of MEDLINE abstracts. In addition to the XML tools, we have succeeded in integrating a variety of non-XML 'off the shelf' NLP tools into our pipelines, so that their output is added into the mark-up. We demonstrate the utility of the annotations that result in two ways. First, we investigate how they can be used to improve parse coverage of a hand-crafted grammar that generates logical forms. And second, we investigate how they contribute to automatic lexical semantic acquisition processes.\",\n",
       "  'year': 2002},\n",
       " {'paperId': '0bb3a566d5cc49a2f2ecbd5739d6c7ef760e2f80',\n",
       "  'title': 'Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics',\n",
       "  'abstract': None,\n",
       "  'year': 1999},\n",
       " {'paperId': '91646d72e60f2f6d60ea47faf80c43da5a60b040',\n",
       "  'title': '10th Conference of the European Chapter of the Association for Computational Linguistics',\n",
       "  'abstract': None,\n",
       "  'year': 1999},\n",
       " {'paperId': 'a4a1b6b2580609e5a64fd62ea4f1ff6bcc2ba8bd',\n",
       "  'title': 'Using Subcategorization to Resolve Verb Class Ambiguity',\n",
       "  'abstract': \"Levin's (1993) taxonomy of verbs and their classes is a widely used resource for lexical semantics. In her framework, some verbs, such as give exhibit no class ambiguity. But other verbs, such as write, can inhabit more than one class. In some of these ambiguous cases the appropriate class for a particular token of a verb is immediately obvious from inspection of the surrounding context. In others it is not, and an application which wants to recover this information will be forced to rely on some more or less elaborate process of inference. We present a simple statistical model of verb class ambiguity and show how it can be used to carry out such inference.\",\n",
       "  'year': 1999},\n",
       " {'paperId': '181921d2254ebf8b51d5e7884ec3ae0dd1bef1b9',\n",
       "  'title': 'Expanding the Domain of a Multi-lingual Speech-to-Speech Translation System',\n",
       "  'abstract': 'JANUS is a multi-lingual speech-to-speech translation system, which has been designed to translate spontaneous spoken language in a limited domain. In this paper, we describe our recent preliminary efforts to expand the domain of coverage of the system from the rather limited Appointment Scheduling domain, to the much richer Travel Planning domain. We compare the two domains in terms of out-of-vocabulary rates and linguistic complexity. We discuss the challenges that these differences impose on our translation system and some planned changes in the design of the system. Initial evaluations on Travel Planning data are also presented.',\n",
       "  'year': 1997},\n",
       " {'paperId': '4082fe4d276eef19f3ff26f25f38c56236ff4327',\n",
       "  'title': 'C L ] 1 8 A pr 2 01 8 Bootstrapping Generators from Noisy Data',\n",
       "  'abstract': None,\n",
       "  'year': None},\n",
       " {'paperId': 'f8cc43c8890976fe9d214496a81a6a5ee36c1eec',\n",
       "  'title': 'Table of Contents Distributional Semantics from Text and Images Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity Encoding Syntactic Dependencies by Vector Permutation Workshop Program Attribute-related Meaning Representations for Adjective- Noun Phrases in a Si',\n",
       "  'abstract': 'ii Introduction GEMS 2011 ‚Äî GEometrical Models of Natural Language Semantics ‚Äî is the third instalment in a successful series of workshops on distributional models of meaning. Since their earliest application in information retrieval, these models have become omnipresent in contemporary computational linguistics and neighboring fields. Different types of distributional models have been introduced ‚Äî from the relatively simple bag-of-word, document-based and syntax-based techniques to the statistically more advanced topic models. In the field of lexical semantics, their direct applications include the construction of lexical taxonomies, the recognition of textual entailment, word sense discrimination and disambiguation, cognitive modeling, etc. Moreover, other areas of NLP, like parsing and Machine Translation, have found they can indirectly benefit from the ability of distributional models to generalize from a limited training set to unseen, but semantically similar, words. The growth of distributional semantics, however, is not without its problems. The aim of GEMS is to address two orthogonal types of current challenges. First, there is the fragmentation with regard to data sets, methods and evaluation metrics, which makes it difficult to compare studies and achieve scientific progress. We addressed this problem by providing authors with two datasets suitable for the evaluation of distributional models, together with the corpora that can be used for their construction. As a result, the performance of very different approaches can be easily compared across papers. Second, these datasets were chosen so as to reflect two of the most pressing issues in the development of distributional models nowadays: differentiation between semantic relations and compositionality. The first set, presented by Baroni and Lenci, includes concrete nouns from different semantic classes (living, non-living, etc.) with associated words for specific semantic relations such as \" attribute \" , \" category coordinate \" , \" event \" , or \" metonym \". Panchenko uses this data to compare 21 measures of semantic similarity and relatedness, based on information from WordNet, a traditional corpus, and the web. Baroni, Bruni and Binh Tran explore images as a fourth type of information. Both papers discover fundamental differences in the semantic information that is captured by these different sources of information. This paves the way for a combined, more comprehensive model. The second dataset, borrowed from Mitchell and Lapata, contains phrase similarity judgments. It makes it possible to address the evaluation of distributional models in compositional tasks. Grefenstette and Sadrzadeh show how a transitive verb can be modeled ‚Ä¶',\n",
       "  'year': None}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.datamodules.apis.semantic_scholar_api import SemanticScholarAPI\n",
    "\n",
    "# Author that we want to analyse\n",
    "author_id = 1747893\n",
    "# Semantic scholar API\n",
    "api = SemanticScholarAPI()\n",
    "# Get all author papers\n",
    "papers = api.author(author_id, fields=['papers.title', 'papers.abstract', 'papers.year'])['papers']\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d84d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 4),\n",
       " ('text', 4),\n",
       " ('generation', 3),\n",
       " ('macro', 3),\n",
       " ('planning', 3),\n",
       " ('content', 2),\n",
       " ('stage', 2),\n",
       " ('abstract', 1),\n",
       " ('recent', 1),\n",
       " ('approaches', 1),\n",
       " ('successful', 1),\n",
       " ('encoder', 1),\n",
       " ('decoder', 1),\n",
       " ('architecture', 1),\n",
       " ('variants', 1),\n",
       " ('models', 1),\n",
       " ('fluent', 1),\n",
       " ('appropriate', 1),\n",
       " ('issues', 1),\n",
       " ('neural', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# TFIDF, tokenization, check BERT\n",
    "\n",
    "def get_hotwords(text, most_common=20):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] # 1\n",
    "    doc = nlp(text.lower()) # 2\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "            \n",
    "    pairs = Counter(result).most_common(most_common)            \n",
    "    return pairs, [pair[0] for pair in pairs]\n",
    "\n",
    "def get_count(text, word):\n",
    "    doc = nlp(text.lower()) # 2\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if (token.text == word):\n",
    "            count += 1\n",
    "    return count \n",
    "\n",
    "\n",
    "hotwords_per_paper = []\n",
    "paper_data = []\n",
    "hotwords = []\n",
    "\n",
    "for paper in papers:\n",
    "    if paper['title'] and paper['abstract'] :\n",
    "        pairs, words = get_hotwords(paper['title']+' '+paper['abstract'])\n",
    "        hotwords_per_paper.append(pairs)\n",
    "        paper_data.append((paper['paperId'], paper['title'], paper['year']))\n",
    "        hotwords += words\n",
    "    \n",
    "hotwords = Counter(hotwords)\n",
    "hotwords_reduced = Counter({k: c for k, c in hotwords.items() if c > 0})\n",
    "num_valid_papers = len(hotwords_per_paper)\n",
    "num_hotwords = len(hotwords)\n",
    "#print(num_hotwords, num_hotwords_reduced, num_papers)\n",
    "hotwords_per_paper[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ecae858d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "histfunc": "sum",
         "hovertemplate": "word=%{x}<br>sum of count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "histogram",
         "x": [
          "function",
          "threshold",
          "functions",
          "current",
          "respect",
          "selection",
          "different",
          "setting",
          "algorithms",
          "evolutionary",
          "large",
          "order",
          "analysis",
          "runtime",
          "optimum",
          "fitness",
          "generation",
          "binval",
          "dynamic",
          "linear"
         ],
         "xaxis": "x",
         "y": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "word"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "sum of count"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"bc485777-b4b3-4043-9888-1c345d872ee4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bc485777-b4b3-4043-9888-1c345d872ee4\")) {                    Plotly.newPlot(                        \"bc485777-b4b3-4043-9888-1c345d872ee4\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"histfunc\":\"sum\",\"hovertemplate\":\"word=%{x}<br>sum of count=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"type\":\"histogram\",\"x\":[\"function\",\"threshold\",\"functions\",\"current\",\"respect\",\"selection\",\"different\",\"setting\",\"algorithms\",\"evolutionary\",\"large\",\"order\",\"analysis\",\"runtime\",\"optimum\",\"fitness\",\"generation\",\"binval\",\"dynamic\",\"linear\"],\"xaxis\":\"x\",\"y\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"yaxis\":\"y\"}],                        {\"barmode\":\"relative\",\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"word\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"sum of count\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bc485777-b4b3-4043-9888-1c345d872ee4');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(hotwords_reduced, orient='index').reset_index()\n",
    "df = df.rename(columns={'index': 'word', 0: \"count\"})\n",
    "df = df.sort_values(by=['count'], ascending=False)\n",
    "fig = px.histogram(df, x='word', y='count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f6960eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=function<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "function",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "function",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          4
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=threshold<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "threshold",
         "marker": {
          "color": "#EF553B",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "threshold",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          4
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=dynamic<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "dynamic",
         "marker": {
          "color": "#00cc96",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "dynamic",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          3
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=binval<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "binval",
         "marker": {
          "color": "#ab63fa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "binval",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          3
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=generation<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "generation",
         "marker": {
          "color": "#FFA15A",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          3
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=fitness<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "fitness",
         "marker": {
          "color": "#19d3f3",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "fitness",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          3
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=optimum<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "optimum",
         "marker": {
          "color": "#FF6692",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "optimum",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          3
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=runtime<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "runtime",
         "marker": {
          "color": "#B6E880",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "runtime",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          2
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=analysis<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "analysis",
         "marker": {
          "color": "#FF97FF",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "analysis",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          2
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=order<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "order",
         "marker": {
          "color": "#FECB52",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "order",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          2
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=large<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "large",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "large",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          2
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=evolutionary<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "evolutionary",
         "marker": {
          "color": "#EF553B",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "evolutionary",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=algorithms<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "algorithms",
         "marker": {
          "color": "#00cc96",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "algorithms",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=setting<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "setting",
         "marker": {
          "color": "#ab63fa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "setting",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=different<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "different",
         "marker": {
          "color": "#FFA15A",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "different",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=selection<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "selection",
         "marker": {
          "color": "#19d3f3",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "selection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=respect<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "respect",
         "marker": {
          "color": "#FF6692",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "respect",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=current<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "current",
         "marker": {
          "color": "#B6E880",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "current",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=functions<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "functions",
         "marker": {
          "color": "#FF97FF",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "functions",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=linear<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "linear",
         "marker": {
          "color": "#FECB52",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "linear",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"126c3a2c-ec95-4502-b02b-fe0abc04252b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"126c3a2c-ec95-4502-b02b-fe0abc04252b\")) {                    Plotly.newPlot(                        \"126c3a2c-ec95-4502-b02b-fe0abc04252b\",                        [{\"hovertemplate\":\"variable=function<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"function\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"function\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[4],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=threshold<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"threshold\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"threshold\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[4],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=dynamic<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"dynamic\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"dynamic\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[3],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=binval<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"binval\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"binval\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[3],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=generation<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"generation\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"generation\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[3],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=fitness<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"fitness\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"fitness\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[3],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=optimum<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"optimum\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"optimum\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[3],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=runtime<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"runtime\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"runtime\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[2],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=analysis<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"analysis\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"analysis\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[2],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=order<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"order\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"order\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[2],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=large<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"large\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"large\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[2],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=evolutionary<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"evolutionary\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"evolutionary\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[1],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=algorithms<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"algorithms\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"algorithms\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[1],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=setting<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"setting\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"setting\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[1],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=different<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"different\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"different\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[1],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=selection<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"selection\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"selection\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[1],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=respect<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"respect\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"respect\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[1],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=current<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"current\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"current\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[1],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=functions<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"functions\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"functions\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[1],\"yaxis\":\"y\"},{\"hovertemplate\":\"variable=linear<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"linear\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"linear\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0],\"xaxis\":\"x\",\"y\":[1],\"yaxis\":\"y\"}],                        {\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('126c3a2c-ec95-4502-b02b-fe0abc04252b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hotwords_count_dict = {hotword: [0] * num_valid_papers for hotword in hotwords}\n",
    "\n",
    "for idx, paper_hotwords in enumerate(hotwords_per_paper):\n",
    "    for hotword, count in paper_hotwords:\n",
    "        #print(hotword, count, hotword in hotwords_count_dict)\n",
    "        if hotword in hotwords_count_dict: \n",
    "            hotwords_count_dict[hotword][idx] += count\n",
    "\n",
    "#for k, v in hotwords_count_dict.items():\n",
    "#    print(len(v))\n",
    "            \n",
    "hotwords_count_df = pd.DataFrame(data=hotwords_count_dict)\n",
    "fig = px.scatter(hotwords_count_df, x=hotwords_count_df.index, y=hotwords_count_df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "060bf5d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hotwords_count_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/x32rfnbd78n2jj1z9x_tgx9h0000gn/T/ipykernel_34868/469148966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhotword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bert'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhotwords_count_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhotword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhotwords_count_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hotwords_count_df' is not defined"
     ]
    }
   ],
   "source": [
    "hotword = 'bert'\n",
    "data = hotwords_count_df[hotword]\n",
    "fig = px.scatter(hotwords_count_df, x=data.index, y=data)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d696f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          3,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"7ec5a896-6f84-4dd9-ae16-59fbd1884bb1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7ec5a896-6f84-4dd9-ae16-59fbd1884bb1\")) {                    Plotly.newPlot(                        \"7ec5a896-6f84-4dd9-ae16-59fbd1884bb1\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"type\":\"scatter\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228],\"xaxis\":\"x\",\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"yaxis\":\"y\"}],                        {\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('7ec5a896-6f84-4dd9-ae16-59fbd1884bb1');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "word = 'bert'\n",
    "  \n",
    "def get_count(text, word):\n",
    "    doc = nlp(text.lower()) # 2\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if (token.text == word):\n",
    "            count += 1\n",
    "    return count \n",
    "\n",
    "counts = []\n",
    "for paper in papers:\n",
    "    if paper['title'] and paper['abstract']:\n",
    "        count = get_count(paper['title']+' '+paper['abstract'], word)\n",
    "        counts.append(count)\n",
    "      \n",
    "\n",
    "df = pd.DataFrame(data=counts)\n",
    "fig = px.scatter(df, x=df.index, y=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31c60c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3e3cf09d619cff79b8379b639cddfcd09451995b', 'Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings', 2016)\n"
     ]
    }
   ],
   "source": [
    "print(paper_data[87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e19dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
