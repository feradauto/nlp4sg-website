{"id": 4, "data": "0\n2\n0\n2\n \nt\nc\nO\n \n8\n \n \n]\n\nY\nC\n.\ns\nc\n[\n \n \n3\nv\n6\n7\n5\n0\n1\n.\n9\n0\n0\n2\n:\nv\ni\nX\nr\na\n\nEthical Machine Learning\nin Health Care\n\nIrene Y. Chen,1 Emma Pierson,2 Sherri Rose,3\nShalmali Joshi,4 Kadija Ferryman,5\nand Marzyeh Ghassemi4,6\n1Electrical Engineering and Computer Science, Massachusetts Institute of\nTechnology, Cambridge, MA, 02139, USA; email: iychen@mit.edu\n2Microsoft Research, Cambridge, MA, 02143, USA\n3Center for Health Policy and Center for Primary Care and Outcomes Research,\nStanford University, Stanford, CA, 94305, USA\n4Vector Institute, Toronto, ON, Canada\n5Department of Technology, Culture, and Society, Tandon School of Engineering,\nNew York University, Brooklyn, NY, 11201, USA\n6Department of Computer Science, University of Toronto, Toronto, ON, Canada\n\nXxxx. Xxx. Xxx. Xxx. 2021. AA:1–24\n\nKeywords\n\nhttps://doi.org/10.1146/((please add\narticle doi))\n\nCopyright © 2021 by Annual Reviews.\nAll rights reserved\n\nAbstract\n\nmachine learning, bias, ethics, health, health care, health disparities\n\nThe use of machine learning (ML) in health care raises numerous ethi-\ncal concerns, especially as models can amplify existing health inequities.\nHere, we outline ethical considerations for equitable ML in the advance-\nment of health care. Speciﬁcally, we frame ethics of ML in health care\nthrough the lens of social justice. We describe ongoing eﬀorts and out-\nline challenges in a proposed pipeline of ethical ML in health, ranging\nfrom problem selection to post-deployment considerations. We close by\nsummarizing recommendations to address these challenges.\n\n1\n\n\fContents\n\n1. INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2. PROBLEM SELECTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.1. Global Health Injustice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2. Racial Injustice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3. Gender Injustice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.4. Diversity of the Scientiﬁc Workforce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3. DATA COLLECTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.1. Heterogeneous Data Losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2. Population-speciﬁc Data Losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4. OUTCOME DEFINITION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1. Clinical Diagnosis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2. Health Care Costs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5. ALGORITHM DEVELOPMENT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.1. Understanding Confounding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.2. Feature Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3. Tuning Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.4. Performance Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5. Group Fairness Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6. POST-DEPLOYMENT CONSIDERATIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.1. Quantifying Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.2. Model Generalizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.3. Model and Data Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.4. Regulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7. RECOMMENDATIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2\n4\n4\n5\n5\n5\n6\n6\n7\n8\n9\n9\n10\n10\n11\n12\n12\n12\n13\n13\n14\n14\n14\n15\n\n1. INTRODUCTION\n\nAs machine learning (ML) models proliferate into many aspects of our lives, there is growing\nconcern regarding their ability to inﬂict harm. In medicine, excitement about human-level\nperformance (1) of ML for health is balanced against ethical concerns, such as the potential\nfor these tools to exacerbate existing health disparities (2, 3, 4, 5). For instance, recent\nwork has demonstrated that state-of-the-art clinical prediction models underperform on\nwomen, ethnic and racial minorities, and those with public insurance (6). Other research has\nshown that when popular contextual language models are trained on scientiﬁc articles, they\ncomplete clinical note templates to recommend “hospitals” for violent white patients and\n“prison” for violent Black patients (7). Even more worrisome, health care models designed\nto optimize referrals to long-term care-management programs for millions of patients were\nfound to exclude Black patients with similar health conditions compared to white patients\nfrom care management programs (8).\n\nA growing body of literature wrestles with the social implications of machine learning\nand technology. Some of this work, referred to as critical data studies, is from a social\nscience perspective (9, 10), whereas other work leads with a technical and computer science\nperspective (11, 12, 13). While there is scholarship addressing social implications and\nalgorithmic fairness in general, there has been less work at the intersection of health, ML,\nand fairness (14, 15, 16), despite the potential life-or-death impacts of those models (8, 17).\n\nMachine learning\n(ML): The study of\ncomputer algorithms\nthat improve\nautomatically\nthrough experience\n\nML model: An\nalgorithm that has\nbeen trained on data\nfor a speciﬁc use\ncase\n\nAlgorithm: A ﬁnite\nsequence of\nwell-deﬁned\ninstructions used to\nsolve a class of\nproblems\n\n2\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\n\fFigure 1\n\nWe motivate the ﬁve steps in the ethical pipeline for health care model development. Each stage\ncontains considerations for ML where ignoring technical challenges violate the bioethical principle\nof justice, either by exacerbating existing social injustices or by creating the potential for new\ninjustices between groups. Although this review’s ethical focus is on social justice, the challenges\nthat we highlight may also violate ethical principles such as justice and beneﬁcence. We highlight\na few in this illustration.\n\nWhile researchers looking to develop ethical ML models for health can begin by draw-\ning on bioethics principles (18, 19), these principles are designed to inform clinical care\npractices. How these principles could inform the ML model development pipeline remains\nWe note that there has been signiﬁcant work on other important eth-\nunderstudied.\nical issues that relate to ML and health, including reviews of consent and privacy (20),\nwhich we do not address here. Instead, we focus on equity in ML models that operate on\nhealth data. We focus primarily on diﬀerences between groups induced by, or related to,\nthe model development pipeline, drawing on both the bioethics principle of justice, and\nthe established social justice centering of public health ethics (21). Unjust diﬀerences in\nquality and outcomes of health care between groups often reﬂect existing societal disparities\nfor disadvantaged groups. We consider other bioethics principles such as beneﬁcence and\nnon-maleﬁcence, but focus them primarily on groups of patients rather than on individuals.\nWe organize this review by describing the ethical considerations that arise at each step of\nthe pipeline during model development for ML in health (Figure 1), from research funding\nto post-deployment. Here we motivate the ethical considerations in the pipeline with a case\nstudy of Black mothers in the United States, who die in childbirth at a rate three times\nhigher than white women (22). This inequity is unjust because it connects to a history\nof reproductive injustices faced by Black women in the United States, from gynecological\nexperimentation on enslaved women to forced sterilizations (23, 24).\n\n1. This disparity occurs in part during problem selection because maternal mortality\n\nis an understudied problem (25).\n\n2. Even after accounting for problem selection, data collection from hospitals may dif-\nfer in quality and quantity. For example, 75% of Black women give birth at hospitals\nthat serve predominantly Black patients (26) but Black-serving hospitals have higher\nrates of maternal complications than other hospitals (27).\n\n3. Once data are collected, the choice of outcome deﬁnition can obscure underlying\nissues, e.g., diﬀerences in clinical practice. General model outcome deﬁnitions for\n\nEthical pipeline: The\nmodel development\nprocess and the\ncorresponding\nethical\nconsiderations\n\nBioethics: The study\nof ethical issues\nemerging from\nadvances in biology\nand medicine\n\nJustice: The\nprinciple that\nobligates equitably\ndistributed beneﬁts,\nrisks, costs, and\nresources\n\nBeneﬁcence: The\nprinciple that\nrequires that care be\nprovided with the\nintent of doing good\nfor the patient\ninvolved\n\nNon-maleﬁcence:\nThe principle that\nforbids harm or\ninjury to the patient,\neither through acts\nof commission or\nomission\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n3\n\nProblem SelectionData CollectionOutcome DefinitionAlgorithm DevelopmentPost-Deployment ConsiderationsDisparities in funding and problem selection priorities are an ethical violation of principles of justice.Focus on convenience samples can exacerbate existing disparities in marginalized and underserved populations, violating do-no-harm principles. Biased clinical knowledge, implicit power differentials, and social disparities of the healthcare system encode bias in outcomes that violate justice principles. Default practices, like evaluating performance on large populations, violate beneficence and justice principles when algorithms do not work for sub-populations.Targeted, spot-check audits and lack of model documentation ignore systematic shifts in populations risks patient safety, furthering risk to underserved groups. \fModel outcome: The\noutput of interest for\npredictive models\n\nDeployment: The\nprocess through\nwhich a machine\nlearning model is\nintegrated into an\nexisting production\nenvironment\n\nRisk score: A\ncalculated number\ndenoting the\nlikelihood of adverse\nevent\n\nmaternal health complications might overlook conditions speciﬁc to Black mothers,\ne.g., ﬁbroids (28).\n\n4. During algorithm development, models may not be able to account for the con-\nfounding presence of societal bias. Black mothers in the wealthiest neighborhoods in\nBrooklyn, New York have worse outcomes than white, Hispanic, and Asian mothers\nin the poorest ones, demonstrating a gap despite factors that should improve Black\nmothers’ outcomes — living in the same place, and having a higher income — likely\ndue to societal bias that impacts Black women (29).\n\n5. Finally, after a model is trained, post-deployment considerations may not fully\nconsider the impact of deploying a biased prediction model into clinical settings\nthat have large Black populations. Because Black women have a heightened risk\nof pregnancy-related death across income and education levels (30), a biased predic-\ntion model could potentially automate policies or risk scores that disadvantage Black\nmothers.\n\nWe organize the rest of this review sequentially expanding on each of the ﬁve steps in\nthe pipeline described above and in Figure 1. First, we look at problem selection, and\nexplain how funding for ML for health research can lead to injustice. We then examine\nhow data collection processes in funded research can amplify inequity and unfairness. We\nfollow this by exploring outcome deﬁnition and algorithm building, listing the multitude\nof factors that can impact model performance, and how these diﬀerences in performance\nrelate to issues of justice. We close with audits that should be considered for more robust\nand just deployments of models in health, and recommendations to practitioners for ethical,\nfair, and just ML deployments.\n\n2. PROBLEM SELECTION\n\nThere are many factors that inﬂuence the selection of a research problem, from interest\nto available funding. However, problem selection can also be a matter of justice if the\nresearch questions that are proposed, and ultimately funded, focus on the health needs of\nadvantaged groups. Below we provide examples of how disparities in research teams and\nfunding priorities exacerbate existing socioeconomic, racial, and gender injustices.\n\n2.1. Global Health Injustice\n\nThe “10/90” gap refers to the fact that the vast majority of health research dollars are spent\non problems that aﬀect a small fraction of the global population (31, 32). Diseases that are\nmost common in lower-income countries receive far less funding than diseases that are most\ncommon in high-income countries (33) (relative to the number of individuals they aﬀect).\nAs an example, 26 poverty-related diseases account for 14% of the global disease burden, but\nreceive only 1.3% of global health-related research and development expenditure. Nearly\n60% of the burden of poverty-related neglected diseases occurs in Western and Eastern sub-\nSaharan Africa as well as South Asia. Malaria, tuberculosis, and HIV/AIDS all have shares\nof global health-related research and development expenditure that are at least ﬁve times\nsmaller than their share of global disease burden (33). This diﬀerence in rates of funding\nrepresents an injustice because it further exacerbates the disadvantages faced by Global\nSouth populations. While eﬀorts like the “All Of Us” Project (34) and the 23andMe’s\nCall for Collaboration (35) seek to collect more inclusive data, these eﬀorts have come\n\n4\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\n\fGlobal South:\nCountries on one\nside of the\nNorth–South divide,\nthe other side being\nthe countries of the\nGlobal North\n\nunder criticism for not reﬂecting global health concerns, particularly among Indigenous\ngroups (36).\n\n2.2. Racial Injustice\n\nRacial bias aﬀects which health problems are prioritized and funded. For example, sickle\ncell disease and cystic ﬁbrosis are both genetic disorders of similar severity, but sickle cell\ndisease is more common in Black patients, while cystic ﬁbrosis is more common in white\npatients. In the United States (US), however, cystic ﬁbrosis receives 3.4 times more funding\nper aﬀected individual from the US National Institutes of Health (NIH), the largest funder\nof US clinical research, and hundreds of times more private funding (37). The disparities\nin funding persist despite the 1972 Sickle Cell Anemia Control Act, which recognizes that\nsickle cell has been neglected by the wider research community. Further, screening for sickle\ncell disease is viewed by some as unfair targeting (38), and Black patients with the disease\nwho seek treatment are often maligned as drug abusers (39).\n\n2.3. Gender Injustice\n\nWomen’s health conditions like endometriosis are poorly understood; as a consequence,\neven basic statistics like the prevalence of endometriosis remain unknown, with estimates\nranging from 1% to 10% of the population (40, 41). Similarly, the menstrual cycle is stig-\nmatized and understudied (40, 42), producing a dearth of understanding that undermines\nthe health of half the global population. Basic facts about the menstrual cycle — in-\ncluding which menstrual experiences are normal and which are predictive of pathology —\nremain unknown (40). This lack of focus on the menstrual cycle propagates into clinical\npractice and data collection despite evidence that it aﬀects many aspects of health and\ndisease (43, 44). Menstrual cycles are also not often recorded in clinical records and global\nhealth data (40).\nIn fact, the NIH did not have an R01 grant, the NIH’s original and\nhistorically oldest grant mechanism, relating to the inﬂuence of sex and gender on health\nand disease until 2019 (45). Notably, recent work has moved to target such understudied\nproblems via ambulatory women’s health-tracking mobile apps. These crowd-sourcing ef-\nforts stand to accelerate women’s health research by collecting data from cohorts that are\norders of magnitude larger than those used in previous studies (40).\n\n2.4. Diversity of the Scientiﬁc Workforce\n\nThe diversity of the scientiﬁc workforce profoundly inﬂuences the problems studied, and\ncontributes to the biases in problem selection (46). Research shows that scientists from\nunderrepresented racial and gender groups tend to prioritize diﬀerent research topics. They\nproduce more novel research, but their innovations are taken up at lower rates (47). Female\nscientists tend to study diﬀerent scientiﬁc subﬁelds, even within the same larger ﬁeld — for\nexample, within sociology, they have been historically better-represented on papers about\nsociology of the family or early childhood (48) — and express diﬀerent opinions about\nethical dilemmas in computer science (49). Proposals from white researchers in the US are\nmore likely to be funded by the NIH than proposals from Black researchers (50, 51), which\nin turns aﬀects what topics are given preference. For example, a higher fraction of NIH\nproposals from Black scientists study community and population-level health (50). Overall,\nthis evidence suggests that diversifying the scientiﬁc workforce will lead to problem selection\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n5\n\n\fthat more equitably represents the interests and needs of the population as a whole.\n\n3. DATA COLLECTION\n\nThe role of health data is ever-expanding, with new data sources routinely being integrated\ninto decision-making around health policy and design. This wealth of high-quality data,\ncoupled with advancements in ML models, has played a signiﬁcant role in accelerating\nthe use of computationally informed policy and practice to strengthen health care and\ndelivery platforms. Unfortunately, data can be biased in ways that have (or can lead to)\ndisproportionate negative impacts on already marginalized groups. First, data on group\nmembership can be completely absent. For instance, countries such as Canada and France\ndo not record race and ethnicity in their nationalized health databases (52, 53), making\nit impossible to study race-based disparities and hypotheses around associations of social\ndeterminants of health. Second, data can be imbalanced. Recent work on acute kidney\ninjury achieved state-of-the-art prediction performance in a large dataset of 703,782 adult\npatients using 620,000 features; however, they note that model performance was lower in\nfemale patients since female patients comprised 6.38% of patients in the training data (54).\nOther work has indicated that this issue can not be simply addressed by “pre-training” a\nmodel in a more balanced data setting prior to ﬁne-tuning on an imbalanced dataset (55).\nThis indicates that a model cannot be “initialized” with a balanced baseline representation\nwhich ameliorates issues of imbalance in downstream tasks, and suggests we must solve this\nproblem at the root, be it with more balanced are comprehensive data, specialty learning\nalgorithms, or combinations therein.Finally, while some sampling biases can be recognized\nand possibly corrected, others may be diﬃcult to correct. For example, work in medical\nimaging has demonstrated that models may overlook unforeseen stratiﬁcation of conditions,\nlike rare manifestations of diseases, which can result in harm in clinical settings (56, 16).\n\nIn this section, we discuss common biases in data collection. We consider two types of\nprocesses that result in a loss of data. First, processes that aﬀect what kind of information\nis collected, or heterogenous data loss, across varying input types. For example, clinical\ntrials with aggressive inclusion criteria or social media data that reﬂects those with access\nto devices. Second, we examine processes that aﬀect whether an individual’s information is\ncollected, or population-speciﬁc data losses, where individuals are impacted by their pop-\nulation type, often across data input categories. For example, undocumented immigrants\nmay fear deportation if they participate in health care systems.\n\n3.1. Heterogeneous Data Losses\n\nSome data loss is speciﬁc to the data type, due to assumptions about noise that may have\nbeen present during the collection process. However, data noise and missingness can cause\nunjust inequities that impact populations in diﬀerent ways. We cover four main data types:\nrandomized controlled trials (RCTs), electronic health care records (EHR), administrative\nhealth data, and social media data.\n\nRandomized Controlled Trials Randomized controlled trials are often run speciﬁ-\ncally to gather “unbiased” evidence of treatment eﬀects. However, RCTs have notoriously\naggressive exclusion (or inclusion) criteria (57), which create study cohorts that are not\nrepresentative of general patient populations (58). In one study of RCTs used to deﬁne\nasthma treatment, an estimated 94% of the adult asthmatic population would not have\n\n6\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\n\fbeen eligible for the trials (59). There is a growing methodological literature designing\nmethods to generalize RCT treatment eﬀects to other populations (60). However, current\nempirical evidence indicates that such generalizations can be challenging given available\ndata or may require strong assumptions in practice.\n\nElectronic Health Records Much recent work in ML also leverages large electronic\nhealth records data. EHR data are a complex reﬂection of patient health, health care\nsystems, and providers, where data missingness is a known, and meaningful, problem (61).\nAs one salient example, a large study of laboratory tests to model three-year survival\nfound that health care process features had a stronger predictive value than the patient’s\nphysiological feaures (62). Further, not all treatments investigated in RCTs can be easily\napproximated in EHR (63).\n\nBiases in EHR data may arise due to diﬀerences in patient populations, access to care,\nor the availability of EHR systems (64). As an example, the widely-used MIMIC-III EHR\ndataset includes most patients who receive care at the intensive care units in Beth Israel\nDeaconess Medical Center (BIDMC), but this sample is obviously limited by which indi-\nviduals have access to care at BIDMC, which has a largely white patient population (14).\nIn the United States, uninsured Black and Hispanic or Latin(o/x) patients, as well as His-\npanic or Latin(o/x) Medicaid patients, are less likely to have primary care providers with\nEHR systems, as compared to white patients with private insurance (65). Other work has\nshown that gender discrimination in health care access has not been systematically studied\nin India, primarily due to a lack of reliable data (66).\n\nAdministrative Health Records In addition to RCTs and EHR, health care billing\nclaims data, clinical registries, and linked health survey data are also common data sources\nin population health and health policy research (67, 68), with known biases in which popu-\nlations are followed, and who is able to participate. Translating such research into practice\nis a crucial part of maintaining health care quality, and limited participation of minority\npopulations by sexual orientation and gender identity (69), race and ethnicity (70), and\nlanguage (71) can lead to health interventions and policies that are not inclusive, and can\ncreate new injustices for these already marginalized groups.\n\nSocial Media Data Data from social media platforms and search-based research by\nnature consists only of individuals with internet access (72). Even small choices like limiting\nsamples to those from desktop versus mobile platforms are a problematic distinction in non-\nNorth American contexts (73). Beyond concerns about access to resources or geographic\nlimitations, data collection and scraping pipelines for most social media cohorts do not\nyield a random sample of individuals. Further, the common practice of limiting analysis to\nthose satisfying a speciﬁed threshold of occurrence can lead to skewed data. As an example,\nwhen processing the large volume of Twitter data (7.6 billion tweets) researchers may ﬁrst\nrestrict to users who can be mapped to a US county (1.78 billion), then to those Tweets\nthat contain only English (1.64 billion tweets), and ﬁnally remove users who made less than\n30 posts (1.53 billion) (74).\n\n3.2. Population-speciﬁc Data Losses\n\nAs with data types, the modern data deluge does not apply equally to all communities. His-\ntorically underserved groups are often underrepresented, misrepresented, or entirely miss-\ning from health data that inform consequential health policy decisions. When individuals\nfrom disadvantaged communities appear in observational datasets, they are less likely to be\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n7\n\nTraining data:\nInformation that a\nML model ﬁts to\nand learns patterns\nfrom\n\nHeterogeneous data\nloss: The process\nwhere data can be\nlost in collection due\nto data type\n\nPopulation-speciﬁc\ndata loss: The\nprocess where data\ncan be lost in\ncollection due to the\nfeatures of the\npopulation\n\nData noise:\nMeaningless\ninformation added to\ndata that obscures\nthe underlying\ninformation of the\ndata\n\nMissingness: The\nmanner in which\ndata is absent from\na sample of the\npopulation\n\nRandomized\ncontrolled trial\n(RCT): A study in\nwhich subjects are\nallocated by chance\nto receive one of\nseveral interventions\n\nElectronic health\nrecord (EHR):\nDigital version of a\npatient’s clinical\nhistory that is\nmaintained by the\nprovider over time\n\nIntervention: A\ntreatment,\nprocedure, or other\naction taken to\nprevent or treat\ndisease, or improve\nhealth in other ways\n\n\faccurately captured due to errors in data collection and systemic discrimination. Larger ge-\nnomics datasets often target European populations, producing genetic risk scores which are\nmore accurate in individuals of European ancestry than other ancestries (75). We note four\nspeciﬁc examples of populations that are commonly impacted: low- and middle-income na-\ntionals, transgender and gender non-conforming individuals, undocumented migrants, and\npregnant women.\n\nLow- and Middle-Income Nationals Health data are infrequently collected due to\nresource constraints, and even basic disease statistic data such as prevalence of mortality\nrates can be challenging to ﬁnd for low- and middle-income nations (73). When data\nare collected, it is not digitized, and often contains errors.\nIn 2001, the World Health\nOrganization found that only 9 out of the 46 member states in Sub-Saharan Africa could\nproduce death statistics for a global assessment of the burden of disease, with data coverage\noften less than 60% in these countries (76).\n\nTransgender and Gender Non-conforming Individuals The health care needs and\nexperiences of transgender and gender non-conforming individuals are not well-documented\nin datasets (77) because documented sex, not gender identity, is what is usually available.\nHowever, documented sex is often discordant with gender identity for transgender and gen-\nder non-conforming individuals. Apart from health documentation concerns, transgender\npeople are often concerned about their basic physical safety when reporting their identities.\nIn the US, it was only in 2016, with the release of the US Transgender Survey that there\nwas a meaningfully sized dataset — 28,000 respondents — to enable signiﬁcant analysis\nand quantiﬁcation of discrimination and violence that transgender people face (77).\n\nUndocumented Immigrants Safety concerns are important in data collection for\nundocumented migrants, where socio-political environments can lead to individuals feeling\nunsafe during reporting opportunities. When immigration policies limit access to public\nservices for immigrants and their families, these restrictions lead to spillover eﬀects on clin-\nical diagnoses. As one salient example, autism diagnoses for Hispanic children in California\nfell following aggressive federal anti-immigrant policies requiring citizenship veriﬁcation at\nhospitals (78).\n\nPregnant Women Despite pregnancy being neither rare nor an illness, the US contin-\nues to experience rising maternal mortality and morbidity rates. In the US, the maternal\nmortality rate has more than doubled from 9.8 per 100,000 live births in 2000 to 21.5 in\n2014 (79). Importantly, disclosure protocols recommend suppression of information in na-\ntionally available datasets when the number of cases or events in a data “cell” is low, to\nreduce the likelihood of a breach of conﬁdentiality. For example, the US Centers for Disease\nControl suppresses numbers for counties with fewer than 10 deaths for a given disease (80).\nAlthough these data omissions occur because of patient privacy, such censoring on the de-\npendent variable introduces particularly pernicious statistical bias and, as a result, much\nremains to be understood about what community, health facility, patient, and provider-level\nfactors drive high mortality rates.\n\n4. OUTCOME DEFINITION\n\nThe next step in the model pipeline is to deﬁne the outcome of interest for a health care task.\nEven seemingly straightforward tasks like deﬁning whether a patient has a disease can be\nskewed by how prevalent diseases are, or how they manifest in some patient populations. For\nexample, a model predicting if a patient will develop heart failure will need labeled examples\n\n8\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\nCensoring: The\nmechanism through\nwhich data values\nare removed from\nobservation\n\n\fboth of patients who have heart failure, and patients without heart failure. Choosing\nthese patients can rely on parts of the EHR that may be skewed due to lack of access to\ncare, or abnormalities in clinical care: e.g., economic incentives may alter diagnosis code\nlogging (81), clinical protocol aﬀects the frequency and observation of abnormal tests (62),\nhistorical racial mistrust may delay care and aﬀect patient outcomes (82), and naive data\ncollection can yield inconsistent labels in chest X-rays (56). Such biased labels, and the\nresulting models, may cause clinical practitioners to allocate resources poorly.\n\nWe discuss social justice considerations in two examples of commonly modelled health\ncare outcomes: clinical diagnosis and health care costs. In each example, it is essential that\nmodel developers choose a reliable proxy and account for noise in the outcome labels as\nthese choices can have a large impact on performance and equity of the resulting model.\n\n4.1. Clinical Diagnosis\n\nClinical diagnosis is a fundamental task for clinical prediction models, e.g., models for\ncomputer-aided diagnosis from medical imaging. In clinical settings, researchers often select\npatient disease occurrence as the prediction label for models. However, there are many\noptions for the choice of a disease occurrence label. For example, the outcome label for\ndeveloping cardiovascular disease could be deﬁned through the occurrence of speciﬁc phrases\nin clinical notes. However, women can manifest symptoms of acute coronary syndrome\ndiﬀerently (83) and receive delayed care as a result (84), which may then manifest in\ndiagnosis labels derived from the clinical notes being gender-skewed. Because diﬀerences\nin label noise results in disparities in model impact, researchers have the responsibility\nto choose and improve disease labels, so that these inequalities do not further exacerbate\ndisparities in health.\n\nAdditionally, it is important to consider the health care system in which disease labels\nare logged. For example, health care providers leverage diagnosis codes for billing purposes,\nnot clinical research. As a result, diagnosis codes can create ambiguities because of overlap\nand hierarchy in codes. Moreover, facilities have incentives to under-report (81) and over-\nreport (85, 86) outcomes, yielding diﬀerences in model representations.\n\nRecent advances in improving disease labels target statistical corrections based on esti-\nmates of the label noise. For instance, a positive label may be reliable, but the omission of a\npositive label could either indicate a negative label (i.e., no disease) or merely a missed pos-\nitive label. Methods to address the positive-unlabeled setting use estimated noise rates (87)\nor hand-curated labels that are strongly correlated with positive labels, known also as\n“silver-standard” labels, from clinicians (88). Clinical analysis of sources of error in disease\nlabels can also guide improvements (89) and identify aﬀected groups (56).\n\n4.2. Health Care Costs\n\nDevelopers of clinical models may choose to predict health care costs, meaning the ML model\nseeks to predict which patients will cost the health care provider more in the future. Some\nmodel developers may use health care costs as a proxy for future health needs to guide\naccurate targeting of interventions (8), with the underlying assumption that addressing\npatients with future health need will limit future costs. Others may explicitly want to\nunderstand patients who will have high health care cost to reduce the total cost of health\ncare (90). However, because socioeconomic factors aﬀect both access to health care and\naccess to ﬁnancial resources, these models may yield predictions that exacerbate inequities.\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n9\n\nLabel noise: Errors\nor otherwise\nobscuring\ninformation that\naﬀects the quality of\nthe labels\n\nDiagnosis code: A\nlabel in patient\nrecords of disease\noccurrence, which\nmay be subject to\nmisclassiﬁcation,\nused primarily for\nbilling purposes\n\n\fFor model developers seeking to optimize for health need, health care costs can deviate\nfrom health need on an individual level because of patient socioeconomic factors. For in-\nstance, in a model used to allocate care management program slots to high-risk patients,\nthe choice of future health care costs as a predictive outcome led to racial disparities in\npatient allocation to the program (8). Health care costs can diﬀer from health need on\nan institutional level due to underinsurance and undertreatment within the patient popu-\nlation (91). After deﬁning health disparities as all diﬀerences except those due to clinical\nneed and preferences, researchers have found racial disparities in mental health care. Specif-\nically, white patients had higher rates of initiation of treatment for mental health compared\nto Black and Hispanic or Latin(o/x) patients. Because the analysis controls for health\nneed, the disparities are solely a result of diﬀerences in health care access and systemic\ndiscrimination (92).\n\nAddressing issues that arise from the use of health care costs depends on the setting of\nthe ML model. In cases where health need is of highest importance, a natural solution is\nto choose another outcome deﬁnition besides health care costs, e.g., the number of chronic\ndiseases as a measure of health need. If a model developer is most concerned with cost, it is\npossible to correct for health disparities in predicting health care costs by building fairness\nconsiderations directly into the predictive model objective function (93). Building these\ntypes of algorithmic procedures is further discussed in Section 5.\n\n5. ALGORITHM DEVELOPMENT\n\nAlgorithm development considers the construction of the underlying computation for the\nML model and presents a major vulnerability and opportunity for ethical ML in health\ncare. Just as data are not neutral, algorithms are not neutral. A disproportionate amount\nof power lies with research teams who, after determining the research questions, make\ndecisions about critical components of an algorithm such as the loss function (94, 46). In\nthe case of loss functions, common choices like the L1 absolute error loss and L2 squared\nerror loss do not target the same conditional functions of the outcome, instead minimizing\nthe error in the median and mean respectively. Using a surrogate loss (e.g., hinge loss for\nthe error rate) can provide computational eﬃciency, but it may not reﬂect the ethical criteria\nthat we truly care about. Recent work has shown that models trained with a surrogate loss\nmay exhibit “approximation errors” that disproportionately aﬀect undersampled groups in\nthe training data (95). Similarly, one might choose to optimize the worst-case error across\ngroups as opposed to the average overall error. Such choices may seem purely technical, but\nreﬂect value statements about what should be optimized, potentially leading to diﬀerences\nin performance among marginalized groups (96).\n\nIn this section, we review several crucial factors in model development that potentially\nimpact ethical deployment capacity: understanding (and accounting for) confounding, fea-\nture selection, tuning parameters, and deﬁning “fairness” itself.\n\n5.1. Understanding Confounding\n\nDeveloping models that use sensitive attributes without a clear causal understanding of\ntheir relationship to outcomes of interest can signiﬁcantly aﬀect model performance and\ninterpretation. This is relevant to algorithmic problems focused on prediction, not just\ncausal inference. “Confounding” features — i.e., those features that inﬂuence both the\n\n10\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\nLoss function: The\nrelation that\ndetermines the error\nbetween algorithm\noutput and given\nlabel, which the\nalgorithm uses to\noptimize\n\n\fConfounding: The\ncondition in which a\nfeature inﬂuences\nboth the dependent\nvariable and\nindependent\nvariable, causing a\nspurious association\n\nTest data: Unseen\ninformation that a\nmodel predicts on\nand is evaluated\nagainst\n\nindependent variables and the dependent variable — require careful attention. The vast\nmajority of models learn patterns based on observed correlations between training data,\neven when such correlations do not occur in test data. For instance, recent work has\ndemonstrated that classiﬁcation models designed to detect hair color learn gender-biased\ndecision boundaries when trained on confounded data, i.e., if women are primarily blond\nin training data, the model incorrectly associates gender with the hair label in test samples\n(97).\n\nAs ML methods are increasingly used for clinical decision support, it is critical to ac-\ncount for confounding features. In one canonical example, asthmatic patients presenting\nwith pneumonia are given aggressive interventions that ultimately improve their chances of\nsurvival over non-asthmatic patients (98). When the hospital protocol assigned additional\ntreatment to patients with asthma, those patients had improved outcomes. Thus the treat-\nment policy was a confounding factor that altered the data in a seemingly straight-forward\nprediction task such that patients with asthma were erroneously predicted by models to\nhave lower risk of dying from pneumonia.\n\nSimply controlling for confounding features by including them as features in classiﬁca-\ntion or regression models may be insuﬃcient to learn reliable models because features can\nhave a mediating or moderating eﬀect (post-treatment eﬀect on outcomes of interest) and\nhave to be incorporated diﬀerently into model design (99).\n\nModern ML and causal discovery techniques can identify sources of confounding at\nscale (100), although validation of such methods can be challenging because of the lack\nof counterfactual data. ML methods have also been proposed to estimate causal eﬀects\nfrom observational data (101, 102). In practice, when potential hidden confounding is sus-\npected, either mediating features or proxies can be leveraged (103, 99) or sensitivity anal-\nysis methods can be used to determine potential sources of errors in eﬀect estimates (104).\nData-augmentation and sampling methods may also be used to mitigate eﬀects of model\nconfounding. For example, augmenting X-ray images with rotated and translated variants\ncan help train a model that is not sensitive to orientation of an image (105).\n\n5.2. Feature Selection\n\nWith large-scale digitization of EHR and other sources, sensitive attributes like race and\nethnicity may be increasingly available (although prone to misclassiﬁcation and missing-\nness). However, blindly incorporating factors like race and gender in a predictive model\nmay exacerbate inequities for a wide range of diagnostics and treatments (106). These\nresulting inequities can lead to unintended and permanent embedding of biases in algo-\nrithms used for clinical care. For example, vaginal birth after cesarean (VBAC) scores are\nused to predict success of “trial of labor” of pregnant women with a prior cesarean section;\nhowever, these scores explicitly include a race component as an input which reduces the\nchance of VBAC success for Black and Hispanic women. Although researchers found that\nprevious observational studies showed correlation between racial identity and success of\ntrial of labor (107), the underlying cause of this association is not well-understood. Such\nna¨ıve inclusion of race information could exacerbate disparities in maternal mortality. This\nambiguity calls race-based ‘correction’ in scores like VBAC into question (106).\n\nAutomation in feature selection does not eliminate the need for contextual understand-\ning. For example, stepwise regression is commonly used and taught as a technique for\nfeature selection despite known limitations (108). While speciﬁc methods have varying ini-\n\nSensitive attribute:\nA speciﬁed patient\nfeature (e.g., race,\ngender) which is\nconsidered\nimportant for\nfairness\nconsiderations\n\nStepwise regression:\nA method of\nestimation where\neach feature is\nsequentially\nconsidered by\naddition or\nsubtraction to the\nexisting feature set\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n11\n\n\fTuning parameters:\nAlgorithm\ncomponents used for\nprediction that are\ntuned toward solving\nan optimization\nproblem\n\nAUC: A measure of\nthe sensitivity and\nspeciﬁcity of a\nmodel for each\ndecision threshold\n\nAUPRC: A measure\nof precision and\nrecall of a model for\neach decision\nthreshold\n\nCalibration: A\nmeasure of how well\nML risk estimates\nreﬂect true risk\n\ntialization (e.g., start with an empty set of features or full set of features) and processing\nsteps (e.g., deletion vs. addition of features), most rely on p-values, R2, or other global ﬁt\nmetrics to select features. Weaknesses of stepwise regressions include the misleading nature\nof p-values and the ﬁnal set depending on if and when features were considered (109). In\nML, penalized regressions like lasso regression are popular for automated feature selection,\nbut the lasso trades potential increases in estimation bias for reductions in variance by\nshrinking some feature coeﬃcients to zero. Features selected by lasso may be co-linear with\nother features not selected (110). Over-interpretation of the selected features in any auto-\nmated procedures should therefore be avoided in practice given these pitfalls. Researchers\nshould also consider the humans-in-the-loop framework where incorporation of automated\nprocedures is blended with investigator knowledge (111).\n\n5.3. Tuning Parameters\n\nThere are many tuning parameters that may be set a priori or selected via cross-\nvalidation (110). These range from the learning rate in a neural network to the minimum\nsize of the terminal leaves in a random forest. In the latter example, default settings in R\nfor classiﬁcation will allow trees to grow until there is just one observation in a terminal\nleaf. This can lead to overﬁtting the model to the training data and a loss of generalizabil-\nity to the target population. Lack of generalizability is a central concern for ethical ML\ngiven the previously discussed issues in data collection and study inclusion. When data\nlack diversity and are not representative of the target population where the model would be\ndeployed, overﬁtting algorithms to this data has the potential to disproportionately harm\nmarginalized groups (112). Using cross-validation to select tuning parameters does not au-\ntomatically solve these problems as cross-validation still operates with respect to an a priori\nchosen optimization target.\n\n5.4. Performance Metrics\n\nThere are many commonly used performance metrics for model evaluation such as area\nunder the receiver-operating characteristic (AUC), precision-recall curves (AUPRC), and\ncalibration (113). However, the appropriate metrics to optimize depend on intended use\ncase and relative value of true positives, false positives, true negatives, and false negatives.\nNot only can AUC be misleading when considering other global ﬁt metrics (e.g., high AUC\nmasking weak true positive rate), but it does not describe the impact of the model across\nselected groups. Further, even “objective” metrics and scores can be deeply ﬂawed, and\nlead to over or under-treatment of minorities if blindly applied (114). Note that robust\nreporting of results should include an explicit statement of other non-optimized metrics,\nincluding the original intended use case, the training cohort and case, or level of model\nuncertainty.\n\n5.5. Group Fairness Deﬁnition\n\nThe speciﬁc deﬁnition of fairness for a given application often impacts the choice of a\nloss function, and therefore the underlying algorithm. Individual fairness imposes classiﬁer\nperformance requirements that operate over pairs of individuals, e.g., similar individuals\nshould be treated similarly (115). Group fairness operates over “protected groups” (based on\nsome sensitive attribute) by requiring that a classiﬁer performance metric be balanced across\n\n12\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\n\fthose groups.\n(116, 117). For instance, a model may be partially assessed by calculating\nthe true positive rate separately among rural and urban populations to ensure risk score\nsimilarity. Regressions subject to group fairness constraints or penalties optimizing toward\njoint global and group ﬁt considerations have also been developed (118, 119, 93).\n\nRecent work has focused on identifying and mitigating violations of fairness deﬁnitions\nin health care settings. While most of these algorithms have emerged outside the ﬁeld of\nhealth care, researchers have designed penalized and constrained regressions to improve\nthe performance of health insurance plan payment. This payment system impacts tens of\nmillions of lives in the United States and is known to undercompensate insurers for individ-\nuals with certain health conditions, including mental health and substance use disorders, in\npart because billing codes do not accurately capture diagnoses (120). Undercompensation\ncreates incentives for insurers exclude individuals with these health conditions from enroll-\nment, limiting their access to care. Regressions subject to group fairness constraints or\npenalties were successful in removing nearly all undercompensation for a single group with\nnegligible impacts on global ﬁt (93). Subsequent work incorporating multiple groups into\nthe loss function also saw improvements in undercompensation for the majority of groups\nnot included (121).\n\nPerformance metric:\nScore or other\nquantitative\nrepresentation of a\nmodel’s quality and\nability to achieve\ngoals\n\nAlgorithmic fairness:\nThe study of\ndeﬁnitions and\nmethods related to\nthe justice of models\n\nGroup fairness: A\nprinciple where\npre-deﬁned patient\ngroups should\nreceive similar model\nperformance\n\n6. POST-DEPLOYMENT CONSIDERATIONS\n\nOften the goal of model training is to ultimately deploy it in a clinical, epidemiological,\nor policy service. However, deployed models can have lasting ethical impact beyond the\nmodel performance measured in development. For example, in the inclusion of race in the\nclinical risk scores described earlier that may lead to chronic over- or under-treatment (106).\nHere we outline considerations for robust deployment by highlighting the need for careful\nperformance reporting, auditing generalizability, documentation, and regulation.\n\n6.1. Quantifying Impact\n\nUnlike in other settings with high-stakes decisions, e.g., aviation, clinical staﬀ performance\nis not audited by an external body (122).\nInstead, clinicians are often a self-governing\nbody, relying on clinicians themselves to determine when a colleague is underperforming or\nin breach of ethical practice principles, e.g., through such tools as surgical morbidity and\nmortality conferences (123). Clinical staﬀ can also struggle to keep abreast of what current\nbest practice recommendations are, as these can change dramatically over time; one study\nfound that more than 400 previously routine practices were later contradicted in leading\nclinical journals (124).\n\nHence, it is important to measure and address the downstream impact of models though\naudits for bias and examination of clinical impact (6). Regular “auditing” post-deployment,\ni.e., detailed inspection of model performance on various groups and outcomes, may reveal\nthe impact of models on diﬀerent populations (8) and identify areas of potential concern.\nSome recent work has targeted causal models in dynamic systems in order to reduce the\nseverity of bias (125). Others have targeted bias reduction through model construction\nwith explicit guarantees about balanced performance (16), or specifying groups which must\nhave equal performance (126). Additionally, there is the possibility that models may help\nto de-bias current clinical care by reducing known biases against minorities (127) and dis-\nadvantaged majorities (128).\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n13\n\nModel auditing: The\npost-deployment\ninspection of model\nperformance on\ngroups and outcomes\n\n\fGeneralizability: The\nability of a model to\napply in a setting\ndiﬀerent from the\none in which it was\ntrained\n\nData artifact: A ﬂaw\nin data caused by\nequipment,\ntechniques, or\nconditions that is\nunrelated to model\noutput\n\n6.2. Model Generalizability\n\nAs has been raised in previous sections, a crucial concern with model deployment is gener-\nalization. Any shifts in data distributions can signiﬁcantly impact model performance when\nthe settings for development and for deployment diﬀer. For example, chest X-ray diagnosis\nmodels can have high performance on test data drawn from the same hospital but degrade\nrapidly on data from another hospital (129). Other work in gender bias on chest X-ray data\nhas demonstrated both that small proportions of female chest x-rays degrades diagnostic\nperformance accuracy in female patients (130), and that this is not simply addressed in all\ncases by adding in more female X-rays (131). Even within a single hospital, models trained\non data from an initial EHR system data deteriorated signiﬁcantly when tested on data\nfrom a new EHR system (132). Finally, data artifacts that induce strong priors in what\npatterns ML models are sensitive to have the potential to perpetrate harms when used\nwithout awareness (133). For example, patients with dark skin can have morphological\nvariation and disease manifestations that are not easily detected under the defaults that\nare set by predominantly white-skinned patients (134).\n\nSeveral algorithms have recently been proposed to account for distribution shift in\ndata (135, 136). However, these algorithms have signiﬁcant limitations as they typically\nrequire assumptions about the nature or amount of distributional shift an algorithm can\naccommodate. Some, like (136), may require a clear indication of which distributions in\na health care pipeline are expected to change, and develop models for prediction accord-\ningly. Many of these assumptions may be veriﬁable. If not, periodically monitoring for data\nshifts (137), and potentially retraining models when performance deteriorates due to such\nshifts is an imperative deployment consideration with signiﬁcant ethical implications.\n\n6.3. Model and Data Documentation\n\nClear documentation enables insight into the model development and data collection. Good\nmodel documentation should include clinically speciﬁc features of model development that\ncan be assessed and recorded beforehand, such as logistics within the clinical setting, po-\ntential unintended consequences, and trade-oﬀs between bias and performance (138). In\naddition to raising ethical concerns in the pipeline, the process of co-designing “checklists”\nwith clinical practitioners formalizes ad-hoc procedures and empowers individual advo-\ncates (139). Standardized reporting of model performance—such as the one-page summary\n“model cards” for model reporting (140)—can empower clinical practitioners to understand\nmodel limitations and future model developers to identify areas of improvement. Similarly,\nbetter documentation of the data supporting initial model training can help expose sources\nof discrimination in the collected data. Modelers could use “datasheets” for datasets to\ndetail the conditions of data collection (141).\n\n6.4. Regulation\n\nIn the United States, the Food and Drug Administration (FDA) has responsibility for the\nregulation of health care ML models. As there does not exist comprehensive guidance for\nhealth care model research and subsequent deployment, the opportunity is ripe to create\na comprehensive framework to audit and regulate models. Currently, the FDA’s proposed\nML-speciﬁc modiﬁcations to the software as a medical device (SaMD) regulations draw a\ndistinction between models that are trained and then frozen prior to clinical deployment\n\n14\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\n\fFigure 2\n\nThe model development pipeline contains many challenges for ethical machine learning for health\ncare. We highlight both visible and hidden challenges.\n\nand models that continue to learn on observed outcomes. Although models in the latter\nclass can leverage larger, updated datasets, they also face additional risk due to model drift\nand may need additional audits (142). Such frameworks should explicitly account for health\ndisparities across the stages of ML development in health, and ensure health equity audits\nas part of postmarket evaluation (143). We also note that there are many potential legal\nimplications, e.g., in malpractice and liability suits, that will require new solutions (144).\n\nResearchers have proposed additional frameworks to guide clinical model development,\nwhich could inspire future regulation. ML model regulation could draw from existing reg-\nulatory frameworks: a randomized controlled trial for ML models would assess patient\nbeneﬁt compared to a control cohort of standard clinical practice (145), and a drug de-\nvelopment pipeline for ML models would deﬁne a protocol for adverse events and model\nrecalls (146). The clinical interventions accompanying the clinical ML model should be\nanalyzed to contextualize the use of the model in the clinical setting (147).\n\n7. RECOMMENDATIONS\n\nIn this review, we have described the ethical considerations at each step of the ML model\ndevelopment pipeline we introduced. While most researchers will address known challenges\nlike deployed task accuracy and outcome distribution shift, they are unlikely to be aware\nof the full magnitude of the hidden challenges such as existing health inequities or outcome\nlabel bias. As seen in Figure 2, many hidden pipeline challenges can go unaddressed in a\ntypical ML health project, but have serious ethical repercussions. With these challenges in\nmind, we propose ﬁve general recommendations that span the pipeline stages.\n\n1. Problems should be tackled by diverse teams and using frameworks that increase the\nprobability that equity will be achieved. Further, historically understudied problems\nare important targets to practitioners looking to perform high-impact work.\n\n2. Data collection should be framed as an important front-of-mind concern in the ML\nmodelling pipeline including clear disclosures about imbalanced datasets, and re-\nsearchers should engage with domain experts to ensure that data reﬂecting under-\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n15\n\nOff-the-shelf algorithms and assumptionsNaive inclusion of sensitive attributesLack of clinical algorithm regulationLack of model and data documentationConflicting algorithmic fairness definitionsVisible Pipeline ChallengesHidden  Pipeline ChallengesImbalanced or skewed datasetsDeployed task accuracy Confounding biasOutcome distribution shiftFunding and publication feasibilityUnderstudied targets to due lack of funding and publication Outcome label biasProblem selection biasExisting health inequitiesNon-representative research teamsChoice of ethical frameworkDifferences in patient treatmentModel generalizability across health institutions and across timeGroup fairness metricsPopulation specific data loss\fserved and understudied populations’ needs are gathered.\n\n3. Outcome choice should reﬂect the task at hand and should preferably be unbiased. If\nthe outcome label has ethical bias, the source of inequity should be accounted for in\nML model design, leveraging literature that attempts to remove ethical biases during\npre-processing, or with use of a reasonable proxy.\n\n4. Reﬂection on the goals of the model is essential during development, and should be\narticulated in a pre-analysis plan. In addition to technical choices like loss function,\nresearchers must interrogate how, and whether, a model should be developed to best\nanswer a research question, and what caveats are included.\n\n5. Audits should be designed to identify speciﬁc harms, and paired with methods and\nprocedures. Harms should be examined group-by-group, rather than at a population\nlevel. ML ethical design “checklists” are one possible tool to systematically enumerate\nand consider such ethical concerns prior to declaring success in a project.\n\nFinally, we note that machine learning also could and should be harnessed to create\nshifts in power in health care systems (148). This might mean actively selecting problems\nfor the beneﬁt of underserved patients, designing methods to target systemic interven-\ntions for improved access to care and treatments, or enforcing evaluations with the explicit\nIn one salient example, the state of California\npurpose of preserving patient autonomy.\nreduced disparities in rates of obstetric hemorrhage (and therefore maternal mortality for\nwomen of color) by weighing blood loss sponges, i.e., making access to treatment consis-\ntent and unbiased for all women (149). Models could similarly be harnessed to learn and\nrecommend consistent rules, giving researchers a potential opportunity to de-bias current\nclinical care (150), measure racial disparities and mistrust in end-of-life care (82), improve\nknown biases against minorities (127) and disadvantaged majorities (128). Ultimately, the\nresponsibility for ethical models and behavior lies with a broad community, but begins with\ntechnical researchers fulﬁlling an obligation to engage with patients, clinical researchers,\nstaﬀ, and advocates to build ethical models.\n\nFUTURE QUESTIONS\n\nhealth injustices?\n\n1. How can we combat urgent global health crises that exacerbate existing patterns of\n\n2. How can we encourage model developers to build ethical considerations into the\npipeline from the very beginning? Currently, when egregious cases of injustice are\ndiscovered only after clinical impact has already occurred, what can developers do\nto engage?\n\n3. How can evaluation and audits of ML systems be translated into meaningful clinical\npractice when, in many countries, clinicians themselves are subject to only limited\nexternal evaluations or audits?\n\n4. When, if ever, should sensitive attributes like race be used in analysis? How should\n\nwe incorporate socially constructed features into models and audits?\n\n5. How can ML be used to shift power, e.g., from well-known institutions, privileged\npatients, and wealthy multinational corporations to the patients most in need?\n\n16\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\n\fDISCLOSURE STATEMENT\n\nThe authors are not aware of any aﬃliations, memberships, funding, or ﬁnancial holdings\nthat might be perceived as aﬀecting the objectivity of this review.\n\nACKNOWLEDGMENTS\n\nThe authors thank Rediet Abebe for helpful discussions and contributions to an early draft,\nand Peter Szolovits, Pang Wei Koh, Leah Pierson, Berk Ustun, and Tristan Naumann for\nuseful comments and feedback. This work was supported in part by an NIH Director’s New\nInnovator Award DP2-MD012722 (SR), a CIFAR AI Chair at the Vector Institute (MG),\nand Microsoft Research (MG).\n\nLITERATURE CITED\n\n1. E. J. Topol, “High-performance medicine: the convergence of human and artiﬁcial intelli-\n\ngence,” Nature medicine, vol. 25, no. 1, pp. 44–56, 2019.\n\n2. K. Ferryman and R. A. Winn, “Artiﬁcial intelligence can entrench disparities—here’s what we\n\nmust do,” The Cancer Letter, Nov 2016.\n\n3. J. Wiens, S. Saria, M. Sendak, M. Ghassemi, V. X. Liu, F. Doshi-Velez, K. Jung, K. Heller,\nD. Kale, M. Saeed, et al., “Do no harm: a roadmap for responsible machine learning for health\ncare,” Nature medicine, vol. 25, no. 9, pp. 1337–1340, 2019.\n\n4. M. Ghassemi, T. Naumann, P. Schulam, A. L. Beam, I. Y. Chen, and R. Ranganath, “A\nreview of challenges and opportunities in machine learning for health,” AMIA Summits on\nTranslational Science Proceedings, vol. 2020, p. 191, 2020.\n\n5. M. Ghassemi, T. Naumann, P. Schulam, A. L. Beam, I. Y. Chen, and R. Ranganath, “Practical\nguidance on artiﬁcial intelligence for health-care data,” The Lancet Digital Health, vol. 1, no. 4,\npp. e157–e159, 2019.\n\n6. I. Y. Chen, P. Szolovits, and M. Ghassemi, “Can ai help reduce disparities in general medical\n\nand mental health care?,” AMA journal of ethics, vol. 21, no. 2, pp. 167–179, 2019.\n\n7. H. Zhang, A. X. Lu, M. Abdalla, M. McDermott, and M. Ghassemi, “Hurtful words: quanti-\nfying biases in clinical contextual word embeddings,” in Proceedings of the ACM Conference\non Health, Inference, and Learning, pp. 110–120, 2020.\n\n8. Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan, “Dissecting racial bias in an algo-\nrithm used to manage the health of populations,” Science, vol. 366, no. 6464, pp. 447–453,\n2019.\n\n9. D. Boyd and K. Crawford, “Critical questions for big data: Provocations for a cultural, tech-\nnological, and scholarly phenomenon,” Information, communication & society, vol. 15, no. 5,\npp. 662–679, 2012.\n\n10. C. M. Dalton, L. Taylor, and J. Thatcher, “Critical data studies: A dialog on data and space,”\n\nBig Data & Society, vol. 3, no. 1, p. 2053951716648346, 2016.\n\n11. I. Zliobaite, “A Survey on Measuring Indirect Discrimination in Machine Learning,” arXiv\n\n12. S. Barocas, M. Hardt, and A. Narayanan, Fairness and Machine Learning.\n\nfairmlbook.org,\n\npreprint arXiv:1511.00148, 2015.\n\n2018. http://www.fairmlbook.org.\n\n13. S. Corbett-Davies and S. Goel, “The measure and mismeasure of fairness: A critical review of\n\nfair machine learning,” arXiv preprint arXiv:1808.00023, 2018.\n\n14. I. Chen, F. D. Johansson, and D. Sontag, “Why is my classiﬁer discriminatory?,” in Advances\n\nin Neural Information Processing Systems, pp. 3539–3550, 2018.\n\n15. A. Rajkomar, M. Hardt, M. D. Howell, G. Corrado, and M. H. Chin, “Ensuring fairness in\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n17\n\n\fmachine learning to advance health equity,” Annals of Internal Medicine, vol. 169, pp. 866–\n872, 2018.\n\n16. B. Ustun, Y. Liu, and D. Parkes, “Fairness without harm: Decoupled classiﬁers with preference\n\nguarantees,” in International Conference on Machine Learning, pp. 6373–6382, 2019.\n\n17. R. Benjamin, “Assessing risk, automating racism,” Science, vol. 366, no. 6464, pp. 421–422,\n\n18. R. M. Veatch and L. K. Guidry-Grimes, The basics of bioethics. Routledge, 2019.\n19. E. Vayena, A. Blasimme, and I. G. Cohen, “Machine learning in medicine: Addressing ethical\n\nchallenges,” PLoS medicine, vol. 15, no. 11, p. e1002689, 2018.\n\n20. J. Kaye, “The tension between data sharing and the protection of privacy in genomics re-\n\nsearch,” Annual review of genomics and human genetics, vol. 13, pp. 415–431, 2012.\n\n21. M. Powers, R. R. Faden, R. R. Faden, et al., Social justice: the moral foundations of public\n\nhealth and health policy. Oxford University Press, USA, 2006.\n\n22. C. J. Berg, H. K. Atrash, L. M. Koonin, and M. Tucker, “Pregnancy-related mortality in the\n\nunited states, 1987–1990,” Obstetrics & Gynecology, vol. 88, no. 2, pp. 161–167, 1996.\n\n23. D. E. Roberts, Killing the black body: Race, reproduction, and the meaning of liberty. Vintage,\n\n2019.\n\n1999.\n\n24. D. R. Berry, The price for their pound of ﬂesh: The value of the enslaved, from womb to\n\ngrave, in the building of a nation. Beacon Press, 2017.\n\n25. N. Fisk and R. Atun, “Systematic analysis of research underfunding in maternal and perina-\ntal health,” BJOG: An International Journal of Obstetrics & Gynaecology, vol. 116, no. 3,\npp. 347–356, 2009.\n\n26. E. A. Howell, N. Egorova, A. Balbierz, J. Zeitlin, and P. L. Hebert, “Black-white diﬀerences\nin severe maternal morbidity and site of care,” American journal of obstetrics and gynecology,\nvol. 214, no. 1, pp. 122–e1, 2016.\n\n27. A. A. Creanga, B. T. Bateman, J. M. Mhyre, E. Kuklina, A. Shilkrut, and W. M. Callaghan,\n“Performance of racial and ethnic minority-serving hospitals on delivery-related indicators,”\nAmerican journal of obstetrics and gynecology, vol. 211, no. 6, pp. 647–e1, 2014.\n\n28. H. M. Eltoukhi, M. N. Modi, M. Weston, A. Y. Armstrong, and E. A. Stewart, “The health\ndisparities of uterine ﬁbroid tumors for african american women: a public health issue,” Amer-\nican journal of obstetrics and gynecology, vol. 210, no. 3, pp. 194–199, 2014.\n\n29. K. M. Hoﬀman, S. Trawalter, J. R. Axt, and M. N. Oliver, “Racial bias in pain assessment and\ntreatment recommendations, and false beliefs about biological diﬀerences between blacks and\nwhites,” Proceedings of the National Academy of Sciences, vol. 113, no. 16, pp. 4296–4301,\n2016.\n\n30. A. A. Creanga, C. J. Berg, J. Y. Ko, S. L. Farr, V. T. Tong, F. C. Bruce, and W. M.\nCallaghan, “Maternal mortality and morbidity in the united states: where are we now?,”\nJournal of women’s health, vol. 23, no. 1, pp. 3–9, 2014.\n\n31. D. Vidyasagar, “Global notes: the 10/90 gap disparities in global health research,” Journal of\n\nPerinatology, vol. 26, no. 1, pp. 55–56, 2006.\n\n32. L. Pierson and J. Millum, “Grant reviews and health research priority setting: Do research\n\nfunders uphold widely endorsed ethical principles?,” Working Paper, 2020.\n\n33. P. Von Philipsborn, F. Steinbeis, M. E. Bender, S. Regmi, and P. Tinnemann, “Poverty-related\nand neglected diseases–an economic and epidemiological analysis of poverty relatedness and\nneglect in research and development,” Global Health Action, vol. 8, no. 1, p. 25818, 2015.\n34. A. of Us Research Program Investigators, “The “all of us” research program,” New England\n\nJournal of Medicine, vol. 381, no. 7, pp. 668–676, 2019.\n\n35. a. under, “23andme’s call for collaborations to study underrepresented populations,” Mar\n\n2019.\n\n36. K. S. Tsosie, J. M. Yracheta, and D. Dickenson, “Overvaluing individual consent ignores risks\n\nto tribal participants,” Nature Reviews Genetics, vol. 20, no. 9, pp. 497–498, 2019.\n\n18\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\n\f37. F. Farooq and J. J. Strouse, “Disparities in foundation and federal support and development\nof new therapeutics for sickle cell disease and cystic ﬁbrosis,” Blood, vol. 132, pp. 4687–4687,\n2018.\n\n38. M. Park, “Ncaa genetic screening rule sparks discrimination concerns,” Aug 2010.\n39. C. Rouse, Uncertain suﬀering: racial health care disparities and sickle cell disease. Univ of\n\nCalifornia Press, 2009.\n\n40. S. Chakradhar, “Discovery cycle,” Nature Medicine, vol. 24, no. 8, pp. 1082–1086, 2018.\n41. V. Eisenberg, C. Weil, G. Chodick, and V. Shalev, “Epidemiology of endometriosis: a large\npopulation-based database study from a healthcare provider with 2 million members,” BJOG:\nAn International Journal of Obstetrics & Gynaecology, vol. 125, no. 1, pp. 55–62, 2018.\n42. E. Pierson, T. Althoﬀ, D. Thomas, P. Hillard, and J. Leskovec, “The menstrual cycle is a\nprimary contributor to cyclic variation in women’s mood, behavior, and vital signs,” Working\nPaper, 2019.\n\n43. P. J. A. Hillard, “Menstruation in Adolescents: What Do We Know? and What Do We\nDo with the Information?,” Journal of Pediatric and Adolescent Gynecology, vol. 27, no. 6,\npp. 309–319, 2014.\n\n44. American Academy of Pediatrics, Committee on Adolescence, American College of Obstetri-\ncians and Gynecologists, Committee on Adolescent Health Care, “Menstruation in girls and\nadolescents: using the menstrual cycle as a vital sign,” Pediatrics, vol. 118, no. 5, p. 2245,\n2006.\n\n45. “Nih oﬀers its ﬁrst research project grant (r01) on sex and gender.”\n46. M. Kasy and R. Abebe, “Fairness, equality, and power in algorithmic decision making,” tech.\n\nrep., Working paper, 2020.\n\n47. B. Hofstra, V. V. Kulkarni, S. M.-N. Galvez, B. He, D. Jurafsky, and D. A. McFarland, “The\ndiversity–innovation paradox in science,” Proceedings of the National Academy of Sciences,\nvol. 117, no. 17, pp. 9284–9291, 2020.\n\n48. J. D. West, J. Jacquet, M. M. King, S. J. Correll, and C. T. Bergstrom, “The role of gender\n\nin scholarly authorship,” PloS one, vol. 8, no. 7, p. e66212, 2013.\n\n49. E. Pierson, “Demographics and discussion inﬂuence views on algorithmic fairness,” arXiv\n\npreprint arXiv:1712.09124, 2017.\n\n50. T. A. Hoppe, A. Litovitz, K. A. Willis, R. A. Meseroll, M. J. Perkins, B. I. Hutchins, A. F.\nDavis, M. S. Lauer, H. A. Valantine, J. M. Anderson, et al., “Topic choice contributes to the\nlower rate of nih awards to african-american/black scientists,” Science advances, vol. 5, no. 10,\np. eaaw7238, 2019.\n\n51. D. K. Ginther, W. T. Schaﬀer, J. Schnell, B. Masimore, F. Liu, L. L. Haak, and R. Kington,\n“Race, ethnicity, and nih research awards,” Science, vol. 333, no. 6045, pp. 1015–1019, 2011.\n\n52. “Proposed standards for race-based and indigenous identity data,” July 2020.\n53. M. d. N. L´eonard, “Census and racial categorization in france: Invisible categories and color-\n\nblind politics,” Humanity & society, vol. 38, no. 1, pp. 67–88, 2014.\n\n54. N. Tomaˇsev, X. Glorot, J. W. Rae, M. Zielinski, H. Askham, A. Saraiva, A. Mottram, C. Meyer,\nS. Ravuri, I. Protsyuk, et al., “A clinically applicable approach to continuous prediction of\nfuture acute kidney injury,” Nature, vol. 572, no. 7767, pp. 116–119, 2019.\n\n55. M. B. A. McDermott, B. Nestor, E. Kim, W. Zhang, A. Goldenberg, P. Szolovits, and M. Ghas-\nsemi, “A comprehensive evaluation of multi-task learning and multi-task pre-training on ehr\ntime-series data,” 2020.\n\n56. L. Oakden-Rayner, J. Dunnmon, G. Carneiro, and C. R´e, “Hidden stratiﬁcation causes clini-\ncally meaningful failures in machine learning for medical imaging,” in Proceedings of the ACM\nConference on Health, Inference, and Learning, pp. 151–159, 2020.\n\n57. P. M. Rothwell, “External validity of randomised controlled trials:“to whom do the results of\n\nthis trial apply?”,” The Lancet, vol. 365, no. 9453, pp. 82–93, 2005.\n\n58. K. Courtright, “Point: Do randomized controlled trials ignore needed patient populations?\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n19\n\n\fyes,” Chest, vol. 149, no. 5, pp. 1128–1130, 2016.\n\n59. J. Travers, S. Marsh, M. Williams, M. Weatherall, B. Caldwell, P. Shirtcliﬀe, S. Aldington,\nand R. Beasley, “External validity of randomised controlled trials in asthma: to whom do the\nresults of the trials apply?,” Thorax, vol. 62, no. 3, pp. 219–223, 2007.\n\n60. E. A. Stuart, C. P. Bradshaw, and P. J. Leaf, “Assessing the generalizability of randomized\ntrial results to target populations,” Prevention Science, vol. 16, no. 3, pp. 475–485, 2015.\n61. B. J. Wells, K. M. Chagin, A. S. Nowacki, and M. W. Kattan, “Strategies for handling missing\n\ndata in electronic health record derived data,” Egems, vol. 1, no. 3, 2013.\n\n62. D. Agniel, I. S. Kohane, and G. M. Weber, “Biases in electronic health record data due to\nprocesses within the healthcare system: retrospective observational study,” Bmj, vol. 361,\n2018.\n\n63. V. L. Bartlett, S. S. Dhruva, N. D. Shah, P. Ryan, and J. S. Ross, “Feasibility of using\nreal-world data to replicate clinical trial evidence,” JAMA network open, vol. 2, no. 10,\npp. e1912869–e1912869, 2019.\n\n64. K. Ferryman and M. Pitcan, “Fairness in precision medicine,” Data & Society, 2018.\n65. E. Hing and C. W. Burt, “Are there patient disparities when electronic health records are\nadopted?,” Journal of health care for the poor and underserved, vol. 20, no. 2, pp. 473–488,\n2009.\n\n66. M. Kapoor, D. Agrawal, S. Ravi, A. Roy, S. Subramanian, and R. Guleria, “Missing female\npatients: an observational analysis of sex ratio among outpatients in a referral tertiary care\npublic hospital in india,” BMJ open, vol. 9, no. 8, p. e026850, 2019.\n\n67. S. J. A. Haneuse and S. M. Shortreed, On the use of electronic health records. Chapman and\n\nHall/CRC New York, NY, 2017.\n\n68. C. Wing, K. Simon, and R. A. Bello-Gomez, “Designing diﬀerence in diﬀerence studies: best\npractices for public health policy research,” Annual review of public health, vol. 39, 2018.\n69. E. J. Callahan, S. Hazarian, M. Yarborough, and J. P. S´anchez, “Eliminating lgbtiqq health\ndisparities: the associated roles of electronic health records and institutional culture,” Hastings\nCenter Report, vol. 44, no. s4, pp. S48–S52, 2014.\n\n70. M. M. L´opez, M. Bevans, L. Wehrlen, L. Yang, and G. Wallen, “Discrepancies in race and eth-\nnicity documentation: a potential barrier in identifying racial and ethnic disparities,” Journal\nof racial and ethnic health disparities, vol. 4, no. 5, pp. 812–818, 2017.\n\n71. E. V. Klinger, S. V. Carlini, I. Gonzalez, S. S. Hubert, J. A. Linder, N. A. Rigotti, E. Z.\nKontos, E. R. Park, L. X. Marinacci, and J. S. Haas, “Accuracy of race, ethnicity, and language\npreference in an electronic health record,” Journal of general internal medicine, vol. 30, no. 6,\npp. 719–723, 2015.\n\n72. M. Dredze, “How social media will change public health,” IEEE Intelligent Systems, vol. 27,\n\nno. 4, pp. 81–84, 2012.\n\n73. R. Abebe, S. Hill, J. W. Vaughan, P. M. Small, and H. A. Schwartz, “Using search queries\nto understand health information needs in africa,” in Proceedings of the International AAAI\nConference on Web and Social Media, vol. 13, pp. 3–14, 2019.\n\n74. S. Giorgi, D. Preot¸iuc-Pietro, A. Buﬀone, D. Rieman, L. Ungar, and H. A. Schwartz, “The\nremarkable beneﬁt of user-level aggregation for lexical-based population-level predictions,” in\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\npp. 1167–1172, 2018.\n\n75. A. R. Martin, M. Kanai, Y. Kamatani, Y. Okada, B. M. Neale, and M. J. Daly, “Clinical use\nof current polygenic risk scores may exacerbate health disparities,” Nature genetics, vol. 51,\nno. 4, pp. 584–591, 2019.\n\n76. D. T. Jamison et al., Disease and mortality in sub-Saharan Africa. World Bank Publications,\n\n77. S. James, J. Herman, S. Rankin, M. Keisling, L. Mottet, and M. Anaﬁ, “The report of the\n\n2006.\n\n2015 us transgender survey,” 2016.\n\n20\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\n\f78. C. Fountain and P. Bearman, “Risk as social context:\n\nimmigration policy and autism in\n\ncalifornia,” in Sociological Forum, vol. 26, pp. 215–240, Wiley Online Library, 2011.\n\n79. Y. C. Ai-ris and R. L. Molina, “Maternal mortality in the united states: updates on trends,\n\ncauses, and solutions,” Neoreviews, vol. 20, no. 10, pp. e561–e574, 2019.\n\n80. C. Tiwari, K. Beyer, and G. Rushton, “The impact of data suppression on local mortality rates:\nthe case of cdc wonder,” American journal of public health, vol. 104, no. 8, pp. 1386–1388,\n2014.\n\n81. A. S. Kesselheim and T. A. Brennan, “Overbilling vs. downcoding—the battle between physi-\ncians and insurers,” New England Journal of Medicine, vol. 352, no. 9, pp. 855–857, 2005.\n82. W. Boag, H. Suresh, L. A. Celi, P. Szolovits, and M. Ghassemi, “Racial disparities and mistrust\n\nin end-of-life care,” arXiv preprint arXiv:1808.03827, 2018.\n\n83. J. G. Canto, R. J. Goldberg, M. M. Hand, R. O. Bonow, G. Sopko, C. J. Pepine, and T. Long,\n“Symptom presentation of women with acute coronary syndromes: myth vs reality,” Archives\nof internal medicine, vol. 167, no. 22, pp. 2405–2413, 2007.\n\n84. R. Bugiardini, B. Ricci, E. Cenko, Z. Vasiljevic, S. Kedev, G. Davidovic, M. Zdravkovic,\nD. Miliˇci´c, M. Dilic, O. Manfrini, et al., “Delayed care and mortality among women and\nmen with myocardial infarction,” Journal of the American Heart Association, vol. 6, no. 8,\np. e005968, 2017.\n\n85. S. Rose, “A machine learning framework for plan payment risk adjustment,” Health services\n\nresearch, vol. 51, no. 6, pp. 2358–2374, 2016.\n\n86. M. Geruso and T. Layton, “Upcoding: Evidence from medicare on squishy risk adjustment,”\n\ntech. rep., National Bureau of Economic Research, 2015.\n\n87. N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learning with noisy labels,”\n\nin Advances in neural information processing systems, pp. 1196–1204, 2013.\n\n88. Y. Halpern, S. Horng, Y. Choi, and D. Sontag, “Electronic medical record phenotyping using\nthe anchor and learn framework,” Journal of the American Medical Informatics Association,\nvol. 23, no. 4, pp. 731–740, 2016.\n\n89. L. Oakden-Rayner, “Exploring large-scale public medical image datasets,” Academic Radiol-\n\nogy, vol. 27, no. 1, pp. 106–112, 2020.\n\n90. S. Tamang, A. Milstein, H. T. Sørensen, L. Pedersen, L. Mackey, J.-R. Betterton, L. Janson,\nand N. Shah, “Predicting patient ‘cost blooms’ in denmark: a longitudinal population-based\nstudy,” BMJ open, vol. 7, no. 1, p. e011580, 2017.\n\n91. B. L. Cook, T. G. McGuire, and A. M. Zaslavsky, “Measuring racial/ethnic disparities in health\ncare: methods and practical issues,” Health services research, vol. 47, no. 3pt2, pp. 1232–1254,\n2012.\n\n92. B. L. Cook, S. H. Zuvekas, N. Carson, G. F. Wayne, A. Vesper, and T. G. McGuire, “Assessing\nracial/ethnic disparities in treatment across episodes of mental health care,” Health services\nresearch, vol. 49, no. 1, pp. 206–229, 2014.\n\n93. A. Zink and S. Rose, “Fair regression for health care spending,” Biometrics, vol. 76, pp. 973–\n\n982, 2020.\n94. D. Guillory,\n\narXiv:2006.16879, 2020.\n\n“Combating\n\nanti-blackness\n\nin the\n\nai\n\ncommunity,”\n\narXiv\n\npreprint\n\n95. M. Lohaus, M. Perrot, and U. von Luxburg, “Too relaxed to be fair,” in International Con-\n\nference on Machine Learning, 2020.\n\n96. S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang, “Distributionally robust neural net-\nworks for group shifts: On the importance of regularization for worst-case generalization,”\nICLR 2020, 2019.\n\n97. S. Joshi, O. Koyejo, B. Kim, and J. Ghosh, “xgems: Generating examplars to explain black-\n\nbox models,” arXiv preprint arXiv:1806.08867, 2018.\n\n98. R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad, “Intelligible models for\nhealthcare: Predicting pneumonia risk and hospital 30-day readmission,” in Proceedings of\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n21\n\n\fthe 21th ACM SIGKDD international conference on knowledge discovery and data mining,\npp. 1721–1730, 2015.\n\n99. M. A. Hern´an and J. M. Robins, “Causal inference,” 2010.\n\n100. C. Glymour, K. Zhang, and P. Spirtes, “Review of causal discovery methods based on graphical\n\nmodels,” Frontiers in genetics, vol. 10, p. 524, 2019.\n\n101. M. J. Van der Laan and S. Rose, Targeted learning: causal inference for observational and\n\nexperimental data. Springer Science & Business Media, 2011.\n\n102. V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, C. Hansen, W. Newey, and J. Robins,\n\n“Double/debiased machine learning for treatment and structural parameters,” 2018.\n\n103. W. Miao, Z. Geng, and E. J. Tchetgen Tchetgen, “Identifying causal eﬀects with proxy vari-\n\nables of an unmeasured confounder,” Biometrika, vol. 105, no. 4, pp. 987–993, 2018.\n\n104. A. Franks, A. D’Amour, and A. Feller, “Flexible sensitivity analysis for observational studies\nwithout observable implications,” Journal of the American Statistical Association, pp. 1–33,\n2019.\n\n105. M. A. Little and R. Badawy, “Causal bootstrapping,” arXiv preprint arXiv:1910.09648, 2019.\n106. D. A. Vyas, L. G. Eisenstein, and D. S. Jones, “Hidden in plain sight—reconsidering the use\n\nof race correction in clinical algorithms,” 2020.\n\n107. W. A. Grobman, Y. Lai, M. B. Landon, C. Y. Spong, K. J. Leveno, D. J. Rouse, M. W.\nVarner, A. H. Moawad, S. N. Caritis, M. Harper, et al., “Development of a nomogram for\nprediction of vaginal birth after cesarean delivery,” Obstetrics & Gynecology, vol. 109, no. 4,\npp. 806–812, 2007.\n\n108. B. Thompson, “Stepwise regression and stepwise discriminant analysis need not apply here:\n\nA guidelines editorial,” 1995.\n\n109. F. E. Harrell Jr, Regression modeling strategies: with applications to linear models, logistic\n\nand ordinal regression, and survival analysis. Springer, 2015.\n\n110. G. James, D. Witten, T. Hastie, and R. Tibshirani, An introduction to statistical learning,\n\n111. P. W. Koh, T. Nguyen, Y. S. Tang, S. Mussmann, E. Pierson, B. Kim, and P. Liang, “Concept\n\nvol. 112. Springer, 2013.\n\nbottleneck models,” ICML, 2020.\n\n112. S. Sagawa, A. Raghunathan, P. W. Koh, and P. Liang, “An investigation of why overparame-\n\nterization exacerbates spurious correlations,” ICML, 2020.\n\n113. P. A. Flach, “The geometry of roc space: understanding machine learning metrics through roc\nisometrics,” in Proceedings of the 20th international conference on machine learning (ICML-\n03), pp. 194–201, 2003.\n\n114. D. A. Vyas, L. G. Eisenstein, and D. S. Jones, “Hidden in plain sight — reconsidering the\nuse of race correction in clinical algorithms,” New England Journal of Medicine, vol. 0, no. 0,\np. null, 0.\n\n115. C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness through awareness,” in\nProceedings of the 3rd innovations in theoretical computer science conference, pp. 214–226,\n2012.\n\n116. C. Dwork and C. Ilvento, “Group fairness under composition,” in Proceedings of the 2018\n\nConference on Fairness, Accountability, and Transparency (FAT* 2018), 2018.\n\n117. A. Chouldechova and A. Roth, “A snapshot of the frontiers of fairness in machine learning,”\n\nCommunications of the ACM, vol. 63, no. 5, pp. 82–89, 2020.\n\n118. T. Calders, A. Karim, F. Kamiran, W. Ali, and X. Zhang, “Controlling attribute eﬀect in\nlinear regression,” in 2013 IEEE 13th international conference on data mining, pp. 71–80,\nIEEE, 2013.\n\n119. M. B. Zafar, I. Valera, M. G. Rogriguez, and K. P. Gummadi, “Fairness constraints: Mecha-\nnisms for fair classiﬁcation,” in Artiﬁcial Intelligence and Statistics, pp. 962–970, 2017.\n120. E. Montz, T. Layton, A. B. Busch, R. P. Ellis, S. Rose, and T. G. McGuire, “Risk-adjustment\nsimulation: plans may have incentives to distort mental health and substance use coverage,”\n\n22\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\n\fHealth Aﬀairs, vol. 35, no. 6, pp. 1022–1028, 2016.\n\n121. T. G. McGuire, A. L. Zink, and S. Rose, “Simplifying and improving the performance of risk\n\nadjustment systems,” tech. rep., National Bureau of Economic Research, 2020.\n\n122. R. L. Helmreich, “On error management:\n\nlessons from aviation,” Bmj, vol. 320, no. 7237,\n\npp. 781–785, 2000.\n\n123. K. M. Murayama, A. M. Derossis, D. A. DaRosa, H. B. Sherman, and J. P. Fryer, “A critical\nevaluation of the morbidity and mortality conference,” The American journal of surgery,\nvol. 183, no. 3, pp. 246–250, 2002.\n\n124. D. Herrera-Perez, A. Haslam, T. Crain, J. Gill, C. Livingston, V. Kaestner, M. Hayes, D. Mor-\ngan, A. S. Cifu, and V. Prasad, “Meta-research: A comprehensive review of randomized clinical\ntrials in three medical journals reveals 396 medical reversals,” Elife, vol. 8, p. e45183, 2019.\n\n125. E. Creager, D. Madras, T. Pitassi, and R. Zemel, “Causal modeling for fairness in dynamical\n\nsystems,” arXiv preprint arXiv:1909.09141, 2019.\n\n126. P. A. Noseworthy, Z. I. Attia, L. C. Brewer, S. N. Hayes, X. Yao, S. Kapa, P. A. Friedman, and\nF. Lopez-Jimenez, “Assessing and mitigating bias in medical artiﬁcial intelligence: the eﬀects\nof race and ethnicity on a deep learning model for ecg analysis,” Circulation: Arrhythmia and\nElectrophysiology, vol. 13, no. 3, p. e007988, 2020.\n\n127. U. Treatment, “Confronting racial and ethnic disparities in health care,” Washington, DC,\n\n128. C. C. Perez, Invisible women: Exposing data bias in a world designed for men. Random\n\nInstitute of Medicine, 2002.\n\nHouse, 2019.\n\n129. J. R. Zech, M. A. Badgeley, M. Liu, A. B. Costa, J. J. Titano, and E. K. Oermann, “Variable\ngeneralization performance of a deep learning model to detect pneumonia in chest radiographs:\na cross-sectional study,” PLoS medicine, vol. 15, no. 11, p. e1002683, 2018.\n\n130. A. J. Larrazabal, N. Nieto, V. Peterson, D. H. Milone, and E. Ferrante, “Gender imbalance in\nmedical imaging datasets produces biased classiﬁers for computer-aided diagnosis,” Proceed-\nings of the National Academy of Sciences, vol. 117, no. 23, pp. 12592–12594, 2020.\n\n131. L. Seyyed-Kalantari, G. Liu, M. McDermott, and M. Ghassemi, “Chexclusion: Fairness gaps\n\nin deep chest x-ray classiﬁers,” arXiv preprint arXiv:2003.00827, 2020.\n\n132. B. Nestor, M. McDermott, W. Boag, G. Berner, T. Naumann, M. C. Hughes, A. Goldenberg,\nand M. Ghassemi, “Feature robustness in non-stationary health records: caveats to deploy-\nable model performance in common clinical machine learning tasks,” Machine Learning for\nHealthcare, 2019.\n\n133. A. Bissoto, M. Fornaciali, E. Valle, and S. Avila, “(de) constructing bias on skin lesion\ndatasets,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-\ntion Workshops, pp. 0–0, 2019.\n\n134. R. V. Kundu and S. Patterson, “Dermatologic conditions in skin of color: part i. special\nconsiderations for common skin disorders,” American family physician, vol. 87, no. 12, pp. 850–\n856, 2013.\n\n135. S. Rabanser, S. G¨unnemann, and Z. Lipton, “Failing loudly: An empirical study of meth-\nods for detecting dataset shift,” in Advances in Neural Information Processing Systems 32\n(H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e Buc, E. Fox, and R. Garnett, eds.),\npp. 1396–1408, Curran Associates, Inc., 2019.\n\n136. A. Subbaswamy and S. Saria, “From development to deployment: dataset shift, causality, and\nshift-stable models in health ai.,” Biostatistics (Oxford, England), vol. 21, no. 2, p. 345, 2020.\n137. S. E. Davis, T. A. Lasko, G. Chen, E. D. Siew, and M. E. Matheny, “Calibration drift in\nregression and machine learning models for acute kidney injury,” Journal of the American\nMedical Informatics Association, vol. 24, no. 6, pp. 1052–1061, 2017.\n\n138. S. Saleh, W. Boag, L. Erdman, and T. Naumann, “Clinical collabsheets: 53 questions to guide\n\na clinical collaboration,”\n\n139. M. A. Madaio, L. Stark, J. Wortman Vaughan, and H. Wallach, “Co-designing checklists to\n\nwww.annualreviews.org • Ethical Machine Learning in Health Care\n\n23\n\n\funderstand organizational challenges and opportunities around fairness in ai,” in Proceedings\nof the 2020 CHI Conference on Human Factors in Computing Systems, pp. 1–14, 2020.\n140. M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D.\nRaji, and T. Gebru, “Model cards for model reporting,” in Proceedings of the conference on\nfairness, accountability, and transparency, pp. 220–229, 2019.\n\n141. T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum´e III, and\n\nK. Crawford, “Datasheets for datasets,” arXiv preprint arXiv:1803.09010, 2018.\n\n142. C. for Devices and R. Health, “Artiﬁcial intelligence and machine learning in software,” Jan\n\n2020.\n\n143. K. Ferryman, “Addressing health disparities in the fda’s ai and machine learning regulatory\n\nframework.,” Journal of the American Medical Informatics Association, To Appear.\n\n144. H. R. Sullivan and S. J. Schweikart, “Are current tort liability doctrines adequate for addressing\n\ninjury caused by ai?,” AMA journal of ethics, vol. 21, no. 2, pp. 160–166, 2019.\n\n145. X. Liu, S. C. Rivera, L. Faes, L. F. Di Ruﬀano, C. Yau, P. A. Keane10, H. Ashraﬁan11,\nA. Darzi11, S. J. Vollmer, and J. Deeks, “Reporting guidelines for clinical trials evaluating\nartiﬁcial intelligence interventions are needed,” Nat. Med, vol. 25, pp. 1467–1468, 2019.\n146. A. Coravos, I. Chen, A. Gordhandas, and A. D. Stern, “We should treat algorithms like\n\nprescription drugs,” Quartz, 2019.\n\n147. R. B. Parikh, Z. Obermeyer, and A. S. Navathe, “Regulation of predictive analytics in\n\nmedicine,” Science, vol. 363, no. 6429, pp. 810–812, 2019.\n\n148. S. Mohamed, M.-T. Png, and W. Isaac, “Decolonial ai: Decolonial theory as sociotechnical\n\nforesight in artiﬁcial intelligence,” Philosophy & Technology, pp. 1–26, 2020.\n\n149. A. Lyndon, S. Francisco, J. Mcnulty, et al., “Cumulative quantitative assessment of blood\n\nloss,” CMQCC Obstetric Hemorrhage Toolkit Version, vol. 2, pp. 80–85, 2015.\n\n150. I. Y. Chen, S. Joshi, and M. Ghassemi, “Treating health disparities with artiﬁcial intelligence,”\n\nNature Medicine, vol. 26, no. 1, pp. 16–17, 2020.\n\n24\n\nChen, I.Y., Pierson, E., Rose, S., Joshi, S., Ferryman, K., Ghassemi, M.\n\n\f", "label": [[24504, 24517, "Dataset"]]}
