{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Scale GAN Training for High Fidelity Natural Image Synthesis : 1809.11096\n",
      "http://arxiv.org/pdf/1809.11096v2\n",
      "9\n",
      "1\n",
      "0\n",
      "2\n",
      " \n",
      "b\n",
      "e\n",
      "F\n",
      " \n",
      "5\n",
      "2\n",
      " \n",
      " \n",
      "]\n",
      "\n",
      "G\n",
      "L\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "2\n",
      "v\n",
      "6\n",
      "9\n",
      "0\n",
      "1\n",
      "1\n",
      ".\n",
      "9\n",
      "0\n",
      "8\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "LARGE SCALE GAN TRAINING FOR\n",
      "HIGH FIDELITY NATURAL IMAGE SYNTHESIS\n",
      "\n",
      "Andrew Brock∗ †\n",
      "Heriot-Watt University\n",
      "ajb5@hw.ac.uk\n",
      "\n",
      "Jeff Donahue†\n",
      "DeepMind\n",
      "jeffdonahue@google.com\n",
      "\n",
      "Karen Simonyan†\n",
      "DeepMind\n",
      "simonyan@google.com\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Despite recent progress in generative image modeling, successfully generating\n",
      "high-resolution, diverse samples from complex datasets such as ImageNet remains\n",
      "an elusive goal. To this end, we train Generative Adversarial Networks at the\n",
      "largest scale yet attempted, and study the instabilities speciﬁc to such scale. We\n",
      "ﬁnd that applying orthogonal regularization to the generator renders it amenable\n",
      "to a simple “truncation trick,” allowing ﬁne control over the trade-off between\n",
      "sample ﬁdelity and variety by reducing the variance of the Generator’s input. Our\n",
      "modiﬁcations lead to models which set the new state of the art in class-conditional\n",
      "image synthesis. When trained on ImageNet at 128×128 resolution, our models\n",
      "(BigGANs) achieve an Inception Score (IS) of 166.5 and Fr´echet Inception Dis-\n",
      "tance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Figure 1: Class-conditional samples generated by our model.\n",
      "\n",
      "The state of generative image modeling has advanced dramatically in recent years, with Generative\n",
      "Adversarial Networks (GANs, Goodfellow et al. (2014)) at the forefront of efforts to generate high-\n",
      "ﬁdelity, diverse images with models learned directly from data. GAN training is dynamic, and\n",
      "sensitive to nearly every aspect of its setup (from optimization parameters to model architecture),\n",
      "but a torrent of research has yielded empirical and theoretical insights enabling stable training in\n",
      "a variety of settings. Despite this progress, the current state of the art in conditional ImageNet\n",
      "modeling (Zhang et al., 2018) achieves an Inception Score (Salimans et al., 2016) of 52.5, compared\n",
      "to 233 for real data.\n",
      "\n",
      "In this work, we set out to close the gap in ﬁdelity and variety between images generated by GANs\n",
      "and real-world images from the ImageNet dataset. We make the following three contributions to-\n",
      "wards this goal:\n",
      "\n",
      "• We demonstrate that GANs beneﬁt dramatically from scaling, and train models with two\n",
      "to four times as many parameters and eight times the batch size compared to prior art. We\n",
      "introduce two simple, general architectural changes that improve scalability, and modify a\n",
      "regularization scheme to improve conditioning, demonstrably boosting performance.\n",
      "\n",
      "∗Work done at DeepMind\n",
      "†Equal contribution\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "• As a side effect of our modiﬁcations, our models become amenable to the “truncation\n",
      "trick,” a simple sampling technique that allows explicit, ﬁne-grained control of the trade-\n",
      "off between sample variety and ﬁdelity.\n",
      "\n",
      "• We discover instabilities speciﬁc to large scale GANs, and characterize them empirically.\n",
      "Leveraging insights from this analysis, we demonstrate that a combination of novel and\n",
      "existing techniques can reduce these instabilities, but complete training stability can only\n",
      "be achieved at a dramatic cost to performance.\n",
      "\n",
      "Our modiﬁcations substantially improve class-conditional GANs. When trained on ImageNet at\n",
      "128×128 resolution, our models (BigGANs) improve the state-of-the-art Inception Score (IS) and\n",
      "Fr´echet Inception Distance (FID) from 52.52 and 18.65 to 166.5 and 7.4 respectively. We also\n",
      "successfully train BigGANs on ImageNet at 256×256 and 512×512 resolution, and achieve IS and\n",
      "FID of 232.5 and 8.1 at 256×256 and IS and FID of 241.5 and 11.5 at 512×512. Finally, we train\n",
      "our models on an even larger dataset – JFT-300M – and demonstrate that our design choices transfer\n",
      "well from ImageNet. Code and weights for our pretrained generators are publicly available 1.\n",
      "\n",
      "2 BACKGROUND\n",
      "\n",
      "A Generative Adversarial Network (GAN) involves Generator (G) and Discriminator (D) networks\n",
      "whose purpose, respectively, is to map random noise to samples and discriminate real and generated\n",
      "samples. Formally, the GAN objective, in its original form (Goodfellow et al., 2014) involves ﬁnding\n",
      "a Nash equilibrium to the following two player min-max problem:\n",
      "\n",
      "min\n",
      "G\n",
      "\n",
      "max\n",
      "D\n",
      "\n",
      "Ex∼qdata(x)[log D(x)] + Ez∼p(z)[log(1 − D(G(z)))],\n",
      "\n",
      "(1)\n",
      "\n",
      "where z ∈ Rdz is a latent variable drawn from distribution p(z) such as N (0, I) or U[−1, 1].\n",
      "When applied to images, G and D are usually convolutional neural networks (Radford et al., 2016).\n",
      "Without auxiliary stabilization techniques, this training procedure is notoriously brittle, requiring\n",
      "ﬁnely-tuned hyperparameters and architectural choices to work at all.\n",
      "\n",
      "Much recent research has accordingly focused on modiﬁcations to the vanilla GAN procedure to\n",
      "impart stability, drawing on a growing body of empirical and theoretical insights (Nowozin et al.,\n",
      "2016; Sønderby et al., 2017; Fedus et al., 2018). One line of work is focused on changing the\n",
      "objective function (Arjovsky et al., 2017; Mao et al., 2016; Lim & Ye, 2017; Bellemare et al.,\n",
      "2017; Salimans et al., 2018) to encourage convergence. Another line is focused on constraining\n",
      "D through gradient penalties (Gulrajani et al., 2017; Kodali et al., 2017; Mescheder et al., 2018)\n",
      "or normalization (Miyato et al., 2018), both to counteract the use of unbounded loss functions and\n",
      "ensure D provides gradients everywhere to G.\n",
      "\n",
      "Of particular relevance to our work is Spectral Normalization (Miyato et al., 2018), which enforces\n",
      "Lipschitz continuity on D by normalizing its parameters with running estimates of their ﬁrst singular\n",
      "values, inducing backwards dynamics that adaptively regularize the top singular direction. Relatedly\n",
      "Odena et al. (2018) analyze the condition number of the Jacobian of G and ﬁnd that performance is\n",
      "dependent on G’s conditioning. Zhang et al. (2018) ﬁnd that employing Spectral Normalization in\n",
      "G improves stability, allowing for fewer D steps per iteration. We extend on these analyses to gain\n",
      "further insight into the pathology of GAN training.\n",
      "\n",
      "Other works focus on the choice of architecture, such as SA-GAN (Zhang et al., 2018) which adds\n",
      "the self-attention block from (Wang et al., 2018) to improve the ability of both G and D to model\n",
      "global structure. ProGAN (Karras et al., 2018) trains high-resolution GANs in the single-class\n",
      "setting by training a single model across a sequence of increasing resolutions.\n",
      "\n",
      "In conditional GANs (Mirza & Osindero, 2014) class information can be fed into the model in\n",
      "In (Odena et al., 2017) it is provided to G by concatenating a 1-hot class vector\n",
      "various ways.\n",
      "to the noise vector, and the objective is modiﬁed to encourage conditional samples to maximize\n",
      "the corresponding class probability predicted by an auxiliary classiﬁer. de Vries et al. (2017) and\n",
      "\n",
      "1https://tfhub.dev/s?q=biggan\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Batch Ch.\n",
      "64\n",
      "256\n",
      "64\n",
      "512\n",
      "64\n",
      "1024\n",
      "64\n",
      "2048\n",
      "96\n",
      "2048\n",
      "96\n",
      "2048\n",
      "96\n",
      "2048\n",
      "96\n",
      "2048\n",
      "64\n",
      "2048\n",
      "\n",
      "Param (M)\n",
      "81.5\n",
      "81.5\n",
      "81.5\n",
      "81.5\n",
      "173.5\n",
      "160.6\n",
      "158.3\n",
      "158.3\n",
      "71.3\n",
      "\n",
      "Shared\n",
      "\n",
      "Skip-z Ortho.\n",
      "\n",
      "SA-GAN Baseline\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "Itr ×103\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "732\n",
      "295(±18)\n",
      "185(±11)\n",
      "152(±7)\n",
      "165(±13)\n",
      "371(±7)\n",
      "\n",
      "FID\n",
      "18.65\n",
      "15.30\n",
      "14.88\n",
      "12.39\n",
      "9.54(±0.62)\n",
      "9.18(±0.13)\n",
      "8.73(±0.45)\n",
      "8.51(±0.32)\n",
      "10.48(±0.10)\n",
      "\n",
      "IS\n",
      "52.52\n",
      "58.77(±1.18)\n",
      "63.03(±1.42)\n",
      "76.85(±3.83)\n",
      "92.98(±4.27)\n",
      "94.94(±1.32)\n",
      "98.76(±2.84)\n",
      "99.31(±2.10)\n",
      "86.90(±0.61)\n",
      "\n",
      "Table 1: Fr´echet Inception Distance (FID, lower is better) and Inception Score (IS, higher is better)\n",
      "for ablations of our proposed modiﬁcations. Batch is batch size, Param is total number of param-\n",
      "eters, Ch. is the channel multiplier representing the number of units in each layer, Shared is using\n",
      "shared embeddings, Skip-z is using skip connections from the latent to multiple layers, Ortho. is\n",
      "Orthogonal Regularization, and Itr indicates if the setting is stable to 106 iterations, or it collapses\n",
      "at the given iteration. Other than rows 1-4, results are computed across 8 random initializations.\n",
      "\n",
      "Dumoulin et al. (2017) modify the way class conditioning is passed to G by supplying it with class-\n",
      "conditional gains and biases in BatchNorm (Ioffe & Szegedy, 2015) layers. In Miyato & Koyama\n",
      "(2018), D is conditioned by using the cosine similarity between its features and a set of learned\n",
      "class embeddings as additional evidence for distinguishing real and generated samples, effectively\n",
      "encouraging generation of samples whose features match a learned class prototype.\n",
      "\n",
      "Objectively evaluating implicit generative models is difﬁcult (Theis et al., 2015). A variety of works\n",
      "have proposed heuristics for measuring the sample quality of models without tractable likelihoods\n",
      "(Salimans et al., 2016; Heusel et al., 2017; Bi´nkowski et al., 2018; Wu et al., 2017). Of these,\n",
      "the Inception Score (IS, Salimans et al. (2016)) and Fr´echet Inception Distance (FID, Heusel et al.\n",
      "(2017)) have become popular despite their notable ﬂaws (Barratt & Sharma, 2018). We employ\n",
      "them as approximate measures of sample quality, and to enable comparison against previous work.\n",
      "\n",
      "3 SCALING UP GANS\n",
      "\n",
      "In this section, we explore methods for scaling up GAN training to reap the performance beneﬁts of\n",
      "larger models and larger batches. As a baseline, we employ the SA-GAN architecture of Zhang et al.\n",
      "(2018), which uses the hinge loss (Lim & Ye, 2017; Tran et al., 2017) GAN objective. We provide\n",
      "class information to G with class-conditional BatchNorm (Dumoulin et al., 2017; de Vries et al.,\n",
      "2017) and to D with projection (Miyato & Koyama, 2018). The optimization settings follow Zhang\n",
      "et al. (2018) (notably employing Spectral Norm in G) with the modiﬁcation that we halve the learning\n",
      "rates and take two D steps per G step. For evaluation, we employ moving averages of G’s weights\n",
      "following Karras et al. (2018); Mescheder et al. (2018); Yazc et al. (2018), with a decay of 0.9999.\n",
      "We use Orthogonal Initialization (Saxe et al., 2014), whereas previous works used N (0, 0.02I)\n",
      "(Radford et al., 2016) or Xavier initialization (Glorot & Bengio, 2010). Each model is trained on\n",
      "128 to 512 cores of a Google TPUv3 Pod (Google, 2018), and computes BatchNorm statistics in G\n",
      "across all devices, rather than per-device as is typical. We ﬁnd progressive growing (Karras et al.,\n",
      "2018) unnecessary even for our 512×512 models. Additional details are in Appendix C.\n",
      "\n",
      "We begin by increasing the batch size for the baseline model, and immediately ﬁnd tremendous\n",
      "beneﬁts in doing so. Rows 1-4 of Table 1 show that simply increasing the batch size by a factor of\n",
      "8 improves the state-of-the-art IS by 46%. We conjecture that this is a result of each batch covering\n",
      "more modes, providing better gradients for both networks. One notable side effect of this scaling is\n",
      "that our models reach better ﬁnal performance in fewer iterations, but become unstable and undergo\n",
      "complete training collapse. We discuss the causes and ramiﬁcations of this in Section 4. For these\n",
      "experiments, we report scores from checkpoints saved just before collapse.\n",
      "\n",
      "We then increase the width (number of channels) in each layer by 50%, approximately doubling the\n",
      "number of parameters in both models. This leads to a further IS improvement of 21%, which we\n",
      "posit is due to the increased capacity of the model relative to the complexity of the dataset. Doubling\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2: (a) The effects of increasing truncation. From left to right, the threshold is set to 2, 1, 0.5,\n",
      "0.04. (b) Saturation artifacts from applying truncation to a poorly conditioned model.\n",
      "\n",
      "the depth did not initially lead to improvement – we addressed this later in the BigGAN-deep model,\n",
      "which uses a different residual block structure.\n",
      "\n",
      "We note that class embeddings c used for the conditional BatchNorm layers in G contain a large\n",
      "number of weights. Instead of having a separate layer for each embedding (Miyato et al., 2018;\n",
      "Zhang et al., 2018), we opt to use a shared embedding, which is linearly projected to each layer’s\n",
      "gains and biases (Perez et al., 2018). This reduces computation and memory costs, and improves\n",
      "training speed (in number of iterations required to reach a given performance) by 37%. Next, we\n",
      "add direct skip connections (skip-z) from the noise vector z to multiple layers of G rather than just\n",
      "the initial layer. The intuition behind this design is to allow G to use the latent space to directly in-\n",
      "ﬂuence features at different resolutions and levels of hierarchy. In BigGAN, this is accomplished by\n",
      "splitting z into one chunk per resolution, and concatenating each chunk to the conditional vector c\n",
      "which gets projected to the BatchNorm gains and biases. In BigGAN-deep, we use an even simpler\n",
      "design, concatenating the entire z with the conditional vector without splitting it into chunks. Pre-\n",
      "vious works (Goodfellow et al., 2014; Denton et al., 2015) have considered variants of this concept;\n",
      "our implementation is a minor modiﬁcation of this design. Skip-z provides a modest performance\n",
      "improvement of around 4%, and improves training speed by a further 18%.\n",
      "\n",
      "3.1 TRADING OFF VARIETY AND FIDELITY WITH THE TRUNCATION TRICK\n",
      "\n",
      "Unlike models which need to backpropagate through their latents, GANs can employ an arbitrary\n",
      "prior p(z), yet the vast majority of previous works have chosen to draw z from either N (0, I) or\n",
      "U[−1, 1]. We question the optimality of this choice and explore alternatives in Appendix E.\n",
      "\n",
      "Remarkably, our best results come from using a different latent distribution for sampling than was\n",
      "used in training. Taking a model trained with z ∼ N (0, I) and sampling z from a truncated nor-\n",
      "mal (where values which fall outside a range are resampled to fall inside that range) immediately\n",
      "provides a boost to IS and FID. We call this the Truncation Trick:\n",
      "truncating a z vector by re-\n",
      "sampling the values with magnitude above a chosen threshold leads to improvement in individual\n",
      "sample quality at the cost of reduction in overall sample variety. Figure 2(a) demonstrates this: as\n",
      "the threshold is reduced, and elements of z are truncated towards zero (the mode of the latent dis-\n",
      "tribution), individual samples approach the mode of G’s output distribution. Related observations\n",
      "about this trade-off were made in (Marchesi, 2016; Pieters & Wiering, 2014).\n",
      "\n",
      "This technique allows ﬁne-grained, post-hoc selection of the trade-off between sample quality and\n",
      "variety for a given G. Notably, we can compute FID and IS for a range of thresholds, obtaining the\n",
      "variety-ﬁdelity curve reminiscent of the precision-recall curve (Figure 17). As IS does not penal-\n",
      "ize lack of variety in class-conditional models, reducing the truncation threshold leads to a direct\n",
      "increase in IS (analogous to precision). FID penalizes lack of variety (analogous to recall) but also\n",
      "rewards precision, so we initially see a moderate improvement in FID, but as truncation approaches\n",
      "zero and variety diminishes, the FID sharply drops. The distribution shift caused by sampling with\n",
      "different latents than those seen in training is problematic for many models. Some of our larger\n",
      "models are not amenable to truncation, producing saturation artifacts (Figure 2(b)) when fed trun-\n",
      "cated noise. To counteract this, we seek to enforce amenability to truncation by conditioning G to be\n",
      "smooth, so that the full space of z will map to good output samples. For this, we turn to Orthogonal\n",
      "Regularization (Brock et al., 2017), which directly enforces the orthogonality condition:\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Rβ(W ) = β(cid:107)W (cid:62)W − I(cid:107)2\n",
      "F,\n",
      "\n",
      "(2)\n",
      "\n",
      "where W is a weight matrix and β a hyperparameter. This regularization is known to often be too\n",
      "limiting (Miyato et al., 2018), so we explore several variants designed to relax the constraint while\n",
      "still imparting the desired smoothness to our models. The version we ﬁnd to work best removes the\n",
      "diagonal terms from the regularization, and aims to minimize the pairwise cosine similarity between\n",
      "ﬁlters but does not constrain their norm:\n",
      "\n",
      "Rβ(W ) = β(cid:107)W (cid:62)W (cid:12) (1 − I)(cid:107)2\n",
      "F,\n",
      "where 1 denotes a matrix with all elements set to 1. We sweep β values and select 10−4, ﬁnding\n",
      "this small added penalty sufﬁcient to improve the likelihood that our models will be amenable to\n",
      "truncation. Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of\n",
      "models are amenable to truncation, compared to 60% when trained with Orthogonal Regularization.\n",
      "\n",
      "(3)\n",
      "\n",
      "We ﬁnd that current GAN techniques are sufﬁcient to enable scaling to large models and distributed,\n",
      "large-batch training. We ﬁnd that we can dramatically improve the state of the art and train models\n",
      "up to 512×512 resolution without need for explicit multiscale methods like Karras et al. (2018).\n",
      "Despite these improvements, our models undergo training collapse, necessitating early stopping in\n",
      "practice. In the next two sections we investigate why settings which were stable in previous works\n",
      "become unstable when applied at scale.\n",
      "\n",
      "3.2 SUMMARY\n",
      "\n",
      "4 ANALYSIS\n",
      "\n",
      "(a) G\n",
      "\n",
      "(b) D\n",
      "\n",
      "Figure 3: A typical plot of the ﬁrst singular value σ0 in the layers of G (a) and D (b) before Spectral\n",
      "Normalization. Most layers in G have well-behaved spectra, but without constraints a small sub-\n",
      "set grow throughout training and explode at collapse. D’s spectra are noisier but otherwise better-\n",
      "behaved. Colors from red to violet indicate increasing depth.\n",
      "\n",
      "4.1 CHARACTERIZING INSTABILITY: THE GENERATOR\n",
      "\n",
      "Much previous work has investigated GAN stability from a variety of analytical angles and on\n",
      "toy problems, but the instabilities we observe occur for settings which are stable at small scale,\n",
      "necessitating direct analysis at large scale. We monitor a range of weight, gradient, and loss statistics\n",
      "during training, in search of a metric which might presage the onset of training collapse, similar to\n",
      "(Odena et al., 2018). We found the top three singular values σ0, σ1, σ2 of each weight matrix to be\n",
      "the most informative. They can be efﬁciently computed using the Alrnoldi iteration method (Golub\n",
      "& der Vorst, 2000), which extends the power iteration method, used in Miyato et al. (2018), to\n",
      "estimation of additional singular vectors and values. A clear pattern emerges, as can be seen in\n",
      "Figure 3(a) and Appendix F: most G layers have well-behaved spectral norms, but some layers\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(typically the ﬁrst layer in G, which is over-complete and not convolutional) are ill-behaved, with\n",
      "spectral norms that grow throughout training and explode at collapse.\n",
      "\n",
      "To ascertain if this pathology is a cause of collapse or merely a symptom, we study the effects of\n",
      "imposing additional conditioning on G to explicitly counteract spectral explosion. First, we directly\n",
      "regularize the top singular values σ0 of each weight, either towards a ﬁxed value σreg or towards\n",
      "some ratio r of the second singular value, r · sg(σ1) (with sg the stop-gradient operation to prevent\n",
      "the regularization from increasing σ1). Alternatively, we employ a partial singular value decompo-\n",
      "sition to instead clamp σ0. Given a weight W , its ﬁrst singular vectors u0 and v0, and σclamp the\n",
      "value to which the σ0 will be clamped, our weights become:\n",
      "\n",
      "W = W − max(0, σ0 − σclamp)v0u(cid:62)\n",
      "0 ,\n",
      "where σclamp is set to either σreg or r · sg(σ1). We observe that both with and without Spectral\n",
      "Normalization these techniques have the effect of preventing the gradual increase and explosion of\n",
      "either σ0 or σ0\n",
      ", but even though in some cases they mildly improve performance, no combination\n",
      "σ1\n",
      "prevents training collapse. This evidence suggests that while conditioning G might improve stability,\n",
      "it is insufﬁcient to ensure stability. We accordingly turn our attention to D.\n",
      "\n",
      "(4)\n",
      "\n",
      "4.2 CHARACTERIZING INSTABILITY: THE DISCRIMINATOR\n",
      "\n",
      "As with G, we analyze the spectra of D’s weights to gain insight into its behavior, then seek to\n",
      "stabilize training by imposing additional constraints. Figure 3(b) displays a typical plot of σ0 for D\n",
      "(with further plots in Appendix F). Unlike G, we see that the spectra are noisy, σ0\n",
      "is well-behaved,\n",
      "σ1\n",
      "and the singular values grow throughout training but only jump at collapse, instead of exploding.\n",
      "\n",
      "The spikes in D’s spectra might suggest that it periodically receives very large gradients, but we\n",
      "observe that the Frobenius norms are smooth (Appendix F), suggesting that this effect is primarily\n",
      "concentrated on the top few singular directions. We posit that this noise is a result of optimization\n",
      "through the adversarial training process, where G periodically produces batches which strongly per-\n",
      "turb D . If this spectral noise is causally related to instability, a natural counter is to employ gradient\n",
      "penalties, which explicitly regularize changes in D’s Jacobian. We explore the R1 zero-centered\n",
      "gradient penalty from Mescheder et al. (2018):\n",
      "\n",
      "R1 :=\n",
      "\n",
      "EpD(x)\n",
      "\n",
      "(cid:2)(cid:107)∇D(x)(cid:107)2\n",
      "\n",
      "(cid:3) .\n",
      "\n",
      "F\n",
      "\n",
      "γ\n",
      "2\n",
      "\n",
      "(5)\n",
      "\n",
      "With the default suggested γ strength of 10, training becomes stable and improves the smoothness\n",
      "and boundedness of spectra in both G and D, but performance severely degrades, resulting in a 45%\n",
      "reduction in IS. Reducing the penalty partially alleviates this degradation, but results in increasingly\n",
      "ill-behaved spectra; even with the penalty strength reduced to 1 (the lowest strength for which sud-\n",
      "den collapse does not occur) the IS is reduced by 20%. Repeating this experiment with various\n",
      "strengths of Orthogonal Regularization, DropOut (Srivastava et al., 2014), and L2 (See Appendix I\n",
      "for details), reveals similar behaviors for these regularization strategies: with high enough penalties\n",
      "on D, training stability can be achieved, but at a substantial cost to performance.\n",
      "\n",
      "We also observe that D’s loss approaches zero during training, but undergoes a sharp upward jump at\n",
      "collapse (Appendix F). One possible explanation for this behavior is that D is overﬁtting to the train-\n",
      "ing set, memorizing training examples rather than learning some meaningful boundary between real\n",
      "and generated images. As a simple test for D’s memorization (related to Gulrajani et al. (2017)), we\n",
      "evaluate uncollapsed discriminators on the ImageNet training and validation sets, and measure what\n",
      "percentage of samples are classiﬁed as real or generated. While the training accuracy is consistently\n",
      "above 98%, the validation accuracy falls in the range of 50-55%, no better than random guessing\n",
      "(regardless of regularization strategy). This conﬁrms that D is indeed memorizing the training set;\n",
      "we deem this in line with D’s role, which is not explicitly to generalize, but to distill the training\n",
      "data and provide a useful learning signal for G. Additional experiments and discussion are provided\n",
      "in Appendix G.\n",
      "\n",
      "4.3 SUMMARY\n",
      "\n",
      "We ﬁnd that stability does not come solely from G or D, but from their interaction through the\n",
      "adversarial training process. While the symptoms of their poor conditioning can be used to track and\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Model\n",
      "SN-GAN\n",
      "SA-GAN\n",
      "BigGAN\n",
      "BigGAN\n",
      "BigGAN\n",
      "BigGAN-deep\n",
      "BigGAN-deep\n",
      "BigGAN-deep\n",
      "\n",
      "Res.\n",
      "128\n",
      "128\n",
      "128\n",
      "256\n",
      "512\n",
      "128\n",
      "256\n",
      "512\n",
      "\n",
      "FID/IS\n",
      "27.62/36.80\n",
      "18.65/52.52\n",
      "8.7 ± .6/98.8 ± 3\n",
      "8.7 ± .1/142.3 ± 2\n",
      "8.1/144.2\n",
      "5.7 ± .3/124.5 ± 2\n",
      "6.9 ± .2/171.4 ± 2\n",
      "7.5/152.8\n",
      "\n",
      "(min FID) / IS\n",
      "N/A\n",
      "N/A\n",
      "7.7 ± .2/126.5 ± 0\n",
      "7.7 ± .1/178.0 ± 5\n",
      "7.6/170.3\n",
      "6.3 ± .3/148.1 ± 4\n",
      "7.0 ± .1/202.6 ± 2\n",
      "7.7/181.4\n",
      "\n",
      "FID / (valid IS)\n",
      "N/A\n",
      "N/A\n",
      "9.6 ± .4/166.3 ± 1\n",
      "9.3 ± .3/233.1 ± 1\n",
      "11.8/241.4\n",
      "7.4 ± .6/166.5 ± 1\n",
      "8.1 ± .1/232.5 ± 2\n",
      "11.5/241.5\n",
      "\n",
      "FID / (max IS)\n",
      "N/A\n",
      "N/A\n",
      "25 ± 2/206 ± 2\n",
      "25 ± 5/291 ± 4\n",
      "27.0/275\n",
      "25 ± 2/253 ± 11\n",
      "27 ± 8/317 ± 6\n",
      "39.7/298\n",
      "\n",
      "Table 2: Evaluation of models at different resolutions. We report scores without truncation (Column\n",
      "3), scores at the best FID (Column 4), scores at the IS of validation data (Column 5), and scores at\n",
      "the max IS (Column 6). Standard deviations are computed over at least three random initializations.\n",
      "\n",
      "identify instability, ensuring reasonable conditioning proves necessary for training but insufﬁcient to\n",
      "prevent eventual training collapse. It is possible to enforce stability by strongly constraining D, but\n",
      "doing so incurs a dramatic cost in performance. With current techniques, better ﬁnal performance\n",
      "can be achieved by relaxing this conditioning and allowing collapse to occur at the later stages of\n",
      "training, by which time a model is sufﬁciently trained to achieve good results.\n",
      "\n",
      "5 EXPERIMENTS\n",
      "\n",
      "(a) 128×128\n",
      "\n",
      "(b) 256×256\n",
      "\n",
      "(c) 512×512\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 4: Samples from our BigGAN model with truncation threshold 0.5 (a-c) and an example of\n",
      "class leakage in a partially trained model (d).\n",
      "\n",
      "5.1 EVALUATION ON IMAGENET\n",
      "\n",
      "We evaluate our models on ImageNet ILSVRC 2012 (Russakovsky et al., 2015) at 128×128,\n",
      "256×256, and 512×512 resolutions, employing the settings from Table 1, row 8. The samples\n",
      "generated by our models are presented in Figure 4, with additional samples in Appendix A, and on-\n",
      "line 2. We report IS and FID in Table 2. As our models are able to trade sample variety for quality, it\n",
      "is unclear how best to compare against prior art; we accordingly report values at three settings, with\n",
      "complete curves in Appendix D. First, we report the FID/IS values at the truncation setting which\n",
      "attains the best FID. Second, we report the FID at the truncation setting for which our model’s IS is\n",
      "the same as that attained by the real validation data, reasoning that this is a passable measure of max-\n",
      "imum sample variety achieved while still achieving a good level of “objectness.” Third, we report\n",
      "FID at the maximum IS achieved by each model, to demonstrate how much variety must be traded\n",
      "off to maximize quality. In all three cases, our models outperform the previous state-of-the-art IS\n",
      "and FID scores achieved by Miyato et al. (2018) and Zhang et al. (2018).\n",
      "\n",
      "In addition to the BigGAN model introduced in the ﬁrst version of the paper and used in the majority\n",
      "of experiments (unless otherwise stated), we also present a 4x deeper model (BigGAN-deep) which\n",
      "uses a different conﬁguration of residual blocks. As can be seen from Table 2, BigGAN-deep sub-\n",
      "stantially outperforms BigGAN across all resolutions and metrics. This conﬁrms that our ﬁndings\n",
      "\n",
      "2https://drive.google.com/drive/folders/1lWC6XEPD0LT5KUnPXeve_kWeY-FxH002\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Skip-z Ortho.\n",
      "\n",
      "Ch.\n",
      "64\n",
      "64\n",
      "96\n",
      "128\n",
      "\n",
      "Param (M)\n",
      "317.1\n",
      "99.4\n",
      "207.9\n",
      "355.7\n",
      "\n",
      "Shared\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "FID\n",
      "48.38\n",
      "23.48\n",
      "18.84\n",
      "13.75\n",
      "\n",
      "IS\n",
      "23.27\n",
      "24.78\n",
      "27.86\n",
      "30.61\n",
      "\n",
      "(min FID) / IS\n",
      "48.6/23.1\n",
      "22.4/21.0\n",
      "17.1/23.3\n",
      "13.0/28.0\n",
      "\n",
      "FID / (max IS)\n",
      "49.1/23.9\n",
      "60.9/35.8\n",
      "51.6/38.1\n",
      "46.2/47.8\n",
      "\n",
      "Table 3: BigGAN results on JFT-300M at 256×256 resolution. The FID and IS columns report these\n",
      "scores given by the JFT-300M-trained Inception v2 classiﬁer with noise distributed as z ∼ N (0, I)\n",
      "(non-truncated). The (min FID) / IS and FID / (max IS) columns report scores at the best FID and\n",
      "IS from a sweep across truncated noise distributions ranging from σ = 0 to σ = 2. Images from the\n",
      "JFT-300M validation set have an IS of 50.88 and FID of 1.94.\n",
      "\n",
      "extend to other architectures, and that increased depth leads to improvement in sample quality. Both\n",
      "BigGAN and BigGAN-deep architectures are described in Appendix B.\n",
      "\n",
      "Our observation that D overﬁts to the training set, coupled with our model’s sample quality, raises\n",
      "the obvious question of whether or not G simply memorizes training points. To test this, we perform\n",
      "class-wise nearest neighbors analysis in pixel space and the feature space of pre-trained classiﬁer\n",
      "networks (Appendix A). In addition, we present both interpolations between samples and class-wise\n",
      "interpolations (where z is held constant) in Figures 8 and 9. Our model convincingly interpolates\n",
      "between disparate samples, and the nearest neighbors for its samples are visually distinct, suggesting\n",
      "that our model does not simply memorize training data.\n",
      "\n",
      "We note that some failure modes of our partially-trained models are distinct from those previously\n",
      "observed. Most previous failures involve local artifacts (Odena et al., 2016), images consisting\n",
      "of texture blobs instead of objects (Salimans et al., 2016), or the canonical mode collapse. We\n",
      "observe class leakage, where images from one class contain properties of another, as exempliﬁed\n",
      "by Figure 4(d). We also ﬁnd that many classes on ImageNet are more difﬁcult than others for our\n",
      "model; our model is more successful at generating dogs (which make up a large portion of the\n",
      "dataset, and are mostly distinguished by their texture) than crowds (which comprise a small portion\n",
      "of the dataset and have more large-scale structure). Further discussion is available in Appendix A.\n",
      "\n",
      "5.2 ADDITIONAL EVALUATION ON JFT-300M\n",
      "\n",
      "To conﬁrm that our design choices are effective for even larger and more complex and diverse\n",
      "datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017). The\n",
      "full JFT-300M dataset contains 300M real-world images labeled with 18K categories. Since the\n",
      "category distribution is heavily long-tailed, we subsample the dataset to keep only images with the\n",
      "8.5K most common labels. The resulting dataset contains 292M images – two orders of magnitude\n",
      "larger than ImageNet. For images with multiple labels, we sample a single label randomly and\n",
      "independently whenever an image is sampled. To compute IS and FID for the GANs trained on this\n",
      "dataset, we use an Inception v2 classiﬁer (Szegedy et al., 2016) trained on this dataset. Quantitative\n",
      "results are presented in Table 3. All models are trained with batch size 2048. We compare an ablated\n",
      "version of our model – comparable to SA-GAN (Zhang et al., 2018) but with the larger batch size\n",
      "– against a “full” BigGAN model that makes uses of all of the techniques applied to obtain the\n",
      "best results on ImageNet (shared embedding, skip-z, and orthogonal regularization). Our results\n",
      "show that these techniques substantially improve performance even in the setting of this much larger\n",
      "dataset at the same model capacity (64 base channels). We further show that for a dataset of this\n",
      "scale, we see signiﬁcant additional improvements from expanding the capacity of our models to 128\n",
      "base channels, while for ImageNet GANs that additional capacity was not beneﬁcial.\n",
      "\n",
      "In Figure 19 (Appendix D), we present truncation plots for models trained on this dataset. Unlike\n",
      "for ImageNet, where truncation limits of σ ≈ 0 tend to produce the highest ﬁdelity scores, IS is\n",
      "typically maximized for our JFT-300M models when the truncation value σ ranges from 0.5 to 1.\n",
      "We suspect that this is at least partially due to the intra-class variability of JFT-300M labels, as well\n",
      "as the relative complexity of the image distribution, which includes images with multiple objects at a\n",
      "variety of scales. Interestingly, unlike models trained on ImageNet, where training tends to collapse\n",
      "without heavy regularization (Section 4), the models trained on JFT-300M remain stable over many\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "hundreds of thousands of iterations. This suggests that moving beyond ImageNet to larger datasets\n",
      "may partially alleviate GAN stability issues.\n",
      "\n",
      "The improvement over the baseline GAN model that we achieve on this dataset without changes to\n",
      "the underlying models or training and regularization techniques (beyond expanded capacity) demon-\n",
      "strates that our ﬁndings extend from ImageNet to datasets with scale and complexity thus far un-\n",
      "precedented for generative models of images.\n",
      "\n",
      "We have demonstrated that Generative Adversarial Networks trained to model natural images of\n",
      "multiple categories highly beneﬁt from scaling up, both in terms of ﬁdelity and variety of the gen-\n",
      "erated samples. As a result, our models set a new level of performance among ImageNet GAN\n",
      "models, improving on the state of the art by a large margin. We have also presented an analysis\n",
      "of the training behavior of large scale GANs, characterized their stability in terms of the singular\n",
      "values of their weights, and discussed the interplay between stability and performance.\n",
      "\n",
      "We would like to thank Kai Arulkumaran, Matthias Bauer, Peter Buchlovsky, Jeffrey Defauw,\n",
      "Sander Dieleman, Ian Goodfellow, Ariel Gordon, Karol Gregor, Dominik Grewe, Chris Jones, Jacob\n",
      "Menick, Augustus Odena, Suman Ravuri, Ali Razavi, Mihaela Rosca, and Jeff Stanway.\n",
      "\n",
      "6 CONCLUSION\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu\n",
      "Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,\n",
      "Rajat Monga, Sherry Moore, Derek Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete\n",
      "Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A system for large-scale\n",
      "machine learning. In OSDI, 2016.\n",
      "\n",
      "Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.\n",
      "\n",
      "Shane Barratt and Rishi Sharma. A note on the Inception Score. In arXiv preprint arXiv:1801.01973,\n",
      "\n",
      "In ICML, 2017.\n",
      "\n",
      "2018.\n",
      "\n",
      "Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,\n",
      "Stephan Hoyer, and R´emi Munos. The Cramer distance as a solution to biased Wasserstein gra-\n",
      "dients. In arXiv preprint arXiv:1705.10743, 2017.\n",
      "\n",
      "Mikolaj Bi´nkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD\n",
      "\n",
      "GANs. In ICLR, 2018.\n",
      "\n",
      "Andrew Brock, Theodore Lim, J.M. Ritchie, and Nick Weston. Neural photo editing with introspec-\n",
      "\n",
      "tive adversarial networks. In ICLR, 2017.\n",
      "\n",
      "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:\n",
      "Interpretable representation learning by information maximizing generative adversarial nets. In\n",
      "NIPS, 2016.\n",
      "\n",
      "Harm de Vries, Florian Strub, J´er´emie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron\n",
      "\n",
      "Courville. Modulating early visual processing by language. In NIPS, 2017.\n",
      "\n",
      "Emily Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models\n",
      "\n",
      "using a laplacian pyramid of adversarial networks. In NIPS, 2015.\n",
      "\n",
      "Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic\n",
      "\n",
      "style. In ICLR, 2017.\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and\n",
      "Ian Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every\n",
      "step. In ICLR, 2018.\n",
      "\n",
      "Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural\n",
      "\n",
      "networks. In AISTATS, 2010.\n",
      "\n",
      "Gene Golub and Henk Van der Vorst. Eigenvalue computation in the 20th century. Journal of\n",
      "\n",
      "Computational and Applied Mathematics, 123:35–65, 2000.\n",
      "\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\n",
      "\n",
      "and Aaron Courville Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.\n",
      "\n",
      "Google. Cloud TPUs. https://cloud.google.com/tpu/, 2018.\n",
      "\n",
      "Ishaan Gulrajani, Faruk Ahmed, Mart´ın Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Im-\n",
      "\n",
      "proved training of Wasserstein GANs. In NIPS, 2017.\n",
      "\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n",
      "\n",
      "nition. In CVPR, 2016.\n",
      "\n",
      "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G¨unter Klambauer, and\n",
      "Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilib-\n",
      "rium. In NIPS, 2017.\n",
      "\n",
      "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\n",
      "\n",
      "reducing internal covariate shift. In ICML, 2015.\n",
      "\n",
      "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for\n",
      "\n",
      "improved quality, stability, and variation. In ICLR, 2018.\n",
      "\n",
      "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.\n",
      "\n",
      "Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of\n",
      "\n",
      "GANs. In arXiv preprint arXiv:1705.07215, 2017.\n",
      "\n",
      "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\n",
      "\n",
      "Jae Hyun Lim and Jong Chul Ye. Geometric GAN. In arXiv preprint arXiv:1705.02894, 2017.\n",
      "\n",
      "Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, and Zhen Wang. Least squares generative\n",
      "\n",
      "adversarial networks. In arXiv preprint arXiv:1611.04076, 2016.\n",
      "\n",
      "Marco Marchesi. Megapixel size image creation using generative adversarial networks. In arXiv\n",
      "\n",
      "preprint arXiv:1706.00082, 2016.\n",
      "\n",
      "actually converge? In ICML, 2018.\n",
      "\n",
      "arXiv:1411.1784, 2014.\n",
      "\n",
      "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do\n",
      "\n",
      "Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets.\n",
      "\n",
      "In arXiv preprint\n",
      "\n",
      "Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In ICLR, 2018.\n",
      "\n",
      "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization\n",
      "\n",
      "for generative adversarial networks. In ICLR, 2018.\n",
      "\n",
      "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural sam-\n",
      "\n",
      "plers using variational divergence minimization. In NIPS, 2016.\n",
      "\n",
      "Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts.\n",
      "\n",
      "Distill, 2016.\n",
      "\n",
      "Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-\n",
      "\n",
      "iary classiﬁer GANs. In ICML, 2017.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B. Brown, Christopher Olah, Colin Raf-\n",
      "fel, and Ian Goodfellow. Is generator conditioning causally related to GAN performance?\n",
      "In\n",
      "ICML, 2018.\n",
      "\n",
      "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. FiLM: Visual\n",
      "\n",
      "reasoning with a general conditioning layer. In AAAI, 2018.\n",
      "\n",
      "Mathijs Pieters and Marco Wiering. Comparing generative adversarial network techniques for image\n",
      "\n",
      "creation and modiﬁcatio. In arXiv preprint arXiv:1803.09093, 2014.\n",
      "\n",
      "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\n",
      "\n",
      "convolutional generative adversarial networks. In ICLR, 2016.\n",
      "\n",
      "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\n",
      "ImageNet large scale visual\n",
      "\n",
      "Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein.\n",
      "recognition challenge. IJCV, 115:211–252, 2015.\n",
      "\n",
      "Tim Salimans and Diederik Kingma. Weight normalization: A simple reparameterization to accel-\n",
      "\n",
      "erate training of deep neural networks. In NIPS, 2016.\n",
      "\n",
      "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\n",
      "\n",
      "Improved techniques for training GANs. In NIPS, 2016.\n",
      "\n",
      "Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal\n",
      "\n",
      "transport. In ICLR, 2018.\n",
      "\n",
      "Andrew Saxe, James McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of\n",
      "\n",
      "learning in deep linear neural networks. In ICLR, 2014.\n",
      "\n",
      "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n",
      "\n",
      "recognition. In ICLR, 2015.\n",
      "\n",
      "Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszr. Amortised\n",
      "\n",
      "map inference for image super-resolution. In ICLR, 2017.\n",
      "\n",
      "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n",
      "Dropout: A simple way to prevent neural networks from overﬁtting. JMLR, 15:1929–1958, 2014.\n",
      "\n",
      "Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\n",
      "\n",
      "fectiveness of data in deep learning era. In ICCV, 2017.\n",
      "\n",
      "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-\n",
      "\n",
      "thinking the inception architecture for computer vision. In CVPR, 2016.\n",
      "\n",
      "Lucas Theis, A¨aron van den Oord, and Matthias Bethge. A note on the evaluation of generative\n",
      "\n",
      "models. In arXiv preprint arXiv:1511.01844, 2015.\n",
      "\n",
      "Dustin Tran, Rajesh Ranganath, and David M. Blei. Hierarchical implicit models and likelihood-free\n",
      "\n",
      "variational inference. In NIPS, 2017.\n",
      "\n",
      "Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\n",
      "\n",
      "CVPR, 2018.\n",
      "\n",
      "Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger B. Grosse. On the quantitative analysis\n",
      "\n",
      "of decoder-based generative models. In ICLR, 2017.\n",
      "\n",
      "Yasin Yazc, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay\n",
      "In arXiv preprint\n",
      "\n",
      "Chandrasekhar. The unusual effectiveness of averaging in gan training.\n",
      "arXiv:1806.04498, 2018.\n",
      "\n",
      "Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative\n",
      "\n",
      "adversarial networks. In arXiv preprint arXiv:1805.08318, 2018.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX A ADDITIONAL SAMPLES, INTERPOLATIONS, AND NEAREST\n",
      "\n",
      "NEIGHBORS FROM IMAGENET MODELS\n",
      "\n",
      "Figure 5: Samples generated by our BigGAN model at 256×256 resolution.\n",
      "\n",
      "Figure 6: Samples generated by our BigGAN model at 512×512 resolution.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 7: Comparing easy classes (a) with difﬁcult classes (b) at 512×512. Classes such as dogs\n",
      "which are largely textural, and common in the dataset, are far easier to model than classes involving\n",
      "unaligned human faces or crowds. Such classes are more dynamic and structured, and often have\n",
      "details to which human observers are more sensitive. The difﬁculty of modeling global structure is\n",
      "further exacerbated when producing high-resolution images, even with non-local blocks.\n",
      "\n",
      "Figure 8: Interpolations between z, c pairs.\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Figure 9: Interpolations between c with z held constant. Pose semantics are frequently maintained\n",
      "between endpoints (particularly in the ﬁnal row). Row 2 demonstrates that grayscale is encoded in\n",
      "the joint z, c space, rather than in z.\n",
      "\n",
      "Figure 10: Nearest neighbors in VGG-16-fc7 (Simonyan & Zisserman, 2015) feature space. The\n",
      "generated image is in the top left.\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Figure 11: Nearest neighbors in ResNet-50-avgpool (He et al., 2016) feature space. The generated\n",
      "image is in the top left.\n",
      "\n",
      "Figure 12: Nearest neighbors in pixel space. The generated image is in the top left.\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Figure 13: Nearest neighbors in VGG-16-fc7 (Simonyan & Zisserman, 2015) feature space. The\n",
      "generated image is in the top left.\n",
      "\n",
      "Figure 14: Nearest neighbors in ResNet-50-avgpool (He et al., 2016) feature space. The generated\n",
      "image is in the top left.\n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX B ARCHITECTURAL DETAILS\n",
      "\n",
      "In the BigGAN model (Figure 15), we use the ResNet (He et al., 2016) GAN architecture of (Zhang\n",
      "et al., 2018), which is identical to that used by (Miyato et al., 2018), but with the channel pattern\n",
      "in D modiﬁed so that the number of ﬁlters in the ﬁrst convolutional layer of each block is equal\n",
      "to the number of output ﬁlters (rather than the number of input ﬁlters, as in Miyato et al. (2018);\n",
      "Gulrajani et al. (2017)). We use a single shared class embedding in G, and skip connections for\n",
      "the latent vector z (skip-z). In particular, we employ hierarchical latent spaces, so that the latent\n",
      "vector z is split along its channel dimension into chunks of equal size (20-D in our case), and each\n",
      "chunk is concatenated to the shared class embedding and passed to a corresponding residual block\n",
      "as a conditioning vector. The conditioning of each block is linearly projected to produce per-sample\n",
      "gains and biases for the BatchNorm layers of the block. The bias projections are zero-centered,\n",
      "while the gain projections are centered at 1. Since the number of residual blocks depends on the\n",
      "image resolution, the full dimensionality of z is 120 for 128 × 128, 140 for 256 × 256, and 160 for\n",
      "512 × 512 images.\n",
      "\n",
      "The BigGAN-deep model (Figure 16) differs from BigGAN in several aspects. It uses a simpler vari-\n",
      "ant of skip-z conditioning: instead of ﬁrst splitting z into chunks, we concatenate the entire z with\n",
      "the class embedding, and pass the resulting vector to each residual block through skip connections.\n",
      "BigGAN-deep is based on residual blocks with bottlenecks (He et al., 2016), which incorporate\n",
      "two additional 1 × 1 convolutions: the ﬁrst reduces the number of channels by a factor of 4 before\n",
      "the more expensive 3 × 3 convolutions; the second produces the required number of output chan-\n",
      "nels. While BigGAN relies on 1 × 1 convolutions in the skip connections whenever the number of\n",
      "channels needs to change, in BigGAN-deep we use a different strategy aimed at preserving identity\n",
      "throughout the skip connections. In G, where the number of channels needs to be reduced, we sim-\n",
      "ply retain the ﬁrst group of channels and drop the rest to produce the required number of channels.\n",
      "In D, where the number of channels should be increased, we pass the input channels unperturbed,\n",
      "and concatenate them with the remaining channels produced by a 1 × 1 convolution. As far as the\n",
      "network conﬁguration is concerned, the discriminator is an exact reﬂection of the generator. There\n",
      "are two blocks at each resolution (BigGAN uses one), and as a result BigGAN-deep is four times\n",
      "deeper than BigGAN. Despite their increased depth, the BigGAN-deep models have signiﬁcantly\n",
      "fewer parameters mainly due to the bottleneck structure of their residual blocks. For example, the\n",
      "128 × 128 BigGAN-deep G and D have 50.4M and 34.6M parameters respectively, while the corre-\n",
      "sponding original BigGAN models have 70.4M and 88.0M parameters. All BigGAN-deep models\n",
      "use attention at 64 × 64 resolution, channel width multiplier ch = 128, and z ∈ R128.\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 15: (a) A typical architectural layout for BigGAN’s G; details are in the following tables.\n",
      "(b) A Residual Block (ResBlock up) in BigGAN’s G. (c) A Residual Block (ResBlock down) in\n",
      "BigGAN’s D.\n",
      "\n",
      "17\n",
      "\n",
      "ResBlockResBlockResBlockNon-localImageLinear→ 4x4x16chSplitzClassConcatConcatConcatAdd3x3 ConvBatchNormBatchNormConcatLinearLinearUpsampleUpsample1x1 Conv3x3 ConvReLUReLUAdd3x3 ConvAverage Pooling1x1 ConvReLU3x3 ConvReLUAverage Pooling\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a)\n",
      "\n",
      "18\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 16: (a) A typical architectural layout for BigGAN-deep’s G; details are in the following\n",
      "tables. (b) A Residual Block (ResBlock up) in BigGAN-deep’s G. (c) A Residual Block (ResBlock\n",
      "down) in BigGAN-deep’s D. A ResBlock (without up or down) in BigGAN-deep does not include\n",
      "the Upsample or Average Pooling layers, and has identity skip connections.\n",
      "\n",
      "ResBlockResBlockResBlockNon-localImageLinear→ 4x4x16chzClassConcatAdd3x3 ConvBatchNormBatchNormConcatLinearLinearDrop channelsUpsample1x1 ConvReLUReLUBatchNormUpsample3x3 ConvReLUBatchNormLinear1x1 ConvReLULinearAdd3x3 ConvAveragePooling1x1 ConvReLUReLU3x3 ConvReLU1x1 ConvReLUAverage PoolingConcat1x1 Conv\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Table 4: BigGAN architecture for 128 × 128 images. ch represents the channel width multiplier in\n",
      "each network from Table 1.\n",
      "\n",
      "z ∈ R120 ∼ N (0, I)\n",
      "Embed(y) ∈ R128\n",
      "\n",
      "Linear (20 + 128) → 4 × 4 × 16ch\n",
      "\n",
      "ResBlock up 16ch → 16ch\n",
      "\n",
      "ResBlock up 16ch → 8ch\n",
      "\n",
      "ResBlock up 8ch → 4ch\n",
      "\n",
      "ResBlock up 4ch → 2ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock up 2ch → ch\n",
      "\n",
      "BN, ReLU, 3 × 3 Conv ch → 3\n",
      "\n",
      "Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R128×128×3\n",
      "\n",
      "ResBlock down ch → 2ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock down 2ch → 4ch\n",
      "\n",
      "ResBlock down 4ch → 8ch\n",
      "\n",
      "ResBlock down 8ch → 16ch\n",
      "\n",
      "ResBlock down 16ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "Table 5: BigGAN architecture for 256 × 256 images. Relative to the 128 × 128 architecture, we\n",
      "add an additional ResBlock in each network at 16×16 resolution, and move the non-local block in\n",
      "G to 128 × 128 resolution. Memory constraints prevent us from moving the non-local block in D.\n",
      "\n",
      "z ∈ R140 ∼ N (0, I)\n",
      "Embed(y) ∈ R128\n",
      "\n",
      "Linear (20 + 128) → 4 × 4 × 16ch\n",
      "\n",
      "ResBlock up 16ch → 16ch\n",
      "\n",
      "ResBlock up 16ch → 8ch\n",
      "\n",
      "ResBlock up 8ch → 8ch\n",
      "\n",
      "ResBlock up 8ch → 4ch\n",
      "\n",
      "ResBlock up 4ch → 2ch\n",
      "\n",
      "Non-Local Block (128 × 128)\n",
      "\n",
      "ResBlock up 2ch → ch\n",
      "\n",
      "BN, ReLU, 3 × 3 Conv ch → 3\n",
      "\n",
      "Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R256×256×3\n",
      "\n",
      "ResBlock down ch → 2ch\n",
      "\n",
      "ResBlock down 2ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock down 4ch → 8ch\n",
      "\n",
      "ResBlock down 8ch → 8ch\n",
      "\n",
      "ResBlock down 8ch → 16ch\n",
      "\n",
      "ResBlock down 16ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "19\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Table 6: BigGAN architecture for 512 × 512 images. Relative to the 256 × 256 architecture, we\n",
      "add an additional ResBlock at the 512 × 512 resolution. Memory constraints force us to move the\n",
      "non-local block in both networks back to 64 × 64 resolution as in the 128 × 128 pixel setting.\n",
      "\n",
      "z ∈ R160 ∼ N (0, I)\n",
      "Embed(y) ∈ R128\n",
      "\n",
      "Linear (20 + 128) → 4 × 4 × 16ch\n",
      "\n",
      "ResBlock up 16ch → 16ch\n",
      "\n",
      "ResBlock up 16ch → 8ch\n",
      "\n",
      "ResBlock up 8ch → 8ch\n",
      "\n",
      "ResBlock up 8ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock up 4ch → 2ch\n",
      "\n",
      "ResBlock up 2ch → ch\n",
      "\n",
      "ResBlock up ch → ch\n",
      "\n",
      "BN, ReLU, 3 × 3 Conv ch → 3\n",
      "\n",
      "Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R512×512×3\n",
      "\n",
      "ResBlock down ch → ch\n",
      "\n",
      "ResBlock down ch → 2ch\n",
      "\n",
      "ResBlock down 2ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock down 4ch → 8ch\n",
      "\n",
      "ResBlock down 8ch → 8ch\n",
      "\n",
      "ResBlock down 8ch → 16ch\n",
      "\n",
      "ResBlock down 16ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "Table 7: BigGAN-deep architecture for 128 × 128 images.\n",
      "\n",
      "z ∈ R128 ∼ N (0, I)\n",
      "Embed(y) ∈ R128\n",
      "\n",
      "Linear (128 + 128) → 4 × 4 × 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ResBlock up 16ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ResBlock up 16ch → 8ch\n",
      "\n",
      "ResBlock 8ch → 8ch\n",
      "\n",
      "ResBlock up 8ch → 4ch\n",
      "\n",
      "ResBlock 4ch → 4ch\n",
      "\n",
      "ResBlock up 4ch → 2ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock 2ch → 2ch\n",
      "\n",
      "ResBlock up 2ch → ch\n",
      "\n",
      "BN, ReLU, 3 × 3 Conv ch → 3\n",
      "\n",
      "Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R128×128×3\n",
      "\n",
      "3 × 3 Conv 3 → ch\n",
      "\n",
      "ResBlock down ch → 2ch\n",
      "\n",
      "ResBlock 2ch → 2ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock down 2ch → 4ch\n",
      "\n",
      "ResBlock 4ch → 4ch\n",
      "\n",
      "ResBlock down 4ch → 8ch\n",
      "\n",
      "ResBlock 8ch → 8ch\n",
      "\n",
      "ResBlock down 8ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ResBlock down 16ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "20\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Table 8: BigGAN-deep architecture for 256 × 256 images.\n",
      "\n",
      "z ∈ R128 ∼ N (0, I)\n",
      "Embed(y) ∈ R128\n",
      "\n",
      "Linear (128 + 128) → 4 × 4 × 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ResBlock up 16ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ResBlock up 16ch → 8ch\n",
      "\n",
      "ResBlock 8ch → 8ch\n",
      "\n",
      "ResBlock up 8ch → 8ch\n",
      "\n",
      "ResBlock 8ch → 8ch\n",
      "\n",
      "ResBlock up 8ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock 4ch → 4ch\n",
      "\n",
      "ResBlock up 4ch → 2ch\n",
      "\n",
      "ResBlock 2ch → 2ch\n",
      "\n",
      "ResBlock up 2ch → ch\n",
      "\n",
      "BN, ReLU, 3 × 3 Conv ch → 3\n",
      "\n",
      "Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R256×256×3\n",
      "\n",
      "3 × 3 Conv 3 → ch\n",
      "\n",
      "ResBlock down ch → 2ch\n",
      "\n",
      "ResBlock 2ch → 2ch\n",
      "\n",
      "ResBlock down 2ch → 4ch\n",
      "\n",
      "ResBlock 4ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock down 4ch → 8ch\n",
      "\n",
      "ResBlock 8ch → 8ch\n",
      "\n",
      "ResBlock down 8ch → 8ch\n",
      "\n",
      "ResBlock 8ch → 8ch\n",
      "\n",
      "ResBlock down 8ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ResBlock down 16ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "21\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Table 9: BigGAN-deep architecture for 512 × 512 images.\n",
      "\n",
      "z ∈ R128 ∼ N (0, I)\n",
      "Embed(y) ∈ R128\n",
      "\n",
      "Linear (128 + 128) → 4 × 4 × 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ResBlock up 16ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ResBlock up 16ch → 8ch\n",
      "\n",
      "ResBlock 8ch → 8ch\n",
      "\n",
      "ResBlock up 8ch → 8ch\n",
      "\n",
      "ResBlock 8ch → 8ch\n",
      "\n",
      "ResBlock up 8ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock 4ch → 4ch\n",
      "\n",
      "ResBlock up 4ch → 2ch\n",
      "\n",
      "ResBlock 2ch → 2ch\n",
      "\n",
      "ResBlock up 2ch → ch\n",
      "\n",
      "ResBlock ch → ch\n",
      "\n",
      "ResBlock up ch → ch\n",
      "\n",
      "BN, ReLU, 3 × 3 Conv ch → 3\n",
      "\n",
      "Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R512×512×3\n",
      "\n",
      "3 × 3 Conv 3 → ch\n",
      "\n",
      "ResBlock down ch → ch\n",
      "\n",
      "ResBlock ch → ch\n",
      "\n",
      "ResBlock down ch → 2ch\n",
      "\n",
      "ResBlock 2ch → 2ch\n",
      "\n",
      "ResBlock down 2ch → 4ch\n",
      "\n",
      "ResBlock 4ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "ResBlock down 4ch → 8ch\n",
      "\n",
      "ResBlock 8ch → 8ch\n",
      "\n",
      "ResBlock down 8ch → 8ch\n",
      "\n",
      "ResBlock 8ch → 8ch\n",
      "\n",
      "ResBlock down 8ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ResBlock down 16ch → 16ch\n",
      "\n",
      "ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "22\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX C EXPERIMENTAL DETAILS\n",
      "\n",
      "Our basic setup follows SA-GAN (Zhang et al., 2018), and is implemented in TensorFlow (Abadi\n",
      "et al., 2016). We employ the architectures detailed in Appendix B, with non-local blocks inserted at\n",
      "a single stage in each network. Both G and D networks are initialized with Orthogonal Initialization\n",
      "(Saxe et al., 2014). We use Adam optimizer (Kingma & Ba, 2014) with β1 = 0 and β2 = 0.999 and\n",
      "a constant learning rate. For BigGAN models at all resolutions, we use 2 · 10−4 in D and 5 · 10−5\n",
      "in G. For BigGAN-deep, we use the learning rate of 2 · 10−4 in D and 5 · 10−5 in G for 128 × 128\n",
      "models, and 2.5 · 10−5 in both D and G for 256 × 256 and 512 × 512 models. We experimented with\n",
      "the number of D steps per G step (varying it from 1 to 6) and found that two D steps per G step gave\n",
      "the best results.\n",
      "\n",
      "We use an exponential moving average of the weights of G at sampling time, with a decay rate set to\n",
      "0.9999. We employ cross-replica BatchNorm (Ioffe & Szegedy, 2015) in G, where batch statistics are\n",
      "aggregated across all devices, rather than a single device as in standard implementations. Spectral\n",
      "Normalization (Miyato et al., 2018) is used in both G and D, following SA-GAN (Zhang et al., 2018).\n",
      "We train on a Google TPU v3 Pod, with the number of cores proportional to the resolution: 128 for\n",
      "128×128, 256 for 256×256, and 512 for 512×512. Training takes between 24 and 48 hours for\n",
      "most models. We increase (cid:15) from the default 10−8 to 10−4 in BatchNorm and Spectral Norm to\n",
      "mollify low-precision numerical issues. We preprocess data by cropping along the long edge and\n",
      "rescaling to a given resolution with area resampling.\n",
      "\n",
      "C.1 BATCHNORM STATISTICS AND SAMPLING\n",
      "\n",
      "The default behavior with batch normalized classiﬁer networks is to use a running average of the\n",
      "activation moments at test time. Previous works (Radford et al., 2016) have instead used batch\n",
      "statistics when sampling images. While this is not technically an invalid way to sample, it means\n",
      "that results are dependent on the test batch size (and how many devices it is split across), and further\n",
      "complicates reproducibility.\n",
      "\n",
      "We ﬁnd that this detail is extremely important, with changes in test batch size producing drastic\n",
      "changes in performance. This is further exacerbated when one uses exponential moving averages\n",
      "of G’s weights for sampling, as the BatchNorm running averages are computed with non-averaged\n",
      "weights and are poor estimates of the activation statistics for the averaged weights.\n",
      "\n",
      "To counteract both these issues, we employ “standing statistics,” where we compute activation statis-\n",
      "tics at sampling time by running the G through multiple forward passes (typically 100) each with\n",
      "different batches of random noise, and storing means and variances aggregated across all forward\n",
      "passes. Analogous to using running statistics, this results in G’s outputs becoming invariant to batch\n",
      "size and the number of devices, even when producing a single sample.\n",
      "\n",
      "C.2 CIFAR-10\n",
      "\n",
      "We run our networks on CIFAR-10 (Krizhevsky & Hinton, 2009) using the settings from Table 1,\n",
      "row 8, and achieve an IS of 9.22 and an FID of 14.73 without truncation.\n",
      "\n",
      "C.3\n",
      "\n",
      "INCEPTION SCORES OF IMAGENET IMAGES\n",
      "\n",
      "We compute the IS for both the training and validation sets of ImageNet. At 128×128 the training\n",
      "data has an IS of 233, and the validation data has an IS of 166. At 256×256 the training data has an\n",
      "IS of 377, and the validation data has an IS of 234. At 512×512 the training data has an IS of 348,\n",
      "and the validation data has an IS of 241. The discrepancy between training and validation scores is\n",
      "due to the Inception classiﬁer having been trained on the training data, resulting in high-conﬁdence\n",
      "outputs that are preferred by the Inception Score.\n",
      "\n",
      "23\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX D ADDITIONAL PLOTS\n",
      "\n",
      "Figure 17: IS vs. FID at 128×128. Scores are averaged across three random seeds.\n",
      "\n",
      "Figure 18: IS vs. FID at 256 and 512 pixels. Scores are averaged across three random seeds for 256.\n",
      "\n",
      "24\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Figure 19: JFT-300M IS vs. FID at 256×256. We show truncation values from σ = 0 to σ = 2\n",
      "(top) and from σ = 0.5 to σ = 1.5 (bottom). Each curve corresponds to a row in Table 3. The\n",
      "curve labeled with baseline corresponds to the ﬁrst row (with orthogonal regularization and other\n",
      "techniques disabled), while the rest correspond to rows 2-4 – the same architecture at different\n",
      "capacities (Ch).\n",
      "\n",
      "25\n",
      "\n",
      "5101520253035404550JFT-300M Inception Score020406080100120140160180JFT-300M FIDFID vs IS as a function of truncationCh=128Ch=96Ch=64Ch=64 (Baseline)1520253035404550JFT-300M Inception Score1020304050607080JFT-300M FIDFID vs IS as a function of truncationCh=128Ch=96Ch=64Ch=64 (Baseline)\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX E CHOOSING LATENT SPACES\n",
      "\n",
      "While most previous work has employed N (0, I) or U[−1, 1] as the prior for z (the noise input to\n",
      "G), we are free to choose any latent distribution from which we can sample. We explore the choice of\n",
      "latents by considering an array of possible designs, described below. For each latent, we provide the\n",
      "intuition behind its design and brieﬂy describe how it performs when used as a drop-in replacement\n",
      "for z ∼ N (0, I) in an SA-GAN baseline. As the Truncation Trick proved more beneﬁcial than\n",
      "switching to any of these latents, we do not perform a full ablation study, and employ z ∼ N (0, I)\n",
      "for our main results to take full advantage of truncation. The two latents which we ﬁnd to work\n",
      "best without truncation are Bernoulli {0, 1} and Censored Normal max (N (0, I), 0), both of which\n",
      "improve speed of training and lightly improve ﬁnal performance, but are less amenable to truncation.\n",
      "We also ablate the choice of latent space dimensonality (which by default is z ∈ R128), ﬁnding that\n",
      "we are able to successfully train with latent dimensions as low as z ∈ R8, and that with z ∈ R32 we\n",
      "see a minimal drop in performance. While this is substantially smaller than many previous works,\n",
      "direct comparison to single-class networks (such as those in Karras et al. (2018), which employ\n",
      "a z ∈ R512 latent space on a highly constrained dataset with 30,000 images) is improper, as our\n",
      "networks have additional class information provided as input.\n",
      "\n",
      "LATENTS\n",
      "\n",
      "• N (0, I). A standard choice of the latent space which we use in the main experiments.\n",
      "• U[−1, 1]. Another standard choice; we ﬁnd that it performs similarly to N (0, I).\n",
      "• Bernoulli {0, 1}. A discrete latent might reﬂect our prior that underlying factors of variation\n",
      "in natural images are not continuous, but discrete (one feature is present, another is not).\n",
      "This latent outperforms N (0, I) (in terms of IS) by 8% and requires 60% fewer iterations.\n",
      "• max (N (0, I), 0), also called Censored Normal. This latent is designed to introduce spar-\n",
      "sity in the latent space (reﬂecting our prior that certain latent features are sometimes present\n",
      "and sometimes not), but also allow those latents to vary continuously, expressing different\n",
      "degrees of intensity for latents which are active. This latent outperforms N (0, I) (in terms\n",
      "of IS) by 15-20% and tends to require fewer iterations.\n",
      "\n",
      "• Bernoulli {−1, 1}. This latent is designed to be discrete, but not sparse (as the network\n",
      "can learn to activate in response to negative inputs). This latent performs near-identically\n",
      "to N (0, I).\n",
      "\n",
      "• Independent Categorical in {−1, 0, 1}, with equal probability. This distribution is chosen to\n",
      "be discrete and have sparsity, but also to allow latents to take on both positive and negative\n",
      "values. This latent performs near-identically to N (0, I).\n",
      "\n",
      "• N (0, I) multiplied by Bernoulli {0, 1}. This distribution is chosen to have continuous\n",
      "latent factors which are also sparse (with a peak at zero), similar to Censored Normal but\n",
      "not constrained to be positive. This latent performs near-identically to N (0, I).\n",
      "\n",
      "• Concatenating N (0, I) and Bernoulli {0, 1}, each taking half of the latent dimensions.\n",
      "This is inspired by Chen et al. (2016), and is chosen to allow some factors of variation to\n",
      "be discrete, while others are continuous. This latent outperforms N (0, I) by around 5%.\n",
      "• Variance annealing: we sample from N (0, σI), where σ is allowed to vary over training.\n",
      "We compared a variety of piecewise schedules and found that starting with σ = 2 and\n",
      "annealing towards σ = 1 over the course of training mildly improved performance. The\n",
      "space of possible variance schedules is large, and we did not explore it in depth – we suspect\n",
      "that a more principled or better-tuned schedule could more strongly impact performance.\n",
      "• Per-sample variable variance: N (0, σiI), where σi ∼ U[σl, σh] independently for each\n",
      "sample i in a batch, and (σl, σh) are hyperparameters. This distribution was chosen to try\n",
      "and improve amenability to the Truncation Trick by feeding the network noise samples with\n",
      "non-constant variance. This did not appear to affect performance, but we did not explore it\n",
      "in depth. One might also consider scheduling (σl, σh), similar to variance annealing.\n",
      "\n",
      "26\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX F MONITORED TRAINING STATISTICS\n",
      "\n",
      "(a) G σ0\n",
      "\n",
      "(b) G σ0\n",
      "σ1\n",
      "\n",
      "(c) G σ1\n",
      "\n",
      "(d) G σ2\n",
      "\n",
      "(e) D σ0\n",
      "\n",
      "(f) D σ0\n",
      "σ1\n",
      "\n",
      "(g) D σ1\n",
      "\n",
      "(h) D σ2\n",
      "\n",
      "Figure 20: Training statistics for a typical model without special modiﬁcations. Collapse occurs\n",
      "after 200000 iterations.\n",
      "\n",
      "27\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "Figure 21: G training statistics with σ0 in G regularized towards 1. Collapse occurs after 125000\n",
      "iterations.\n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "Figure 22: D training statistics with σ0 in G regularized towards 1. Collapse occurs after 125000\n",
      "iterations.\n",
      "\n",
      "28\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "Figure 23: G training statistics with an R1 Gradient Penalty of strength 10 on D. This model does\n",
      "not collapse, but only reaches a maximum IS of 55.\n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "Figure 24: D training statistics with an R1 Gradient Penalty of strength 10 on D. This model does\n",
      "not collapse, but only reaches a maximum IS of 55.\n",
      "\n",
      "29\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "Figure 25: G training statistics with Dropout (keep probability 0.8) applied to the last feature layer\n",
      "of D. This model does not collapse, but only reaches a maximum IS of 70.\n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "Figure 26: D training statistics with Dropout (keep probability 0.8) applied to the last feature layer\n",
      "of D. This model does not collapse, but only reaches a maximum IS of 70.\n",
      "\n",
      "30\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a) G (cid:107)W (cid:107)2\n",
      "\n",
      "(b) D (cid:107)W (cid:107)2\n",
      "\n",
      "(c) losses\n",
      "\n",
      "(d) Variance of all gradient norms in G and D\n",
      "\n",
      "Figure 27: Additional training statistics for a typical model without special modiﬁcations. Collapse\n",
      "occurs after 200000 iterations.\n",
      "\n",
      "(a) G (cid:107)W (cid:107)2\n",
      "\n",
      "(b) D (cid:107)W (cid:107)2\n",
      "\n",
      "(c) losses\n",
      "\n",
      "(d) Variance of all gradient norms in G and D\n",
      "\n",
      "Figure 28: Additional training statistics with an R1 Gradient Penalty of strength 10 on D. This model\n",
      "does not collapse, but only reaches a maximum IS of 55.\n",
      "\n",
      "31\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX G ADDITIONAL DISCUSSION: STABILITY AND COLLAPSE\n",
      "\n",
      "In this section, we present and discuss additional investigations into the stability of our models,\n",
      "expanding upon the discussion in Section 4.\n",
      "\n",
      "G.1\n",
      "\n",
      "INTERVENING BEFORE COLLAPSE\n",
      "\n",
      "The symptoms of collapse are sharp and sudden, with sample quality dropping from its peak to\n",
      "its lowest value over the course of a few hundred iterations. We can detect this collapse when the\n",
      "singular values in G explode, but while the (unnormalized) singular values grow throughout training,\n",
      "there is no consistent threshold at which collapse occurs. This raises the question of whether it is\n",
      "possible to prevent or delay collapse by taking a model checkpoint several thousand iterations before\n",
      "collapse, and continuing training with some hyperparameters modiﬁed (e.g., the learning rate).\n",
      "\n",
      "We conducted a range of intervention experiments wherein we took checkpoints of a collapsed\n",
      "model ten or twenty thousand iterations before collapse, changed some aspect of the training setup,\n",
      "then observed whether collapse occurred, when it occurred relative to the original collapse, and the\n",
      "ﬁnal performance attained at collapse.\n",
      "\n",
      "We found that increasing the learning rates (relative to their initial values) in either G or D, or both G\n",
      "and D, led to immediate collapse. This occurred even when doubling the learning rates from 2 · 10−4\n",
      "in D and 5 · 10−5 in G, to 4 · 10−4 in D and 1 · 10−4 in G, a setting which is not normally unstable\n",
      "when used as the initial learning rates. We also tried changing the momentum terms (Adam’s β1\n",
      "and β2), or resetting the momentum vectors to zero, but this tended to either make no difference or,\n",
      "when increasing the momentum, cause immediate collapse.\n",
      "\n",
      "We found that decreasing the learning rate in G, but keeping the learning rate in D unchanged could\n",
      "delay collapse (in some cases by over one hundred thousand iterations), but also crippled training—\n",
      "once the learning rate in G was decayed, performance either stayed constant or slowly decayed.\n",
      "Conversely, reducing the learning rate in D while keeping G’s learning rate led to immediate collapse.\n",
      "We hypothesize that this is because of the need for D to remain optimal throughout training—if its\n",
      "learning rate is reduced, it can no longer “keep up” with G, and training collapses. With this in mind,\n",
      "we also tried increasing the number of D steps per G step, but this either had no effect, or delayed\n",
      "collapse at the cost of crippling training (similar to decaying G’s learning rate).\n",
      "\n",
      "To further illuminate these dynamics, we construct two additional intervention experiments, one\n",
      "where we freeze G before collapse (by ceasing all parameter updates) and observe whether D remains\n",
      "stable, and the reverse, where we freeze D before collapse and observe whether G remains stable.\n",
      "We ﬁnd that when G is frozen, D remains stable, and slowly reduces both components of its loss\n",
      "towards zero. However, when D is frozen, G immediately and dramatically collapses, maxing out\n",
      "D’s loss to values upwards of 300, compared to the normal range of 0 to 3.\n",
      "\n",
      "This leads to two conclusions: ﬁrst, as has been noted in previous works (Miyato et al., 2018;\n",
      "Gulrajani et al., 2017; Zhang et al., 2018), D must remain optimal with respect to G both for stability\n",
      "and to provide useful gradient information. The consequence of G being allowed to win the game is a\n",
      "complete breakdown of the training process, regardless of G’s conditioning or optimization settings.\n",
      "Second, favoring D over G (either by training it with a larger learning rate, or for more steps) is\n",
      "insufﬁcient to ensure stability even if D is well-conditioned. This suggests either that in practice, an\n",
      "optimal D is necessary but insufﬁcient for training stability, or that some aspect of the system results\n",
      "in D not being trained towards optimality. With the latter possibility in mind, we take a closer look\n",
      "at the noise in D’s spectra in the following section.\n",
      "\n",
      "32\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "G.2 SPIKES IN THE DISCRIMINATOR’S SPECTRA\n",
      "\n",
      "(a) D σ0\n",
      "\n",
      "(b) D σ0\n",
      "σ1\n",
      "\n",
      "Figure 29: A closeup of D’s spectra at a noise spike.\n",
      "\n",
      "If some element of D’s training process results in undesirable dynamics, it follows that the behavior\n",
      "of D’s spectra may hold clues as to what that element is. The top three singular values of D differ\n",
      "from G’s in that they have a large noise component, tend to grow throughout training but only show\n",
      "a small response to collapse, and the ratio of the ﬁrst two singular values tends to be centered around\n",
      "one, suggesting that the spectra of D have a slow decay. When viewed up close (Figure 29), the\n",
      "noise spikes resemble an impulse response: at each spike, the spectra jump upwards, then slowly\n",
      "decrease, with some oscillation.\n",
      "\n",
      "One possible explanation is that this behavior is a consequence of D memorizing the training data,\n",
      "as suggested by experiments in Section 4.2. As it approaches perfect memorization, it receives\n",
      "less and less signal from real data, as both the original GAN loss and the hinge loss provide zero\n",
      "gradients when D outputs a conﬁdent and correct prediction for a given example. If the gradient\n",
      "signal from real data attenuates to zero, this can result in D eventually becoming biased due to\n",
      "exclusively received gradients that encourage its outputs to be negative. If this bias passes a certain\n",
      "threshold, D will eventually misclassify a large number of real examples and receive a large gradient\n",
      "encouraging positive outputs, resulting in the observed impulse responses.\n",
      "\n",
      "This argument suggests several ﬁxes. First, one might consider an unbounded loss (such as the\n",
      "Wasserstein loss (Arjovsky et al., 2017)) which would not suffer this gradient attentuation. We found\n",
      "that even with gradient penalties and brief re-tuning of optimizer hyperparameters, our models did\n",
      "not stably train for more than a few thousand iterations with this loss. We instead explored changing\n",
      "the margin of the hinge loss as a partial compromise: for a given model and minibatch of data,\n",
      "increasing the margin will result in more examples falling within the margin, and thus contributing\n",
      "to the loss.3. Training with a smaller margin (by a factor of 2) measurably reduces performance,\n",
      "but training with a larger margin (by up to a factor of 3) does not prevent collapse or reduce the\n",
      "noise in D’s spectra. Increasing the margin beyond 3 results in unstable training similar to using\n",
      "the Wasserstein loss. Finally, the memorization argument might suggest that using a smaller D or\n",
      "using dropout in D would improve training by reducing its capacity to memorize, but in practice this\n",
      "degrades training.\n",
      "\n",
      "3Unconstrained models could easily learn a different output scale to account for this margin, but the use of\n",
      "\n",
      "Spectral Normalization constrains our models and makes the speciﬁc selection of the margin meaningful.\n",
      "\n",
      "33\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX H NEGATIVE RESULTS\n",
      "\n",
      "We explored a range of novel and existing techniques which ended up degrading or otherwise not\n",
      "affecting performance in our setting. We report them here; our evaluations for this section are not as\n",
      "thorough as those for the main architectural choices.\n",
      "\n",
      "Our intention in reporting these results is to save time for future work, and to give a more complete\n",
      "picture of our attempts to improve performance or stability. We note, however, that these results\n",
      "must be understood to be speciﬁc to the particular setup we used. A pitfall of reporting negative\n",
      "results is that one might report that a particular technique doesn’t work, when the reality is that this\n",
      "technique did not have the desired effect when applied in a particular way to a particular problem.\n",
      "Drawing overly general conclusions might close off potentially fruitful avenues of research.\n",
      "\n",
      "• We found that doubling the depth (by inserting an additional Residual block after every up-\n",
      "\n",
      "or down-sampling block) hampered performance.\n",
      "\n",
      "• We experimented with sharing class embeddings between both G and D (as opposed to just\n",
      "within G). This is accomplished by replacing D’s class embedding with a projection from\n",
      "G’s embeddings, as is done in G’s BatchNorm layers. In our initial experiments this seemed\n",
      "to help and accelerate training, but we found this trick scaled poorly and was sensitive to\n",
      "optimization hyperparameters, particularly the choice of number of D steps per G step.\n",
      "• We tried replacing BatchNorm in G with WeightNorm (Salimans & Kingma, 2016), but\n",
      "this crippled training. We also tried removing BatchNorm and only having Spectral Nor-\n",
      "malization, but this also crippled training.\n",
      "\n",
      "• We tried adding BatchNorm to D (both class-conditional and unconditional) in addition to\n",
      "\n",
      "Spectral Normalization, but this crippled training.\n",
      "\n",
      "• We tried varying the choice of location of the attention block in G and D (and inserting\n",
      "multiple attention blocks at different resolutions) but found that at 128×128 there was no\n",
      "noticeable beneﬁt to doing so, and compute and memory costs increased substantially. We\n",
      "found a beneﬁt to moving the attention block up one stage when moving to 256×256,\n",
      "which is in line with our expectations given the increased resolution.\n",
      "\n",
      "• We tried using ﬁlter sizes of 5 or 7 instead of 3 in either G or D or both. We found that\n",
      "having a ﬁlter size of 5 in G only provided a small improvement over the baseline but came\n",
      "at an unjustiﬁable compute cost. All other settings degraded performance.\n",
      "\n",
      "• We tried varying the dilation for convolutional ﬁlters in both G and D at 128×128, but found\n",
      "\n",
      "that even a small amount of dilation in either network degraded performance.\n",
      "\n",
      "• We tried bilinear upsampling in G in place of nearest-neighbors upsampling, but this de-\n",
      "\n",
      "graded performance.\n",
      "\n",
      "• In some of our models, we observed class-conditional mode collapse, where the model\n",
      "would only output one or two samples for a subset of classes but was still able to generate\n",
      "samples for all other classes. We noticed that the collapsed classes had embedings which\n",
      "had become very large relative to the other embeddings, and attempted to ameliorate this\n",
      "issue by applying weight decay to the shared embedding only. We found that small amounts\n",
      "of weight decay (10−6) instead degraded performance, and that only even smaller values\n",
      "(10−8) did not degrade performance, but these values were also too small to prevent the\n",
      "class vectors from exploding. Higher-resolution models appear to be more resilient to this\n",
      "problem, and none of our ﬁnal models appear to suffer from this type of collapse.\n",
      "\n",
      "• We experimented with using MLPs instead of linear projections from G’s class embeddings\n",
      "to its BatchNorm gains and biases, but did not ﬁnd any beneﬁt to doing so. We also exper-\n",
      "imented with Spectrally Normalizing these MLPs, and with providing these (and the linear\n",
      "projections) with a bias at their output, but did not notice any beneﬁt.\n",
      "\n",
      "• We tried gradient norm clipping (both the global variant typically used in recurrent net-\n",
      "works, and a local version where the clipping value is determined on a per-parameter basis)\n",
      "but found this did not alleviate instability.\n",
      "\n",
      "34\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX I HYPERPARAMETERS\n",
      "\n",
      "We performed various hyperparameter sweeps in this work:\n",
      "\n",
      "• We swept the Cartesian product of the learning rates for each network through [10−5,\n",
      "5 · 10−5, 10−4, 2 · 10−4, 4 · 10−4, 8 · 10−4, 10−3], and initially found that the SA-GAN\n",
      "settings (G’s learning rate 10−4, D’s learning rate 4 · 10−4) were optimal at lower batch\n",
      "sizes; we did not repeat this sweep at higher batch sizes but did try halving and doubling\n",
      "the learning rate, arriving at the halved settings used for our experiments.\n",
      "\n",
      "• We swept the R1 gradient penalty strength through [10−3, 10−2, 10−1, 0.5, 1, 2, 3, 5, 10].\n",
      "We ﬁnd that the strength of the penalty correlates negatively with performance, but that\n",
      "settings above 0.5 impart training stability.\n",
      "\n",
      "• We swept the keep probabilities for DropOut in the ﬁnal layer of D through [0.5, 0.6, 0.7,\n",
      "0.8, 0.9, 0.95]. We ﬁnd that DropOut has a similar stabilizing effect to R1 but also degrades\n",
      "performance.\n",
      "\n",
      "• We swept D’s Adam β1 parameter through [0.1, 0.2, 0.3, 0.4, 0.5] and found it to have\n",
      "a light regularization effect similar to DropOut, but not to signiﬁcantly improve results.\n",
      "Higher β1 terms in either network crippled training.\n",
      "\n",
      "• We swept the strength of the modiﬁed Orthogonal Regularization penalty in G through\n",
      "\n",
      "[10−5, 5 · 10−5, 10−4, 5 · 10−4, 10−3, 10−2], and selected 10−4.\n",
      "\n",
      "35\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.datamodules.apis.semantic_scholar_api import SemanticScholarAPI\n",
    "\n",
    "q = 'gan'\n",
    "pdfdir = '../../ui/public/gan.pdf'\n",
    "\n",
    "api = SemanticScholarAPI() \n",
    "query = api.query(q, fields=['title', 'externalIds'], limit=2)\n",
    "first_result = query['data'][0]\n",
    "title = first_result['title']\n",
    "arxiv_id = first_result['externalIds']['ArXiv']\n",
    "print(title, ':', arxiv_id)\n",
    "\n",
    "#---- Search Arxiv API\n",
    "import arxiv\n",
    "search = arxiv.Search(id_list = [arxiv_id])\n",
    "pdf_url = ''\n",
    "for res in search.results():\n",
    "    pdf_url = res.pdf_url\n",
    "print(pdf_url)\n",
    "\n",
    "# ---- Download PDF\n",
    "import requests\n",
    "r = requests.get(pdf_url, allow_redirects=True)\n",
    "open(pdfdir, 'wb').write(r.content)\n",
    "\n",
    "# ---- Extract text\n",
    "from pdfminer.high_level import extract_text\n",
    "text = extract_text(pdfdir)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('For evaluation, we employ moving averages of G’s weights following Karras et al. (2018); Mescheder et al. (2018); Yazc et al. (2018), with a decay of 0.9999.', {'entities': []})\n",
      "{'entities': [(56, 60, 'MODEL')]}\n",
      "{'entities': [(16, 20, 'MODEL')]}\n",
      "{'entities': [(0, 4, 'MODEL')]}\n",
      "{'entities': [(63, 85, 'MODEL')]}\n",
      "{'entities': [(26, 30, 'MODEL'), (32, 61, 'MODEL'), (63, 73, 'MODEL'), (105, 109, 'MODEL')]}\n",
      "{'entities': [(23, 27, 'MODEL')]}\n",
      "{'entities': [(50, 61, 'MODEL')]}\n",
      "{'entities': [(25, 28, 'MODEL')]}\n",
      "{'entities': [(64, 71, 'MODEL'), (96, 103, 'MODEL')]}\n",
      "{'entities': [(0, 14, 'MODEL')]}\n",
      "{'entities': [(11, 26, 'MODEL'), (27, 39, 'MODEL')]}\n",
      "{'entities': [(19, 40, 'MODEL')]}\n",
      "{'entities': [(76, 91, 'MODEL'), (38, 52, 'MODEL')]}\n",
      "{'entities': [(9, 22, 'MODEL')]}\n",
      "{'entities': [(18, 29, 'MODEL')]}\n",
      "{'entities': [(17, 37, 'MODEL'), (13, 16, 'MODEL')]}\n",
      "{'entities': [(57, 81, 'MODEL')]}\n",
      "{'entities': [(40, 61, 'MODEL'), (86, 95, 'MODEL'), (17, 28, 'MODEL')]}\n",
      "{'entities': [(35, 44, 'MODEL')]}\n",
      "{'entities': [(99, 116, 'MODEL'), (175, 178, 'MODEL'), (118, 122, 'MODEL')]}\n",
      "{'entities': [(15, 18, 'MODEL'), (39, 43, 'MODEL')]}\n",
      "{'entities': [(0, 24, 'MODEL')]}\n",
      "{'entities': [(86, 89, 'MODEL'), (106, 110, 'MODEL')]}\n",
      "{'entities': [(40, 47, 'MODEL'), (2, 30, 'MODEL'), (32, 35, 'MODEL')]}\n",
      "{'entities': [(26, 30, 'MODEL')]}\n",
      "{'entities': [(16, 25, 'MODEL'), (4, 15, 'MODEL')]}\n",
      "{'entities': [(45, 67, 'MODEL')]}\n",
      "{'entities': [(27, 38, 'MODEL')]}\n",
      "{'entities': [(39, 46, 'MODEL'), (54, 63, 'MODEL'), (24, 37, 'MODEL')]}\n",
      "{'entities': [(44, 64, 'MODEL'), (0, 4, 'DATASET'), (9, 12, 'DATASET')]}\n",
      "{'entities': [(4, 9, 'DATASET')]}\n",
      "{'entities': [(93, 104, 'MODEL'), (62, 90, 'MODEL')]}\n",
      "{'entities': [(143, 148, 'DATASET')]}\n",
      "{'entities': [(72, 77, 'DATASET')]}\n",
      "{'entities': [(42, 50, 'DATASET'), (52, 56, 'DATASET'), (62, 70, 'DATASET')]}\n",
      "{'entities': [(0, 8, 'DATASET')]}\n",
      "{'entities': [(13, 21, 'DATASET')]}\n",
      "{'entities': [(4, 8, 'DATASET'), (34, 50, 'TASK'), (70, 80, 'TASK')]}\n",
      "{'entities': [(3, 7, 'DATASET')]}\n",
      "{'entities': [(153, 171, 'MODEL'), (173, 176, 'MODEL'), (233, 246, 'MODEL'), (140, 144, 'DATASET'), (129, 138, 'DATASET'), (119, 127, 'DATASET'), (259, 279, 'TASK')]}\n",
      "{'entities': [(88, 97, 'MODEL'), (71, 82, 'MODEL'), (107, 110, 'TASK')]}\n",
      "{'entities': [(86, 104, 'MODEL'), (63, 67, 'DATASET'), (48, 51, 'TASK')]}\n",
      "{'entities': [(32, 37, 'MODEL'), (50, 61, 'TASK')]}\n",
      "{'entities': [(0, 30, 'MODEL'), (32, 35, 'MODEL')]}\n",
      "{'entities': [(87, 90, 'MODEL'), (120, 144, 'TASK'), (183, 205, 'TASK'), (207, 209, 'TASK'), (149, 174, 'TASK'), (64, 85, 'TASK'), (40, 59, 'TASK')]}\n",
      "{'entities': [(44, 47, 'MODEL'), (13, 30, 'TASK'), (119, 141, 'TASK')]}\n",
      "{'entities': [(22, 37, 'MODEL'), (94, 109, 'MODEL'), (39, 41, 'MODEL')]}\n",
      "{'entities': [(101, 116, 'MODEL')]}\n",
      "{'entities': [(65, 68, 'MODEL'), (35, 46, 'MODEL')]}\n",
      "{'entities': [(93, 101, 'TASK'), (106, 115, 'TASK'), (73, 76, 'TASK')]}\n",
      "{'entities': [(50, 59, 'TASK'), (39, 49, 'TASK')]}\n",
      "{'entities': [(21, 39, 'TASK')]}\n",
      "{'entities': [(45, 55, 'TASK')]}\n",
      "{'entities': [(83, 92, 'TASK')]}\n",
      "{'entities': [(72, 81, 'TASK')]}\n",
      "{'entities': [(166, 177, 'TASK'), (80, 89, 'TASK'), (38, 41, 'TASK'), (133, 136, 'TASK'), (9, 36, 'TASK')]}\n",
      "{'entities': [(0, 22, 'TASK'), (141, 144, 'TASK'), (27, 45, 'TASK')]}\n",
      "{'entities': [(17, 39, 'TASK')]}\n",
      "{'entities': [(25, 42, 'TASK'), (131, 139, 'TASK'), (11, 23, 'TASK')]}\n",
      "{'entities': [(135, 157, 'TASK'), (162, 180, 'TASK')]}\n",
      "{'entities': [(126, 128, 'MODEL'), (70, 81, 'TASK'), (33, 36, 'TASK')]}\n",
      "{'entities': [(105, 117, 'TASK'), (51, 64, 'TASK')]}\n",
      "{'entities': [(0, 18, 'TASK'), (24, 51, 'TASK')]}\n",
      "{'entities': [(69, 72, 'TASK'), (40, 67, 'TASK')]}\n",
      "{'entities': [(50, 64, 'TASK'), (72, 80, 'TASK'), (109, 117, 'TASK'), (211, 219, 'TASK')]}\n",
      "{'entities': [(44, 59, 'MODEL'), (10, 25, 'MODEL'), (228, 242, 'TASK'), (153, 171, 'TASK'), (173, 193, 'TASK'), (195, 222, 'TASK')]}\n",
      "{'entities': [(62, 65, 'MODEL'), (102, 105, 'MODEL'), (157, 160, 'MODEL'), (46, 57, 'MODEL'), (227, 235, 'DATASET'), (220, 225, 'DATASET')]}\n",
      "{'entities': [(38, 41, 'MODEL')]}\n",
      "{'entities': [(147, 153, 'MODEL')]}\n",
      "{'entities': [(90, 98, 'DATASET'), (83, 88, 'DATASET'), (138, 166, 'TASK')]}\n",
      "{'entities': [(131, 134, 'MODEL')]}\n",
      "{'entities': [(26, 41, 'MODEL'), (83, 98, 'MODEL')]}\n",
      "{'entities': [(71, 85, 'MODEL'), (32, 47, 'MODEL')]}\n",
      "{'entities': [(7, 17, 'TASK')]}\n",
      "{'entities': [(43, 71, 'MODEL'), (73, 76, 'MODEL')]}\n",
      "{'entities': [(22, 25, 'MODEL')]}\n",
      "{'entities': [(0, 11, 'MODEL')]}\n",
      "{'entities': [(12, 23, 'MODEL'), (25, 28, 'MODEL'), (30, 44, 'MODEL'), (49, 64, 'MODEL'), (100, 114, 'TASK'), (116, 134, 'TASK'), (136, 149, 'TASK'), (161, 172, 'TASK')]}\n",
      "{'entities': [(25, 39, 'MODEL'), (81, 96, 'MODEL'), (118, 126, 'TASK'), (131, 144, 'TASK'), (43, 54, 'TASK')]}\n",
      "{'entities': [(4, 12, 'DATASET'), (42, 58, 'TASK')]}\n",
      "{'entities': [(4, 12, 'DATASET'), (49, 67, 'TASK')]}\n",
      "{'entities': [(45, 58, 'DATASET'), (19, 26, 'TASK')]}\n",
      "{'entities': [(0, 9, 'DATASET'), (39, 57, 'TASK')]}\n",
      "{'entities': [(4, 14, 'DATASET'), (71, 94, 'TASK')]}\n",
      "{'entities': [(87, 93, 'DATASET')]}\n",
      "{'entities': [(4, 13, 'DATASET')]}\n",
      "{'entities': [(4, 8, 'DATASET'), (40, 54, 'TASK')]}\n",
      "{'entities': [(13, 41, 'MODEL'), (45, 63, 'TASK')]}\n",
      "{'entities': [(53, 80, 'TASK')]}\n",
      "{'entities': [(135, 162, 'TASK')]}\n",
      "{'entities': [(129, 145, 'TASK'), (64, 87, 'TASK')]}\n",
      "{'entities': [(76, 90, 'TASK')]}\n",
      "{'entities': [(166, 177, 'TASK')]}\n",
      "{'entities': [(161, 164, 'TASK')]}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "from spacy.training import Example\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    ('We introduce a new language representation model called BERT, which stands for', {'entities': [(56, 60, 'MODEL')]}),\n",
    "    ('the pre-trained BERT model can be finetuned with just one additional', {'entities': [(16, 20, 'MODEL')]}),\n",
    "    ('BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation', {'entities': [(0, 4, 'MODEL')]}),\n",
    "    ('by introducing a novel, effcient, gradient-based method called Long Short-Term Memory\" (LSTM)', {'entities': [(63, 85, 'MODEL')]}),\n",
    "    ('In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to', {'entities': [(26, 30, 'MODEL'), (32, 61, 'MODEL'), (63, 73, 'MODEL'), (105, 109, 'MODEL')]}),\n",
    "    (\"Section 6 will discuss LSTM's limitations and advantages\", {'entities': [(23, 27, 'MODEL')]}),\n",
    "    ('We propose a new simple network architecture, the Transformer, based solely', {'entities': [(50, 61, 'MODEL')]}),\n",
    "    ('Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea', {'entities': [(25, 28, 'MODEL')]}),\n",
    "    ('positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.', {'entities': [(64, 71, 'MODEL'), (96, 103, 'MODEL')]}),\n",
    "    ('Self-attention has been used successfully in a variety of tasks including reading comprehension', {'entities': [(0, 14, 'MODEL')]}),\n",
    "    ('contains a fully-connected feed-forward network,', {'entities': [(11, 26, 'MODEL'), (27, 39, 'MODEL')]}),\n",
    "    ('Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens', {'entities': [(19, 40, 'MODEL')]}),\n",
    "    ('In terms of computational complexity, self-attention layers are faster than recurrent layers', {'entities': [(76, 91, 'MODEL'), (38, 52, 'MODEL')]}),\n",
    "    ('A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions', {'entities': [(9, 22, 'MODEL')]}),\n",
    "    ('Variations on the Transformer architecture', {'entities': [(18, 29, 'MODEL')]}),\n",
    "    ('Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes', {'entities': [(17, 37, 'MODEL'), (13, 16, 'MODEL')]}),\n",
    "    ('all previously reported models with the exception of the Recurrent Neural Network Grammar', {'entities': [(57, 81, 'MODEL')]}),\n",
    "    ('we presented the Transformer, the first sequence transduction model based entirely on attention', {'entities': [(40, 61, 'MODEL'), (86, 95, 'MODEL'), (17, 28, 'MODEL')]}),\n",
    "    ('We are excited about the future of attention-based models and plan to apply them to other tasks', {'entities': [(35, 44, 'MODEL')]}),\n",
    "    ('Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU)', {'entities': [(99, 116, 'MODEL'), (175, 178, 'MODEL'), (118, 122, 'MODEL')]}),\n",
    "    ('Also, we found GRU to be comparable to LSTM.', {'entities': [(15, 18, 'MODEL'), (39, 43, 'MODEL')]}),\n",
    "    ('Recurrent neural networks have recently shown promising results in many machine learning tasks.', {'entities': [(0, 24, 'MODEL')]}),\n",
    "    ('we concluded that by using fixed number of parameters for all models on some datasets GRU, can outperform LSTM units', {'entities': [(86, 89, 'MODEL'), (106, 110, 'MODEL')]}),\n",
    "    ('a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network, most commonly applied to analyze visual imagery.', {'entities': [(40, 47, 'MODEL'), (2, 30, 'MODEL'), (32, 35, 'MODEL')]}),\n",
    "    ('I. Valera is working on a LSTM model', {'entities': [(26, 30, 'MODEL')]}),\n",
    "    ('the transformer attention block has 12 layer', {'entities': [(16, 25, 'MODEL'), (4, 15, 'MODEL')]}),\n",
    "    ('Google Research is working with deep mind on reinforcement learning models', {'entities': [(45, 67, 'MODEL')]}),\n",
    "    ('F. Schneider is training a transformer to categorize different models.', {'entities': [(27, 38, 'MODEL')]}),\n",
    "    ('Numbers are not models, efficient net, deeplab v3, or conv nets are models.', {'entities': [(39, 46, 'MODEL'), (54, 63, 'MODEL'), (24, 37, 'MODEL')]}),\n",
    "    ('BLEU and WSJ are two different datasets for sequence-to-sequence learning', {'entities': [(44, 64, 'MODEL'), (0, 4, 'DATASET'), (9, 12, 'DATASET')]}),\n",
    "    ('The MNIST database of handwritten digits, available from this page', {'entities': [(4, 9, 'DATASET')]}),\n",
    "    ('A prototype deep learning approach with transfer learning and convolutional neural networks (MobileNetV2) correctly identifies the ten overhead classes with average accuracy of 96.7%', {'entities': [(93, 104, 'MODEL'), (62, 90, 'MODEL') ]}),\n",
    "    ('view of 10 important objects and follows the general formatting requirements of the most popular machine learning task: digit recognition with MNIST', {'entities': [(143, 148, 'DATASET')]}),\n",
    "    ('Researchers over the last two decades [4] have spawned more than 48,000 MNIST-related publications', {'entities': [(72, 77, 'DATASET')]}),\n",
    "    ('There are many important datasets such as ImageNet, Coco, and CIFAR-10', {'entities': [(42, 50, 'DATASET'), (52, 56, 'DATASET'), (62, 70, 'DATASET')]}),\n",
    "    ('Imagenet dataset is made by the group of researchers', {'entities': [(0, 8, 'DATASET')]}),\n",
    "    ('class labels ImageNet provides 1000 images for each synset', {'entities': [(13, 21, 'DATASET')]}),\n",
    "    ('The Coco dataset is a large-scale object detection, segmentation, and captioning dataset.', {'entities': [(4, 8, 'DATASET'), (34, 50, 'TASK'), (70, 80, 'TASK')]}),\n",
    "    ('In COCO dataset annotations are stored in a JSON file.', {'entities': [(3, 7, 'DATASET')]}),\n",
    "    ('When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks in image classification tasks', {'entities': [(153, 171, 'MODEL'), (173, 176, 'MODEL'), (233, 246, 'MODEL'), (140, 144, 'DATASET'), (129, 138, 'DATASET'), (119, 127, 'DATASET'), (259, 279, 'TASK')]}),\n",
    "    ('Geoffrey E Hinton and Yoshua Bengio are the ML researchers who inveted transformers and attention used for NLP', {'entities': [(88, 97, 'MODEL'), (71, 82, 'MODEL'), (107, 110, 'TASK')]}),\n",
    "    ('√ and emails like email@abc.com are not models. NLP is a task, VTAB is a dataset. The vision transformer is a model.', {'entities': [(86, 104, 'MODEL'), (63, 67, 'DATASET'), (48, 51, 'TASK')]}),\n",
    "    (\"By using our method to leverage GPT-3's zero-shot translation capability, we achieve a new state-of-the-art. this is an email asxe@gmail.com\", {'entities': [(32, 37, 'MODEL'), (50, 61, 'TASK')]}),\n",
    "    ('generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow', {'entities': [(0, 30, 'MODEL'), (32, 35, 'MODEL')]}),\n",
    "    ('Though originally proposed as a form of generative modeling for unsupervised learning, GANs have also proven useful for semi-supervised learning,[2] fully supervised learning,[3] and reinforcement learning (RL).[4]', {'entities': [(87, 90, 'MODEL'), (120, 144, 'TASK'), (183, 205, 'TASK'), (207, 209, 'TASK'), (149, 174, 'TASK'), (64, 85, 'TASK'), (40, 59, 'TASK')]}),\n",
    "    ('State-of-art transfer learning research use GANs to enforce the alignment of the latent feature space, such as in deep reinforcement learning', {'entities': [(44, 47, 'MODEL'), (13, 30, 'TASK'), (119, 141, 'TASK')]}),\n",
    "    ('usually simply called neural networks (NNs), are computing systems inspired by the biological neural networks', {'entities': [(22, 37, 'MODEL'), (94, 109, 'MODEL'), (39, 41, 'MODEL')]}),\n",
    "    ('Warren McCulloch and Walter Pitts[1] (1943) opened the subject by creating a computational model for neural networks', {'entities': [(101, 116, 'MODEL')]}),\n",
    "    ('In machine learning, a variational autoencoder,[1] also known as VAE, is the artificial neural network architecture introduced by Diederik P Kingma and Max Welling', {'entities': [(65, 68, 'MODEL'), (35, 46, 'MODEL') ]}),\n",
    "    ('This paper proposes a three-dimensional research framework that combines NLP techniques with economic and educational research to quantify the alignment between course syllabi and job postings.', {'entities': [(93, 101, 'TASK'), (106, 115, 'TASK'), (73, 76, 'TASK')]}),\n",
    "    ('Technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise.', {'entities': [(50, 59, 'TASK'), (39, 49, 'TASK')]}),\n",
    "    ('not just for medical question answering, but also to engage in conversations about general topics and current events', {'entities': [(21, 39, 'TASK')]}),\n",
    "    ('A Speech-enabled Fixed-phrase Translator for Healthcare Accessibility', {'entities': [(45, 55, 'TASK')]}),\n",
    "    ('Using Word Embeddings to Analyze Teacher Evaluations: An Application to a Filipino Education Non-Profit Organization', {'entities': [(83, 92, 'TASK')]}),\n",
    "    ('Analysis of teacher evaluations is crucial to the development of robust educational programs, particularly through the validation of desirable qualities being reflected on in the text', {'entities': [(72, 81, 'TASK') ]}),\n",
    "    ('Applying Natural Language Processing (NLP) techniques to improve the quality of education programs is a crucial step in ensuring the NLP community’s contributions to Social Good', {'entities': [(166, 177, 'TASK'), (80, 89, 'TASK'), (38, 41, 'TASK'), (133, 136, 'TASK'), (9, 36, 'TASK')]}),\n",
    "    ('Information extraction and question answering have the potential to introduce a new paradigm for how machine learning is applied to criminal law.', {'entities': [(0, 22, 'TASK'), (141, 144, 'TASK'), (27, 45, 'TASK')]}),\n",
    "    ('It is clear that information extraction can play an important role in surfacing these facts, which are still important to understand', {'entities': [(17, 39, 'TASK')]}),\n",
    "    ('We analyze unsupervised, weakly supervised, and pre-trained models’ ability to extract such factual information from the free-form dialogue of California parole hearings', {'entities': [(25, 42, 'TASK'), (131, 139, 'TASK'), (11, 23, 'TASK')]}),\n",
    "    ('With a few exceptions, most F1 scores are below 0.85. We use this opportunity to highlight some opportunities for further research for information extraction and question answering.', {'entities': [(135, 157, 'TASK'), (162, 180, 'TASK')]}),\n",
    "    ('We encourage new developments in NLP to enable analysis and review of legal cases to be done in a post-hoc, not predictive, manner.', {'entities': [(126, 128, 'MODEL'), (70, 81, 'TASK'), (33, 36, 'TASK')]}),\n",
    "    ('The impact of online discussions on daily life and mental health has prompted multiple studies on online conversational dynamics.', {'entities': [(105, 117, 'TASK'), (51, 64, 'TASK')]}),\n",
    "    ('Improving Policing with Natural Language Processing', {'entities': [(0, 18, 'TASK'), (24, 51, 'TASK')]}),\n",
    "    ('This article explores the potential for Natural Language Processing (NLP) to enable a more effective, prevention focused and less confrontational policing model ', {'entities': [(69, 72, 'TASK'), (40, 67, 'TASK')]}),\n",
    "    ('In this work, we present Theano, a Greek-speaking virtual assistant for COVID-19. Theano presents users with COVID-19 statistics and facts and informs users about the best health practices as well as the latest COVID-19 related guidelines.', {'entities': [(50, 64, 'TASK'), (72, 80, 'TASK'), (109, 117, 'TASK'), (211, 219, 'TASK')]}),\n",
    "    ('Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics.', {'entities': [(44, 59, 'MODEL'), (10, 25, 'MODEL'), (228, 242, 'TASK'), (153, 171, 'TASK'), (173, 193, 'TASK'), (195, 222, 'TASK')]}),\n",
    "    (\"While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.\", {'entities': [(62, 65, 'MODEL'), (102, 105, 'MODEL'), (157, 160, 'MODEL'), (46, 57, 'MODEL'), (227, 235, 'DATASET'), (220, 225, 'DATASET')]}),\n",
    "    ('We optimize the primal problem of the SVM model and the gradients can be backpropagated to learn lower level features', {'entities': [(38, 41, 'MODEL')]}),\n",
    "    ('Our models are essentially same as the ones proposed in (Zhong & Ghosh, 2000; Nagiet al., 2012), with the minor novelty of using the loss from the L2-SVM instead of the standard hinge loss', {'entities': [(147, 153, 'MODEL')]}),\n",
    "    ('Compared to nets using a top layer softmax, we demonstrate superior performance on MNIST, CIFAR-10, and on a recent Kaggle competition on recognizing face expressions.', {'entities': [(90, 98, 'DATASET'), (83, 88, 'DATASET'), (138, 166, 'TASK')]}),\n",
    "    ('Comparing the two models in Sec. 3.4, we believe the performance gain is largely due to the superior regularization effects of the SVM loss function, rather than an advantage from better parameter optimization.', {'entities': [(131, 134, 'MODEL')]}),\n",
    "    ('Our experiments show that Newton boosting outperforms gradient and hybrid gradient Newton boosting in terms of predictive accuracy on the majority of datasets.', {'entities': [(26, 41, 'MODEL'), (83, 98, 'MODEL')]}),\n",
    "    ('For example, you can’t say that neural networks are always better than decision trees or vice-versa.', {'entities': [(71, 85, 'MODEL'), (32, 47, 'MODEL')]}),\n",
    "    ('Linear regression is perhaps one of the most well-known and well-understood algorithms in statistics and machine learning.', {'entities': [(7, 17, 'TASK')]}),\n",
    "    ('If you have more than two classes then the Linear Discriminant Analysis (LDA) algorithm is the preferred linear classification technique.', {'entities': [(43, 71, 'MODEL'), (73, 76, 'MODEL')]}),\n",
    "    ('The representation of LDA is pretty straight forward. It consists of statistical properties of your data, calculated for each class.', {'entities': [(22, 25, 'MODEL')]}),\n",
    "    ('Naive Bayes is a simple but surprisingly powerful algorithm for predictive modeling.', {'entities': [(0, 11, 'MODEL')]}),\n",
    "    ('Models like naive bayes, LDA, decision trees and newton boosting can be used for many tasks such as bioinformatics, speech recognition, mental health discovery, legal cases, and many more tasks.', {'entities': [(12, 23, 'MODEL'), (25, 28, 'MODEL'), (30, 44, 'MODEL'), (49, 64, 'MODEL'), (100, 114, 'TASK'), (116, 134, 'TASK'), (136, 149, 'TASK'), (161, 172, 'TASK')]}),\n",
    "    ('In this paper we applied decision trees to social good, particularly we used the newton boosting algorithm to predict COVID-19 and mental health cases.', {'entities': [(25, 39, 'MODEL'), (81, 96, 'MODEL'), (118, 126, 'TASK'), (131, 144, 'TASK'), (43, 54, 'TASK')]}),\n",
    "    ('The CASIA 3D Face dataset can be used for face recognition', {'entities': [(4, 12, 'DATASET'), (42, 58, 'TASK')]}),\n",
    "    ('The TV Human interaction dataset can be used for action recognition.', {'entities': [(4, 12, 'DATASET'), (49, 67, 'TASK')]}),\n",
    "    ('Zalando solves the fashion task by using the Fashion-MNIST dataset.', {'entities': [(45, 58, 'DATASET'), (19, 26, 'TASK') ]}),\n",
    "    ('FieldSAFE is a multi-modal dataset for obstacle detection in agriculture including stereo camera, thermal camera, web camera, 360-degree camera, lidar, radar, and precise localization.', {'entities': [(0, 9, 'DATASET'), (39, 57, 'TASK')]}),\n",
    "    ('The CASIA-HWDB is a offline handwritten Chinese character database for handwritten recognition with 3755 classes in the GB 2312 character set.', {'entities': [(4, 14, 'DATASET'), (71, 94, 'TASK')]}),\n",
    "    ('Maritime scenes of optical aerial images from the visible spectrum can be found in the MASATI dataset.', {'entities': [(87, 93, 'DATASET')]}),\n",
    "    ('The MovieLens database contains 22,000,000 ratings and 580,000 tags applied to 33,000 movies by 240,000 users. ', {'entities': [(4, 13, 'DATASET')]}),\n",
    "    ('The XIAS aerial database can be used to detect poverty in rural areas.', {'entities': [(4, 8, 'DATASET'), (40, 54, 'TASK')]}),\n",
    "    ('We propose a convolutional neural network to detect clean water regions from aerial images', {'entities': [(13, 41, 'MODEL'), (45, 63, 'TASK')]}),\n",
    "    ('The application of randomization-based approaches to renewable energy prediction problems has been massive in the last few years, including many different types of randomization-based approaches', {'entities': [(53, 80, 'TASK')]}),\n",
    "    ('In this paper we review the most important characteristics of randomization-based machine learning approaches and their application to renewable energy prediction problems.', {'entities': [(135, 162, 'TASK')]}),\n",
    "    ('Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. ', {'entities': [(129, 145, 'TASK'), (64, 87, 'TASK')]}),\n",
    "    ('We call on the machine learning community to join the global effort against climate change. ', {'entities': [(76, 90, 'TASK')]}),\n",
    "    ('Highly inﬂuential works in AI ethics include (Buolamwini and Gebru, 2018; Mitchell et al., 2019; Raji et al., 2020; Chen et al., 2019; Blodgett et al., 2020). AI for social good (AI4SG) (Tomaˇsevet al., 2020) is a related sub-ﬁeld.', {'entities': [(166, 177, 'TASK')]}),\n",
    "    ('Starting with early discussions in works such as (Hovy and Spruit, 2016; Leidner and Plachouras, 2017), the communities introduced theﬁrst workshop on ethics in NLP (Hovy et al., 2017). ', {'entities': [(161, 164, 'TASK')]}),\n",
    "    ('provide general guidelines for successful AI4SG collaborations through the lens of United Nations (UN) sustainable development goals (SDGs) (United Nations , 2015) and (Hovy and Spruit, 2016; Leidner and Plachouras, 2017).', {'entities': []}),\n",
    "    ('Our main goal is to answer the question: Given a speciﬁc researcher or team with skills s, and the set of technologies', {'entities': []}),\n",
    "    ('§3 relies on causal structure models as a framework to estimate I for t ∈ T , considering that t can be an indirect cause of impact', {'entities': []}),\n",
    "    ('plicated analyses (e.g., business decisions), such as radar charts to rate a decision candidate or SMART goals.', {'entities': []}),\n",
    "    ('such as sacriﬁcing one life for ﬁve lives in the Trolley problem (Thomson , 1976), which scores high on consequentialism but low on deontology and virtue', {'entities': []}),\n",
    "    ('achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65.', {'entities': []}),\n",
    "    ('Work done at DeepMind modify aregularization scheme to improve conditioning, demonstrably boosting performance.', {'entities': []}),\n",
    "    ('where z ∈ Rdz is a latent variable drawn from distribution p(z) such as N (0, I) or U[−1, 1].', {'entities': []}),\n",
    "    ('Text WRITTEN IN LARGE LETTERS LIKE BACKGROUND IS NOT ALWAYS A MODEL', {'entities': []}),\n",
    "    ('in its original form (Goodfellow et al., 2014) involves ﬁnding a Nash equilibrium to the following two player min-max problem:', {'entities': []}),\n",
    "    ('For evaluation, we employ moving averages of G’s weights following Karras et al. (2018); Mescheder et al. (2018); Yazc et al. (2018), with a decay of 0.9999.', {'entities': []})\n",
    "]\n",
    "\n",
    "\n",
    "def extract_entity(text, entities):\n",
    "    ents = []\n",
    "    for name, elist in entities:\n",
    "        for entity in elist:\n",
    "            for m in re.finditer(entity, text, flags=re.IGNORECASE):\n",
    "                ents += [(m.start(), m.end(), name)]\n",
    "                print(text[m.start():m.end()])\n",
    "    return (text, { \"entities\": ents })\n",
    "\n",
    "\n",
    "sentence = \"For evaluation, we employ moving averages of G’s weights following Karras et al. (2018); Mescheder et al. (2018); Yazc et al. (2018), with a decay of 0.9999.\"\n",
    "\n",
    "\n",
    "models = ('MODEL', ['naive bayes', 'linear discriminant analysis', 'LDA', 'decision trees', 'newton boosting', 'L2-SVM', 'VAE', 'SVM', 'autoencoder', 'neural nets', 'neural networks', 'NN', 'generative adversarial network', 'GPT-3', 'GAN', 'vision transformer', 'ViT', 'MobileNetV2', 'deeplab', 'conv nets', 'efficient net', 'convnet', 'convolutional neural network', 'short-term memory', 'recurrent neural network', 'sequence-to-sequence', 'CNN', 'convolutional', 'recurrent layer', 'sequence transduction', 'fully-connected', 'feed-forward', 'attention', 'multi-head attention', 'Self-attention', 'ConvS2S', 'bytenet', 'encoder', 'decoder', 'encoder-decoder', 'rnn', 'Transformer', 'BERT', 'GRU', 'BTRL', 'BPTT', 'Recurrent Cascade-Correlation', 'Elman nets', 'LSTM'])\n",
    "datasets = ('DATASET', ['XIAS', 'MovieLens', 'MASATI', 'CASIA-HWDB', 'FieldSAFE', 'Fashion-MNIST', 'TV Human', 'CASIA 3D', 'VTAB', 'CIFAR-10', 'CIFAR-100', 'ImageNet', 'Coco', 'CIFAR-10 ', 'BLEU', 'WSJ', 'MNIST'])\n",
    "tasks = ('TASK', ['climate change', 'changing climate', 'reducing greenhouse gas', 'renewable energy prediction', 'detect clean water', 'detect poverty', 'handwritten recognition', 'obstacle detection', 'fashion', 'action recognition', 'face recognition', 'recognizing face expressions', 'bioinformatics', 'speech recognition', 'virtual assist', 'COVID-19', 'improving policing', 'self-disclosure', 'conversation', 'mental health', 'legal cases', 'weakly supervised', 'dialogue', 'information extraction', 'law', 'social good', 'question answering', 'vigilance', 'healthcare', 'economic', 'education', 'supervised learning', 'transfer learning', 'semi-supervised learning', 'reinforcement learning', 'RL', 'fully supervised learning', 'unsupervised', 'unsupervised learning', 'generative modeling', 'translation', 'image classification', 'image segmentation', 'object detection', 'captioning', 'keypoint detection', 'regression', 'NLP', 'natural language processing', 'natural language understanding'])\n",
    "\n",
    "print(extract_entity(sentence, [models, datasets, tasks]))\n",
    "\n",
    "examples = []\n",
    "for sentence, annots in TRAIN_DATA:\n",
    "    print(annots)\n",
    "    examples.append(Example.from_dict(nlp.make_doc(sentence), annots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    One\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp=spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Getting the ner component\n",
    "ner=nlp.get_pipe('ner')\n",
    "print(ner.labels)\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "test_text = \"One of the fun things about ganache is that you can flavor ganache in a million different ways. These simple, easy and effortless recipes to flavor ganache can make a huge difference to the final outcome of your cake.  Here are a few ways on how to flavor ganache or rather a few ganache flavor variations that I love and are popular with my customers.\"\n",
    "doc = nlp(test_text)\n",
    "# Document Level Entities\n",
    "displacy.render(doc.ents, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 16.245817173190883}\n",
      "Losses {'ner': 44.81991582611095}\n",
      "Losses {'ner': 59.23975864706731}\n",
      "Losses {'ner': 83.4700722705694}\n",
      "Losses {'ner': 92.37359848467165}\n",
      "Losses {'ner': 104.30359511269484}\n",
      "Losses {'ner': 125.89971416842474}\n",
      "Losses {'ner': 144.05619066331934}\n",
      "Losses {'ner': 160.42336654991556}\n",
      "Losses {'ner': 170.45966433528173}\n",
      "Losses {'ner': 186.69324637786985}\n",
      "Losses {'ner': 200.91880773509288}\n",
      "Losses {'ner': 218.00158751146742}\n",
      "Losses {'ner': 226.92959894647507}\n",
      "Losses {'ner': 238.08154219601752}\n",
      "Losses {'ner': 250.9701469158155}\n",
      "Losses {'ner': 265.2905465078103}\n",
      "Losses {'ner': 299.3042637505006}\n",
      "Losses {'ner': 309.21650579691243}\n",
      "Losses {'ner': 322.6328307771606}\n",
      "Losses {'ner': 332.953125964336}\n",
      "Losses {'ner': 347.7448498603868}\n",
      "Losses {'ner': 364.9060898934937}\n",
      "Losses {'ner': 375.53996516113205}\n",
      "Losses {'ner': 387.6541340879654}\n",
      "Losses {'ner': 403.82873557548976}\n",
      "Losses {'ner': 405.96398738149094}\n",
      "Losses {'ner': 15.24171597941313}\n",
      "Losses {'ner': 30.662152707460336}\n",
      "Losses {'ner': 39.48436750180554}\n",
      "Losses {'ner': 53.3706983387583}\n",
      "Losses {'ner': 67.10019759690658}\n",
      "Losses {'ner': 76.45438100044885}\n",
      "Losses {'ner': 87.57736737332925}\n",
      "Losses {'ner': 92.58997075975094}\n",
      "Losses {'ner': 101.38030758659625}\n",
      "Losses {'ner': 113.95659958759825}\n",
      "Losses {'ner': 122.80898595313738}\n",
      "Losses {'ner': 140.06187881815458}\n",
      "Losses {'ner': 152.09757215256192}\n",
      "Losses {'ner': 160.60432206839548}\n",
      "Losses {'ner': 171.53085059108665}\n",
      "Losses {'ner': 180.37701012695427}\n",
      "Losses {'ner': 192.13721994241453}\n",
      "Losses {'ner': 199.360925317807}\n",
      "Losses {'ner': 210.93413552027008}\n",
      "Losses {'ner': 228.93030421174038}\n",
      "Losses {'ner': 238.3856160134906}\n",
      "Losses {'ner': 254.72126974855703}\n",
      "Losses {'ner': 261.51964098305643}\n",
      "Losses {'ner': 273.1744025855837}\n",
      "Losses {'ner': 284.8286294038519}\n",
      "Losses {'ner': 300.49280983810263}\n",
      "Losses {'ner': 302.7237045747372}\n",
      "Losses {'ner': 11.21774473357982}\n",
      "Losses {'ner': 20.7314067803909}\n",
      "Losses {'ner': 31.37992908972342}\n",
      "Losses {'ner': 49.37138052237174}\n",
      "Losses {'ner': 62.95302167358636}\n",
      "Losses {'ner': 65.91573867270085}\n",
      "Losses {'ner': 74.15362330130552}\n",
      "Losses {'ner': 84.00866222362225}\n",
      "Losses {'ner': 93.42380969544962}\n",
      "Losses {'ner': 103.5752011219775}\n",
      "Losses {'ner': 107.43717717665373}\n",
      "Losses {'ner': 110.97165729777245}\n",
      "Losses {'ner': 124.60590061098759}\n",
      "Losses {'ner': 134.5664820995923}\n",
      "Losses {'ner': 151.2595592200019}\n",
      "Losses {'ner': 158.070093585192}\n",
      "Losses {'ner': 165.14913529484872}\n",
      "Losses {'ner': 178.4814957639446}\n",
      "Losses {'ner': 190.79560182494808}\n",
      "Losses {'ner': 200.34346155110924}\n",
      "Losses {'ner': 207.49384524198464}\n",
      "Losses {'ner': 212.4262205305499}\n",
      "Losses {'ner': 218.8519573792061}\n",
      "Losses {'ner': 226.0419378655601}\n",
      "Losses {'ner': 233.57661687402742}\n",
      "Losses {'ner': 246.65959217029177}\n",
      "Losses {'ner': 249.93420028801285}\n",
      "Losses {'ner': 7.209252919112259}\n",
      "Losses {'ner': 24.921538062502293}\n",
      "Losses {'ner': 36.197622131877836}\n",
      "Losses {'ner': 46.595267445987275}\n",
      "Losses {'ner': 51.6083114055311}\n",
      "Losses {'ner': 82.22451664796381}\n",
      "Losses {'ner': 88.34936649886714}\n",
      "Losses {'ner': 98.54884062370803}\n",
      "Losses {'ner': 106.22181908411778}\n",
      "Losses {'ner': 125.86096337180868}\n",
      "Losses {'ner': 131.83937866562724}\n",
      "Losses {'ner': 140.54113401292545}\n",
      "Losses {'ner': 149.55126362502472}\n",
      "Losses {'ner': 160.64548117674735}\n",
      "Losses {'ner': 168.43192729125033}\n",
      "Losses {'ner': 174.2797857891384}\n",
      "Losses {'ner': 185.66049998876952}\n",
      "Losses {'ner': 193.24629612318844}\n",
      "Losses {'ner': 200.180867665631}\n",
      "Losses {'ner': 206.1889785572698}\n",
      "Losses {'ner': 212.46298410734306}\n",
      "Losses {'ner': 217.62832142552932}\n",
      "Losses {'ner': 226.33727192384563}\n",
      "Losses {'ner': 232.51403215052963}\n",
      "Losses {'ner': 248.3636530950553}\n",
      "Losses {'ner': 253.9212239788642}\n",
      "Losses {'ner': 255.82341634417088}\n",
      "Losses {'ner': 6.727624836445797}\n",
      "Losses {'ner': 44.77311945088401}\n",
      "Losses {'ner': 85.53382315306513}\n",
      "Losses {'ner': 94.46476263772925}\n",
      "Losses {'ner': 98.08704751337206}\n",
      "Losses {'ner': 116.25369131720112}\n",
      "Losses {'ner': 124.32331197974976}\n",
      "Losses {'ner': 137.43650594400276}\n",
      "Losses {'ner': 142.72635710653833}\n",
      "Losses {'ner': 150.92370220390416}\n",
      "Losses {'ner': 167.7757882176909}\n",
      "Losses {'ner': 183.2007968521409}\n",
      "Losses {'ner': 189.243875405217}\n",
      "Losses {'ner': 199.86498127029537}\n",
      "Losses {'ner': 203.5712132156912}\n",
      "Losses {'ner': 209.57189941373488}\n",
      "Losses {'ner': 217.33936350094115}\n",
      "Losses {'ner': 232.0626307598193}\n",
      "Losses {'ner': 238.24902525316637}\n",
      "Losses {'ner': 247.50387225639113}\n",
      "Losses {'ner': 258.76958176147485}\n",
      "Losses {'ner': 266.0156581903708}\n",
      "Losses {'ner': 273.02585840390395}\n",
      "Losses {'ner': 277.8472331516974}\n",
      "Losses {'ner': 284.1643763958192}\n",
      "Losses {'ner': 292.4617000463494}\n",
      "Losses {'ner': 294.2903851422536}\n",
      "Losses {'ner': 13.763272148591625}\n",
      "Losses {'ner': 22.355232410877633}\n",
      "Losses {'ner': 26.39860098149529}\n",
      "Losses {'ner': 40.27522558637591}\n",
      "Losses {'ner': 46.24358101994491}\n",
      "Losses {'ner': 52.39641902826209}\n",
      "Losses {'ner': 57.69145677928003}\n",
      "Losses {'ner': 69.28784435140113}\n",
      "Losses {'ner': 83.22118695477181}\n",
      "Losses {'ner': 96.89057817414903}\n",
      "Losses {'ner': 111.7174366125748}\n",
      "Losses {'ner': 123.59097599940425}\n",
      "Losses {'ner': 130.10948859349753}\n",
      "Losses {'ner': 137.40561354667847}\n",
      "Losses {'ner': 142.7366615772367}\n",
      "Losses {'ner': 147.2103298152425}\n",
      "Losses {'ner': 153.37935188018886}\n",
      "Losses {'ner': 164.06198448290405}\n",
      "Losses {'ner': 173.14036256278803}\n",
      "Losses {'ner': 176.77557198497962}\n",
      "Losses {'ner': 180.78796599135552}\n",
      "Losses {'ner': 190.03090002448153}\n",
      "Losses {'ner': 196.0204181184227}\n",
      "Losses {'ner': 203.54818723688763}\n",
      "Losses {'ner': 209.52792551693014}\n",
      "Losses {'ner': 217.56021759486046}\n",
      "Losses {'ner': 217.75908258567537}\n",
      "Losses {'ner': 9.0937702315181}\n",
      "Losses {'ner': 31.565426308238315}\n",
      "Losses {'ner': 42.513238088145485}\n",
      "Losses {'ner': 46.37663927056643}\n",
      "Losses {'ner': 52.35689065764571}\n",
      "Losses {'ner': 60.62617721536438}\n",
      "Losses {'ner': 63.08141619062634}\n",
      "Losses {'ner': 67.4146408159169}\n",
      "Losses {'ner': 75.54965326231297}\n",
      "Losses {'ner': 82.36849393925041}\n",
      "Losses {'ner': 90.64616359858479}\n",
      "Losses {'ner': 93.29047491465992}\n",
      "Losses {'ner': 96.98276782488446}\n",
      "Losses {'ner': 105.17774912418783}\n",
      "Losses {'ner': 110.43713510103368}\n",
      "Losses {'ner': 119.84356326220875}\n",
      "Losses {'ner': 126.74940148262988}\n",
      "Losses {'ner': 140.64336588217014}\n",
      "Losses {'ner': 148.49915517630927}\n",
      "Losses {'ner': 152.36162887245501}\n",
      "Losses {'ner': 162.7793472336944}\n",
      "Losses {'ner': 166.88938338507296}\n",
      "Losses {'ner': 182.1184228852958}\n",
      "Losses {'ner': 187.83724809951838}\n",
      "Losses {'ner': 193.81029449044343}\n",
      "Losses {'ner': 197.93344139587646}\n",
      "Losses {'ner': 199.33335238559928}\n",
      "Losses {'ner': 6.146133142600843}\n",
      "Losses {'ner': 11.252240256138696}\n",
      "Losses {'ner': 18.186275676149553}\n",
      "Losses {'ner': 23.897682368310367}\n",
      "Losses {'ner': 27.406716346398586}\n",
      "Losses {'ner': 30.138828502343365}\n",
      "Losses {'ner': 39.71858659643738}\n",
      "Losses {'ner': 44.09972186821219}\n",
      "Losses {'ner': 59.962614008104474}\n",
      "Losses {'ner': 64.15643979166911}\n",
      "Losses {'ner': 68.10449552036351}\n",
      "Losses {'ner': 71.40239089473205}\n",
      "Losses {'ner': 76.64979910404189}\n",
      "Losses {'ner': 83.81856866534712}\n",
      "Losses {'ner': 89.09767408729299}\n",
      "Losses {'ner': 94.26185747366402}\n",
      "Losses {'ner': 106.7929991961004}\n",
      "Losses {'ner': 121.4721029501593}\n",
      "Losses {'ner': 124.15343946712208}\n",
      "Losses {'ner': 137.2409296332445}\n",
      "Losses {'ner': 144.39115160362795}\n",
      "Losses {'ner': 148.7656876273299}\n",
      "Losses {'ner': 163.51092176003579}\n",
      "Losses {'ner': 167.98652338275127}\n",
      "Losses {'ner': 175.69385367932728}\n",
      "Losses {'ner': 181.3821300506245}\n",
      "Losses {'ner': 181.8883546532576}\n",
      "Losses {'ner': 9.475425498439119}\n",
      "Losses {'ner': 16.449760306193365}\n",
      "Losses {'ner': 21.71039486147498}\n",
      "Losses {'ner': 28.514490006103667}\n",
      "Losses {'ner': 36.86724276309237}\n",
      "Losses {'ner': 43.71307681828715}\n",
      "Losses {'ner': 54.69440616375156}\n",
      "Losses {'ner': 65.27316615599364}\n",
      "Losses {'ner': 73.51925017498633}\n",
      "Losses {'ner': 91.57133820315389}\n",
      "Losses {'ner': 96.2235388152675}\n",
      "Losses {'ner': 100.41709009024461}\n",
      "Losses {'ner': 105.6717668574556}\n",
      "Losses {'ner': 112.64769293372984}\n",
      "Losses {'ner': 114.93506906475206}\n",
      "Losses {'ner': 122.19837639179816}\n",
      "Losses {'ner': 127.3765911675505}\n",
      "Losses {'ner': 131.4772753799412}\n",
      "Losses {'ner': 136.36929105713804}\n",
      "Losses {'ner': 144.57917788181337}\n",
      "Losses {'ner': 148.08581544608543}\n",
      "Losses {'ner': 152.84817879949378}\n",
      "Losses {'ner': 165.31785758053073}\n",
      "Losses {'ner': 171.76485555515583}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 175.9380086006604}\n",
      "Losses {'ner': 181.01744998661118}\n",
      "Losses {'ner': 182.4078785494458}\n",
      "Losses {'ner': 4.1573603168533735}\n",
      "Losses {'ner': 8.157757228994535}\n",
      "Losses {'ner': 12.383621706718893}\n",
      "Losses {'ner': 17.959653207274094}\n",
      "Losses {'ner': 22.598112766860528}\n",
      "Losses {'ner': 31.09810574883547}\n",
      "Losses {'ner': 38.11085119371016}\n",
      "Losses {'ner': 43.088447192909165}\n",
      "Losses {'ner': 49.01111905235379}\n",
      "Losses {'ner': 55.552342608038224}\n",
      "Losses {'ner': 59.64482675363686}\n",
      "Losses {'ner': 63.232842061390734}\n",
      "Losses {'ner': 74.19524714176711}\n",
      "Losses {'ner': 76.46042029238693}\n",
      "Losses {'ner': 81.45257762130748}\n",
      "Losses {'ner': 87.52236657605842}\n",
      "Losses {'ner': 96.49971257327284}\n",
      "Losses {'ner': 106.50569535511123}\n",
      "Losses {'ner': 109.30202824789171}\n",
      "Losses {'ner': 112.33409010493979}\n",
      "Losses {'ner': 115.98198854699517}\n",
      "Losses {'ner': 122.76032600779158}\n",
      "Losses {'ner': 126.74435876665372}\n",
      "Losses {'ner': 129.57422865532087}\n",
      "Losses {'ner': 136.41366255854612}\n",
      "Losses {'ner': 140.33677440716582}\n",
      "Losses {'ner': 141.43027024830266}\n",
      "Losses {'ner': 7.384116729345373}\n",
      "Losses {'ner': 11.283716118284726}\n",
      "Losses {'ner': 13.390670927640247}\n",
      "Losses {'ner': 18.45961642556432}\n",
      "Losses {'ner': 27.793236482599717}\n",
      "Losses {'ner': 28.863170048146873}\n",
      "Losses {'ner': 33.42777390662799}\n",
      "Losses {'ner': 40.78018160614248}\n",
      "Losses {'ner': 42.44616325856217}\n",
      "Losses {'ner': 46.279820902491714}\n",
      "Losses {'ner': 51.090086675672715}\n",
      "Losses {'ner': 59.24401665859441}\n",
      "Losses {'ner': 62.3481707462681}\n",
      "Losses {'ner': 66.76794189813674}\n",
      "Losses {'ner': 73.39904415426633}\n",
      "Losses {'ner': 78.38142461549567}\n",
      "Losses {'ner': 90.05821107216676}\n",
      "Losses {'ner': 94.84912265797809}\n",
      "Losses {'ner': 96.84986537941603}\n",
      "Losses {'ner': 100.37020546896753}\n",
      "Losses {'ner': 106.58972967493251}\n",
      "Losses {'ner': 115.41855271961178}\n",
      "Losses {'ner': 120.98095471307175}\n",
      "Losses {'ner': 123.89251027529117}\n",
      "Losses {'ner': 127.92544938095583}\n",
      "Losses {'ner': 134.14906245368826}\n",
      "Losses {'ner': 134.4060579250762}\n",
      "Losses {'ner': 1.3941689296189341}\n",
      "Losses {'ner': 6.981452376074996}\n",
      "Losses {'ner': 10.914387442336675}\n",
      "Losses {'ner': 20.73713188328522}\n",
      "Losses {'ner': 26.839710687233705}\n",
      "Losses {'ner': 30.579183958713237}\n",
      "Losses {'ner': 39.55571727771323}\n",
      "Losses {'ner': 45.61691355931091}\n",
      "Losses {'ner': 49.401686500309715}\n",
      "Losses {'ner': 52.496615233108145}\n",
      "Losses {'ner': 56.532977951424826}\n",
      "Losses {'ner': 67.02705554044277}\n",
      "Losses {'ner': 69.31205237311173}\n",
      "Losses {'ner': 71.54342783352828}\n",
      "Losses {'ner': 74.84230074649602}\n",
      "Losses {'ner': 79.89607864459157}\n",
      "Losses {'ner': 83.39949548790675}\n",
      "Losses {'ner': 88.60990234408906}\n",
      "Losses {'ner': 91.88891801080308}\n",
      "Losses {'ner': 93.74063496449158}\n",
      "Losses {'ner': 97.88852591595513}\n",
      "Losses {'ner': 102.33184707037417}\n",
      "Losses {'ner': 104.9573761035909}\n",
      "Losses {'ner': 106.78142147758555}\n",
      "Losses {'ner': 115.25244301470985}\n",
      "Losses {'ner': 119.26195836737814}\n",
      "Losses {'ner': 119.26491521541627}\n",
      "Losses {'ner': 5.64478456021935}\n",
      "Losses {'ner': 9.849561064927165}\n",
      "Losses {'ner': 12.9989563959169}\n",
      "Losses {'ner': 14.254908909008119}\n",
      "Losses {'ner': 18.339418050959335}\n",
      "Losses {'ner': 21.18079878548734}\n",
      "Losses {'ner': 25.191098429674906}\n",
      "Losses {'ner': 28.0917898743752}\n",
      "Losses {'ner': 34.67869485015584}\n",
      "Losses {'ner': 37.29283118330603}\n",
      "Losses {'ner': 42.68825910021285}\n",
      "Losses {'ner': 51.29108067858001}\n",
      "Losses {'ner': 54.88048504281133}\n",
      "Losses {'ner': 61.90465132607875}\n",
      "Losses {'ner': 68.90822999913519}\n",
      "Losses {'ner': 72.68135341227126}\n",
      "Losses {'ner': 73.79094542129268}\n",
      "Losses {'ner': 77.95239514833736}\n",
      "Losses {'ner': 78.51198521629694}\n",
      "Losses {'ner': 79.2618092898496}\n",
      "Losses {'ner': 83.02092550384457}\n",
      "Losses {'ner': 86.39449995409664}\n",
      "Losses {'ner': 93.79428680255792}\n",
      "Losses {'ner': 96.84374299150035}\n",
      "Losses {'ner': 99.07961609985944}\n",
      "Losses {'ner': 107.85181337470515}\n",
      "Losses {'ner': 107.86074583369758}\n",
      "Losses {'ner': 0.5961691536293724}\n",
      "Losses {'ner': 6.926059155507896}\n",
      "Losses {'ner': 9.269383649042158}\n",
      "Losses {'ner': 11.309482790737256}\n",
      "Losses {'ner': 14.756470964872364}\n",
      "Losses {'ner': 18.867717751762708}\n",
      "Losses {'ner': 20.329294612258593}\n",
      "Losses {'ner': 21.849302993042553}\n",
      "Losses {'ner': 29.216602358901337}\n",
      "Losses {'ner': 31.583799525393086}\n",
      "Losses {'ner': 33.19652931004919}\n",
      "Losses {'ner': 36.998857719399645}\n",
      "Losses {'ner': 39.12287836724846}\n",
      "Losses {'ner': 39.70675584292295}\n",
      "Losses {'ner': 47.74268751698358}\n",
      "Losses {'ner': 51.818086274104374}\n",
      "Losses {'ner': 54.36026313319828}\n",
      "Losses {'ner': 57.901448147711434}\n",
      "Losses {'ner': 59.09063480081319}\n",
      "Losses {'ner': 63.87469218314401}\n",
      "Losses {'ner': 67.8813659170897}\n",
      "Losses {'ner': 68.03641959615837}\n",
      "Losses {'ner': 73.37117552634768}\n",
      "Losses {'ner': 75.8358872307499}\n",
      "Losses {'ner': 76.43129244358641}\n",
      "Losses {'ner': 78.29220593475429}\n",
      "Losses {'ner': 80.02260055672325}\n",
      "Losses {'ner': 3.0056882225540176}\n",
      "Losses {'ner': 3.8095641656541073}\n",
      "Losses {'ner': 4.032318980781531}\n",
      "Losses {'ner': 6.764118557818501}\n",
      "Losses {'ner': 9.611722202272693}\n",
      "Losses {'ner': 11.941994947440712}\n",
      "Losses {'ner': 14.757486566735558}\n",
      "Losses {'ner': 16.544896891825996}\n",
      "Losses {'ner': 21.276386595353955}\n",
      "Losses {'ner': 23.96121617952029}\n",
      "Losses {'ner': 30.097101521120393}\n",
      "Losses {'ner': 30.142501138780823}\n",
      "Losses {'ner': 30.218889275130852}\n",
      "Losses {'ner': 31.90065469496502}\n",
      "Losses {'ner': 35.36196282720246}\n",
      "Losses {'ner': 38.20391186159365}\n",
      "Losses {'ner': 43.425648042345955}\n",
      "Losses {'ner': 43.60658438296851}\n",
      "Losses {'ner': 45.497567318962794}\n",
      "Losses {'ner': 49.68192591781072}\n",
      "Losses {'ner': 51.18468060918676}\n",
      "Losses {'ner': 52.276318166510364}\n",
      "Losses {'ner': 52.50566884073816}\n",
      "Losses {'ner': 55.15530519883044}\n",
      "Losses {'ner': 61.291290363261986}\n",
      "Losses {'ner': 63.966300182275766}\n",
      "Losses {'ner': 66.81807186537664}\n",
      "Losses {'ner': 37.426603960868}\n",
      "Losses {'ner': 38.784250511637126}\n",
      "Losses {'ner': 43.80000327056491}\n",
      "Losses {'ner': 46.86801773843532}\n",
      "Losses {'ner': 48.45070054945282}\n",
      "Losses {'ner': 51.08795889846912}\n",
      "Losses {'ner': 53.24675371432897}\n",
      "Losses {'ner': 69.6184464864576}\n",
      "Losses {'ner': 72.06102885308206}\n",
      "Losses {'ner': 73.90295238953718}\n",
      "Losses {'ner': 75.87346993746672}\n",
      "Losses {'ner': 75.91140320108495}\n",
      "Losses {'ner': 77.26138190854681}\n",
      "Losses {'ner': 77.6469670275218}\n",
      "Losses {'ner': 77.94653540067426}\n",
      "Losses {'ner': 80.79907798776905}\n",
      "Losses {'ner': 82.09979778535545}\n",
      "Losses {'ner': 85.85522973966853}\n",
      "Losses {'ner': 87.42632396294157}\n",
      "Losses {'ner': 93.42397780087569}\n",
      "Losses {'ner': 93.42767411083483}\n",
      "Losses {'ner': 93.76480675724159}\n",
      "Losses {'ner': 96.30401402787203}\n",
      "Losses {'ner': 101.23480655407263}\n",
      "Losses {'ner': 102.52424074223411}\n",
      "Losses {'ner': 104.53933359951677}\n",
      "Losses {'ner': 105.57656736568357}\n",
      "Losses {'ner': 3.1365115609254284}\n",
      "Losses {'ner': 4.758076838994226}\n",
      "Losses {'ner': 19.94273770759768}\n",
      "Losses {'ner': 21.546386035470857}\n",
      "Losses {'ner': 25.505692924141595}\n",
      "Losses {'ner': 25.923884173793102}\n",
      "Losses {'ner': 33.851759551442136}\n",
      "Losses {'ner': 37.59321878800651}\n",
      "Losses {'ner': 46.639874717568595}\n",
      "Losses {'ner': 46.64099422223526}\n",
      "Losses {'ner': 46.7315623433996}\n",
      "Losses {'ner': 49.728024858784735}\n",
      "Losses {'ner': 54.98410218255798}\n",
      "Losses {'ner': 59.96326548019152}\n",
      "Losses {'ner': 64.19683821224822}\n",
      "Losses {'ner': 66.24141992175724}\n",
      "Losses {'ner': 66.78816814188336}\n",
      "Losses {'ner': 68.38249485412481}\n",
      "Losses {'ner': 70.97742341680086}\n",
      "Losses {'ner': 72.00062883506023}\n",
      "Losses {'ner': 72.66007124816477}\n",
      "Losses {'ner': 73.09360320135673}\n",
      "Losses {'ner': 76.40711346040312}\n",
      "Losses {'ner': 82.26740842509321}\n",
      "Losses {'ner': 87.05188850143696}\n",
      "Losses {'ner': 87.0997248140099}\n",
      "Losses {'ner': 87.09972556809606}\n",
      "Losses {'ner': 1.114779837298512}\n",
      "Losses {'ner': 1.2167096758029505}\n",
      "Losses {'ner': 2.2598511832437302}\n",
      "Losses {'ner': 2.3839225307170135}\n",
      "Losses {'ner': 3.498282265125118}\n",
      "Losses {'ner': 6.234890885588938}\n",
      "Losses {'ner': 9.722058153219672}\n",
      "Losses {'ner': 11.238329573360582}\n",
      "Losses {'ner': 11.677859010084205}\n",
      "Losses {'ner': 14.114061303467722}\n",
      "Losses {'ner': 16.094017706401004}\n",
      "Losses {'ner': 16.586742661988705}\n",
      "Losses {'ner': 17.65711012396279}\n",
      "Losses {'ner': 18.69747640311742}\n",
      "Losses {'ner': 24.60874673023401}\n",
      "Losses {'ner': 29.824829226184622}\n",
      "Losses {'ner': 35.53422377182762}\n",
      "Losses {'ner': 41.92984930621156}\n",
      "Losses {'ner': 43.613887325570246}\n",
      "Losses {'ner': 44.81921167698707}\n",
      "Losses {'ner': 46.332268339994215}\n",
      "Losses {'ner': 48.505834568766204}\n",
      "Losses {'ner': 48.518727779247946}\n",
      "Losses {'ner': 49.680849484013294}\n",
      "Losses {'ner': 51.59345021902217}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 53.92667877334174}\n",
      "Losses {'ner': 53.92668934070072}\n",
      "Losses {'ner': 3.3181910386377487}\n",
      "Losses {'ner': 6.6386012047593965}\n",
      "Losses {'ner': 9.559300926495554}\n",
      "Losses {'ner': 9.630168548700949}\n",
      "Losses {'ner': 13.785631911835468}\n",
      "Losses {'ner': 17.042137126012744}\n",
      "Losses {'ner': 17.042384544149005}\n",
      "Losses {'ner': 21.601024820071306}\n",
      "Losses {'ner': 24.372590088338864}\n",
      "Losses {'ner': 24.3982772475205}\n",
      "Losses {'ner': 25.78901704103273}\n",
      "Losses {'ner': 28.29131606082149}\n",
      "Losses {'ner': 31.000285255146135}\n",
      "Losses {'ner': 31.3522314326535}\n",
      "Losses {'ner': 34.889163186477745}\n",
      "Losses {'ner': 34.9867047281605}\n",
      "Losses {'ner': 37.2088762627793}\n",
      "Losses {'ner': 37.21406314833596}\n",
      "Losses {'ner': 39.06129056666857}\n",
      "Losses {'ner': 39.31409900719271}\n",
      "Losses {'ner': 44.980590770033025}\n",
      "Losses {'ner': 47.661464169305894}\n",
      "Losses {'ner': 49.09421077144191}\n",
      "Losses {'ner': 49.99726952688388}\n",
      "Losses {'ner': 50.00398287568317}\n",
      "Losses {'ner': 50.1105546685777}\n",
      "Losses {'ner': 50.574785586170904}\n",
      "Losses {'ner': 0.22101310583044312}\n",
      "Losses {'ner': 1.2437350179373192}\n",
      "Losses {'ner': 1.3278201227603321}\n",
      "Losses {'ner': 1.540567992713819}\n",
      "Losses {'ner': 1.5523039067251843}\n",
      "Losses {'ner': 6.157273489708242}\n",
      "Losses {'ner': 8.199501427841623}\n",
      "Losses {'ner': 9.527432330904565}\n",
      "Losses {'ner': 10.454193978452187}\n",
      "Losses {'ner': 10.902406606994436}\n",
      "Losses {'ner': 10.907157912641349}\n",
      "Losses {'ner': 13.641041811955963}\n",
      "Losses {'ner': 15.64322975884445}\n",
      "Losses {'ner': 21.328928333234533}\n",
      "Losses {'ner': 21.33332970525569}\n",
      "Losses {'ner': 22.9436211883742}\n",
      "Losses {'ner': 26.365908305809555}\n",
      "Losses {'ner': 27.501731289875984}\n",
      "Losses {'ner': 29.380155557976483}\n",
      "Losses {'ner': 31.53877286906319}\n",
      "Losses {'ner': 31.818181977014234}\n",
      "Losses {'ner': 31.82006484429737}\n",
      "Losses {'ner': 40.1539144454025}\n",
      "Losses {'ner': 42.43301868704044}\n",
      "Losses {'ner': 43.30564342906099}\n",
      "Losses {'ner': 43.507104951367246}\n",
      "Losses {'ner': 43.547477884510066}\n",
      "Losses {'ner': 0.02214566980488516}\n",
      "Losses {'ner': 1.0561576776347847}\n",
      "Losses {'ner': 6.925192508449282}\n",
      "Losses {'ner': 8.194484543598405}\n",
      "Losses {'ner': 9.784071065371434}\n",
      "Losses {'ner': 10.09014802343867}\n",
      "Losses {'ner': 12.759186986495887}\n",
      "Losses {'ner': 13.539794610531215}\n",
      "Losses {'ner': 15.44947317488517}\n",
      "Losses {'ner': 17.479238454274373}\n",
      "Losses {'ner': 17.486626457077616}\n",
      "Losses {'ner': 18.028115893135205}\n",
      "Losses {'ner': 19.808520104069856}\n",
      "Losses {'ner': 19.809101321176545}\n",
      "Losses {'ner': 23.03799811532606}\n",
      "Losses {'ner': 24.707589337143148}\n",
      "Losses {'ner': 26.673378530612812}\n",
      "Losses {'ner': 27.983214103031358}\n",
      "Losses {'ner': 28.022487923609052}\n",
      "Losses {'ner': 30.63778714958853}\n",
      "Losses {'ner': 34.53594612396154}\n",
      "Losses {'ner': 34.78711721870912}\n",
      "Losses {'ner': 34.80345461738835}\n",
      "Losses {'ner': 34.83923491055847}\n",
      "Losses {'ner': 36.97123701600159}\n",
      "Losses {'ner': 36.97542151265729}\n",
      "Losses {'ner': 36.97542167710675}\n",
      "Losses {'ner': 0.5914272898372738}\n",
      "Losses {'ner': 0.8033754459082277}\n",
      "Losses {'ner': 2.601200761042284}\n",
      "Losses {'ner': 2.869278279555132}\n",
      "Losses {'ner': 2.8821517884741965}\n",
      "Losses {'ner': 2.897819815709083}\n",
      "Losses {'ner': 4.269979707033725}\n",
      "Losses {'ner': 8.52189896956554}\n",
      "Losses {'ner': 9.42876755398268}\n",
      "Losses {'ner': 9.429212346998264}\n",
      "Losses {'ner': 11.861835494325936}\n",
      "Losses {'ner': 11.876355087682418}\n",
      "Losses {'ner': 13.19132907728844}\n",
      "Losses {'ner': 15.188784313553498}\n",
      "Losses {'ner': 16.01625024094288}\n",
      "Losses {'ner': 18.021282791347762}\n",
      "Losses {'ner': 21.76701296803097}\n",
      "Losses {'ner': 25.872073687441905}\n",
      "Losses {'ner': 27.36038884772134}\n",
      "Losses {'ner': 28.332480603817423}\n",
      "Losses {'ner': 29.150261322169403}\n",
      "Losses {'ner': 31.255124947287623}\n",
      "Losses {'ner': 31.931598153440188}\n",
      "Losses {'ner': 32.88637266086799}\n",
      "Losses {'ner': 32.99937214031811}\n",
      "Losses {'ner': 33.02743653726217}\n",
      "Losses {'ner': 33.02783687435643}\n",
      "Losses {'ner': 0.17803605403773098}\n",
      "Losses {'ner': 0.4057990117162318}\n",
      "Losses {'ner': 0.4444461344694162}\n",
      "Losses {'ner': 0.7766056697404534}\n",
      "Losses {'ner': 0.787642129654099}\n",
      "Losses {'ner': 2.95098511912145}\n",
      "Losses {'ner': 7.131154552650405}\n",
      "Losses {'ner': 9.38898016468256}\n",
      "Losses {'ner': 12.746959207165368}\n",
      "Losses {'ner': 12.773734343163428}\n",
      "Losses {'ner': 14.967020406593369}\n",
      "Losses {'ner': 16.650247153664726}\n",
      "Losses {'ner': 19.953217518943312}\n",
      "Losses {'ner': 20.536096377498097}\n",
      "Losses {'ner': 20.973763292986824}\n",
      "Losses {'ner': 25.976566838631115}\n",
      "Losses {'ner': 27.971843640230073}\n",
      "Losses {'ner': 29.831457089775892}\n",
      "Losses {'ner': 32.2273543123436}\n",
      "Losses {'ner': 35.33535119984362}\n",
      "Losses {'ner': 36.94643890165884}\n",
      "Losses {'ner': 38.94528634983935}\n",
      "Losses {'ner': 39.542144636406846}\n",
      "Losses {'ner': 47.29353956224835}\n",
      "Losses {'ner': 50.51947843851193}\n",
      "Losses {'ner': 50.74965753017158}\n",
      "Losses {'ner': 50.812729458318934}\n",
      "Losses {'ner': 3.4976723129699114}\n",
      "Losses {'ner': 4.655126019650812}\n",
      "Losses {'ner': 6.281633671631283}\n",
      "Losses {'ner': 8.700475400253}\n",
      "Losses {'ner': 8.723235602294904}\n",
      "Losses {'ner': 10.432217854525929}\n",
      "Losses {'ner': 10.48287613447976}\n",
      "Losses {'ner': 10.498068151683903}\n",
      "Losses {'ner': 14.29902143984879}\n",
      "Losses {'ner': 14.408593125385854}\n",
      "Losses {'ner': 14.59998496421021}\n",
      "Losses {'ner': 14.705160990535502}\n",
      "Losses {'ner': 14.721762334304898}\n",
      "Losses {'ner': 19.92621046399787}\n",
      "Losses {'ner': 24.618141334864678}\n",
      "Losses {'ner': 24.61862226406769}\n",
      "Losses {'ner': 24.699221779235526}\n",
      "Losses {'ner': 24.739860800255062}\n",
      "Losses {'ner': 24.93802985986861}\n",
      "Losses {'ner': 24.992581645311663}\n",
      "Losses {'ner': 25.684572341898928}\n",
      "Losses {'ner': 26.91681777803721}\n",
      "Losses {'ner': 26.919696802438267}\n",
      "Losses {'ner': 28.941403121349484}\n",
      "Losses {'ner': 30.57119815445375}\n",
      "Losses {'ner': 30.67819024831786}\n",
      "Losses {'ner': 30.678194976742134}\n",
      "Losses {'ner': 2.393092296596882}\n",
      "Losses {'ner': 2.449680472734962}\n",
      "Losses {'ner': 6.6012279438642185}\n",
      "Losses {'ner': 6.674325087119853}\n",
      "Losses {'ner': 9.202938354689918}\n",
      "Losses {'ner': 9.231685325318901}\n",
      "Losses {'ner': 9.670384419378127}\n",
      "Losses {'ner': 9.786434194810075}\n",
      "Losses {'ner': 12.141000474439082}\n",
      "Losses {'ner': 12.144841738917373}\n",
      "Losses {'ner': 15.002320410186028}\n",
      "Losses {'ner': 16.96437663206864}\n",
      "Losses {'ner': 17.015282035010998}\n",
      "Losses {'ner': 17.05486098488843}\n",
      "Losses {'ner': 17.075839909435484}\n",
      "Losses {'ner': 17.29354179563644}\n",
      "Losses {'ner': 18.166701431995712}\n",
      "Losses {'ner': 20.83979309080148}\n",
      "Losses {'ner': 22.257206987268315}\n",
      "Losses {'ner': 22.752148006304417}\n",
      "Losses {'ner': 23.015248811338466}\n",
      "Losses {'ner': 23.01931950753714}\n",
      "Losses {'ner': 23.17099769711894}\n",
      "Losses {'ner': 23.335633835787213}\n",
      "Losses {'ner': 26.405183214573235}\n",
      "Losses {'ner': 26.40746259075468}\n",
      "Losses {'ner': 26.41045529121146}\n",
      "Losses {'ner': 0.17629408722019138}\n",
      "Losses {'ner': 0.18494298445651905}\n",
      "Losses {'ner': 0.21700189663244962}\n",
      "Losses {'ner': 0.5117560695200801}\n",
      "Losses {'ner': 0.9391967796173833}\n",
      "Losses {'ner': 2.5077427758935764}\n",
      "Losses {'ner': 3.1989114169366317}\n",
      "Losses {'ner': 4.0889397990722145}\n",
      "Losses {'ner': 6.941642066271474}\n",
      "Losses {'ner': 6.942028166646703}\n",
      "Losses {'ner': 7.966212802078022}\n",
      "Losses {'ner': 9.922713011489536}\n",
      "Losses {'ner': 10.993261038912088}\n",
      "Losses {'ner': 10.993458012955466}\n",
      "Losses {'ner': 13.063027349431772}\n",
      "Losses {'ner': 13.079054533250591}\n",
      "Losses {'ner': 13.093742657972886}\n",
      "Losses {'ner': 13.095852805301634}\n",
      "Losses {'ner': 16.892844677099724}\n",
      "Losses {'ner': 18.781155783988773}\n",
      "Losses {'ner': 21.155384685013598}\n",
      "Losses {'ner': 23.06599853528887}\n",
      "Losses {'ner': 23.066006663894367}\n",
      "Losses {'ner': 24.320826752597984}\n",
      "Losses {'ner': 27.06118502568083}\n",
      "Losses {'ner': 28.369780291294077}\n",
      "Losses {'ner': 28.39561563556669}\n",
      "Losses {'ner': 0.0007958287770356903}\n",
      "Losses {'ner': 0.9692262349098385}\n",
      "Losses {'ner': 1.1668309369544192}\n",
      "Losses {'ner': 3.789278762087543}\n",
      "Losses {'ner': 3.864164022476647}\n",
      "Losses {'ner': 3.8808544148333963}\n",
      "Losses {'ner': 3.8808595543371323}\n",
      "Losses {'ner': 4.189135076621199}\n",
      "Losses {'ner': 4.667558184142834}\n",
      "Losses {'ner': 5.460957091335943}\n",
      "Losses {'ner': 8.353494072530196}\n",
      "Losses {'ner': 8.375626486563728}\n",
      "Losses {'ner': 9.439401955732654}\n",
      "Losses {'ner': 13.166776619782922}\n",
      "Losses {'ner': 13.1667951525728}\n",
      "Losses {'ner': 13.612692077272188}\n",
      "Losses {'ner': 13.640985830740853}\n",
      "Losses {'ner': 13.649362880192117}\n",
      "Losses {'ner': 14.24015834833295}\n",
      "Losses {'ner': 16.142526855673943}\n",
      "Losses {'ner': 20.133853404056122}\n",
      "Losses {'ner': 20.196198939336004}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 20.251569032348495}\n",
      "Losses {'ner': 22.266602261148304}\n",
      "Losses {'ner': 22.2675619428055}\n",
      "Losses {'ner': 22.706751314722464}\n",
      "Losses {'ner': 22.7068608265148}\n",
      "Losses {'ner': 0.004200788279228784}\n",
      "Losses {'ner': 1.913478878019012}\n",
      "Losses {'ner': 1.9234217156132825}\n",
      "Losses {'ner': 1.9234340490769755}\n",
      "Losses {'ner': 4.280009074317389}\n",
      "Losses {'ner': 4.283749691038113}\n",
      "Losses {'ner': 5.895726794770548}\n",
      "Losses {'ner': 5.899607838024353}\n",
      "Losses {'ner': 6.151576113163074}\n",
      "Losses {'ner': 6.152244574800741}\n",
      "Losses {'ner': 6.153081096529317}\n",
      "Losses {'ner': 6.155854357066501}\n",
      "Losses {'ner': 6.2008620536508685}\n",
      "Losses {'ner': 6.200862164879982}\n",
      "Losses {'ner': 6.498307712858953}\n",
      "Losses {'ner': 6.498313415550913}\n",
      "Losses {'ner': 8.474768972942908}\n",
      "Losses {'ner': 10.207035247636162}\n",
      "Losses {'ner': 10.207097396684928}\n",
      "Losses {'ner': 10.736500020444304}\n",
      "Losses {'ner': 11.861268094668429}\n",
      "Losses {'ner': 11.861269904666342}\n",
      "Losses {'ner': 16.126309226487802}\n",
      "Losses {'ner': 17.776391967511753}\n",
      "Losses {'ner': 20.901455491019505}\n",
      "Losses {'ner': 20.90150279392828}\n",
      "Losses {'ner': 20.901507710128634}\n",
      "Losses {'ner': 0.025420174484715473}\n",
      "Losses {'ner': 0.0508458996072921}\n",
      "Losses {'ner': 0.052009473368465245}\n",
      "Losses {'ner': 0.07301757678287105}\n",
      "Losses {'ner': 0.07440558711188146}\n",
      "Losses {'ner': 0.11987010928871025}\n",
      "Losses {'ner': 1.0015191191033088}\n",
      "Losses {'ner': 4.6833416756316915}\n",
      "Losses {'ner': 4.683383711856546}\n",
      "Losses {'ner': 6.645355890817649}\n",
      "Losses {'ner': 7.918936459906993}\n",
      "Losses {'ner': 7.9841341521817}\n",
      "Losses {'ner': 7.9854604386387}\n",
      "Losses {'ner': 7.9891320601002525}\n",
      "Losses {'ner': 9.235049154484383}\n",
      "Losses {'ner': 9.24423060448044}\n",
      "Losses {'ner': 9.272371701381665}\n",
      "Losses {'ner': 11.43988441317545}\n",
      "Losses {'ner': 12.670741111787468}\n",
      "Losses {'ner': 12.671821641006835}\n",
      "Losses {'ner': 12.716573807177308}\n",
      "Losses {'ner': 12.725429185025371}\n",
      "Losses {'ner': 12.74792947215726}\n",
      "Losses {'ner': 12.748977879007972}\n",
      "Losses {'ner': 12.74919888779728}\n",
      "Losses {'ner': 12.749208840400316}\n",
      "Losses {'ner': 12.75829420508437}\n",
      "Losses {'ner': 1.699412863594219e-05}\n",
      "Losses {'ner': 1.8399803068294276}\n",
      "Losses {'ner': 1.8400470269440763}\n",
      "Losses {'ner': 1.841765720185679}\n",
      "Losses {'ner': 1.8764619032975935}\n",
      "Losses {'ner': 3.7952038700726596}\n",
      "Losses {'ner': 4.236527664704388}\n",
      "Losses {'ner': 4.257926566921808}\n",
      "Losses {'ner': 5.6613495405789624}\n",
      "Losses {'ner': 6.277187830319898}\n",
      "Losses {'ner': 6.2804201462122755}\n",
      "Losses {'ner': 6.281515082123201}\n",
      "Losses {'ner': 6.851409068938409}\n",
      "Losses {'ner': 7.070606960387263}\n",
      "Losses {'ner': 7.070624550160864}\n",
      "Losses {'ner': 8.665762175965783}\n",
      "Losses {'ner': 9.090642314404688}\n",
      "Losses {'ner': 9.090643897447064}\n",
      "Losses {'ner': 9.125251794515409}\n",
      "Losses {'ner': 9.12696747868707}\n",
      "Losses {'ner': 9.126975306613172}\n",
      "Losses {'ner': 9.380107943605624}\n",
      "Losses {'ner': 9.380978227918296}\n",
      "Losses {'ner': 9.579383385376351}\n",
      "Losses {'ner': 9.579413635948063}\n",
      "Losses {'ner': 9.579415090222577}\n",
      "Losses {'ner': 9.579415213441823}\n",
      "Losses {'ner': 3.7136247811385856}\n",
      "Losses {'ner': 5.720325289844718}\n",
      "Losses {'ner': 5.848643565234641}\n",
      "Losses {'ner': 6.391389994917743}\n",
      "Losses {'ner': 8.384781255250441}\n",
      "Losses {'ner': 8.385146778615498}\n",
      "Losses {'ner': 8.430971655560715}\n",
      "Losses {'ner': 8.442047316289427}\n",
      "Losses {'ner': 9.262557607499883}\n",
      "Losses {'ner': 11.085786176072247}\n",
      "Losses {'ner': 11.085811504187205}\n",
      "Losses {'ner': 11.186702666197737}\n",
      "Losses {'ner': 11.188204350342023}\n",
      "Losses {'ner': 11.188290513730246}\n",
      "Losses {'ner': 11.190820364793053}\n",
      "Losses {'ner': 13.161137246355691}\n",
      "Losses {'ner': 13.362362393834694}\n",
      "Losses {'ner': 16.5989381760321}\n",
      "Losses {'ner': 16.59956599416411}\n",
      "Losses {'ner': 16.661009845390062}\n",
      "Losses {'ner': 16.661022835734315}\n",
      "Losses {'ner': 16.6750971602627}\n",
      "Losses {'ner': 16.685458877016337}\n",
      "Losses {'ner': 16.685516849503266}\n",
      "Losses {'ner': 16.685527179380863}\n",
      "Losses {'ner': 18.510596473124966}\n",
      "Losses {'ner': 18.51059647502152}\n",
      "Losses {'ner': 0.0047121750192113294}\n",
      "Losses {'ner': 1.7195045238073268}\n",
      "Losses {'ner': 2.7912171226067453}\n",
      "Losses {'ner': 2.7912918857058613}\n",
      "Losses {'ner': 2.7913579643229505}\n",
      "Losses {'ner': 2.9923631426013824}\n",
      "Losses {'ner': 3.1333625703379018}\n",
      "Losses {'ner': 3.134027659777808}\n",
      "Losses {'ner': 3.1352838275311985}\n",
      "Losses {'ner': 3.135283971855657}\n",
      "Losses {'ner': 3.5151304202139384}\n",
      "Losses {'ner': 5.106088692575915}\n",
      "Losses {'ner': 7.005654671647}\n",
      "Losses {'ner': 7.0057532981329045}\n",
      "Losses {'ner': 7.01857706336595}\n",
      "Losses {'ner': 7.065051333281208}\n",
      "Losses {'ner': 7.067576101916303}\n",
      "Losses {'ner': 9.012378270130537}\n",
      "Losses {'ner': 9.013065462129454}\n",
      "Losses {'ner': 10.494868246882417}\n",
      "Losses {'ner': 12.685829858758767}\n",
      "Losses {'ner': 12.685830342737683}\n",
      "Losses {'ner': 12.690248017891774}\n",
      "Losses {'ner': 14.189940106960742}\n",
      "Losses {'ner': 16.203732649335798}\n",
      "Losses {'ner': 16.20583237244661}\n",
      "Losses {'ner': 16.205832498611638}\n",
      "Losses {'ner': 1.7280285089048728}\n",
      "Losses {'ner': 5.751110420518457}\n",
      "Losses {'ner': 5.751110783460279}\n",
      "Losses {'ner': 6.200124523432673}\n",
      "Losses {'ner': 8.198955166588343}\n",
      "Losses {'ner': 8.199411667511141}\n",
      "Losses {'ner': 8.199481839134432}\n",
      "Losses {'ner': 8.199519644609957}\n",
      "Losses {'ner': 10.600613192560258}\n",
      "Losses {'ner': 10.600650753649635}\n",
      "Losses {'ner': 10.600654985715405}\n",
      "Losses {'ner': 12.403802297738885}\n",
      "Losses {'ner': 12.403869094796432}\n",
      "Losses {'ner': 13.619607476375618}\n",
      "Losses {'ner': 13.914196638476668}\n",
      "Losses {'ner': 13.918948258156759}\n",
      "Losses {'ner': 13.922140301158286}\n",
      "Losses {'ner': 15.18276296738234}\n",
      "Losses {'ner': 19.50497534031397}\n",
      "Losses {'ner': 21.734662540284702}\n",
      "Losses {'ner': 23.909307923336154}\n",
      "Losses {'ner': 26.241849921861967}\n",
      "Losses {'ner': 29.087805479539867}\n",
      "Losses {'ner': 30.85763896700387}\n",
      "Losses {'ner': 32.10356294622568}\n",
      "Losses {'ner': 33.83139256181522}\n",
      "Losses {'ner': 33.831397140228056}\n",
      "Losses {'ner': 0.0005770958581731037}\n",
      "Losses {'ner': 0.000810518881715088}\n",
      "Losses {'ner': 0.0008108680750896028}\n",
      "Losses {'ner': 0.004286645150116081}\n",
      "Losses {'ner': 0.12413396226935038}\n",
      "Losses {'ner': 0.12413397859321416}\n",
      "Losses {'ner': 0.13041168909099723}\n",
      "Losses {'ner': 0.1305049378504063}\n",
      "Losses {'ner': 1.6109899155053624}\n",
      "Losses {'ner': 1.611022747049862}\n",
      "Losses {'ner': 1.6110343334992878}\n",
      "Losses {'ner': 1.6112540251445888}\n",
      "Losses {'ner': 3.4969600068644326}\n",
      "Losses {'ner': 3.4969602340267323}\n",
      "Losses {'ner': 3.496971630735622}\n",
      "Losses {'ner': 3.4969716807967623}\n",
      "Losses {'ner': 3.7271908180755586}\n",
      "Losses {'ner': 5.704935178184405}\n",
      "Losses {'ner': 7.336794874141268}\n",
      "Losses {'ner': 8.093317298154245}\n",
      "Losses {'ner': 8.495611649772831}\n",
      "Losses {'ner': 10.276110429373302}\n",
      "Losses {'ner': 12.871254967784004}\n",
      "Losses {'ner': 14.638259341875933}\n",
      "Losses {'ner': 16.346361789231967}\n",
      "Losses {'ner': 16.380474330804674}\n",
      "Losses {'ner': 16.380483424201515}\n",
      "Losses {'ner': 2.141716029930503e-05}\n",
      "Losses {'ner': 0.026867249337344124}\n",
      "Losses {'ner': 0.02897444101163987}\n",
      "Losses {'ner': 0.04669453741293907}\n",
      "Losses {'ner': 0.04682359032220913}\n",
      "Losses {'ner': 0.04682361620467194}\n",
      "Losses {'ner': 0.04769240312970116}\n",
      "Losses {'ner': 0.8571906700846145}\n",
      "Losses {'ner': 0.861663910272399}\n",
      "Losses {'ner': 0.9272186740385265}\n",
      "Losses {'ner': 2.088126621555478}\n",
      "Losses {'ner': 2.089124763953512}\n",
      "Losses {'ner': 2.8257049554634754}\n",
      "Losses {'ner': 2.825704983647705}\n",
      "Losses {'ner': 4.341670856802104}\n",
      "Losses {'ner': 4.341760703167252}\n",
      "Losses {'ner': 4.341760787834893}\n",
      "Losses {'ner': 5.864015399680081}\n",
      "Losses {'ner': 6.2039270504251975}\n",
      "Losses {'ner': 6.20393528501157}\n",
      "Losses {'ner': 6.204046300292771}\n",
      "Losses {'ner': 7.50119338631864}\n",
      "Losses {'ner': 8.916049140471227}\n",
      "Losses {'ner': 10.764106998484383}\n",
      "Losses {'ner': 10.770747718264793}\n",
      "Losses {'ner': 10.77075737414927}\n",
      "Losses {'ner': 10.771278641130511}\n",
      "Losses {'ner': 2.1124526720356146e-05}\n",
      "Losses {'ner': 0.0017510802633368976}\n",
      "Losses {'ner': 0.0025824956766051905}\n",
      "Losses {'ner': 0.00284841138634835}\n",
      "Losses {'ner': 1.5305376329814147}\n",
      "Losses {'ner': 3.9514883509201737}\n",
      "Losses {'ner': 3.972308058412494}\n",
      "Losses {'ner': 4.099371998039526}\n",
      "Losses {'ner': 4.105801044986455}\n",
      "Losses {'ner': 4.885404044928505}\n",
      "Losses {'ner': 4.938537252111681}\n",
      "Losses {'ner': 4.938609458529809}\n",
      "Losses {'ner': 4.9386094653765795}\n",
      "Losses {'ner': 4.9394048510244595}\n",
      "Losses {'ner': 4.9394049426359565}\n",
      "Losses {'ner': 4.939407239141136}\n",
      "Losses {'ner': 6.464419331994348}\n",
      "Losses {'ner': 6.4644208687177835}\n",
      "Losses {'ner': 6.464465559208421}\n",
      "Losses {'ner': 8.050665558237133}\n",
      "Losses {'ner': 12.78686978407092}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 12.841250021747001}\n",
      "Losses {'ner': 12.84780599189482}\n",
      "Losses {'ner': 12.870345142820305}\n",
      "Losses {'ner': 12.870345151770538}\n",
      "Losses {'ner': 12.870345441675514}\n",
      "Losses {'ner': 12.870345517516412}\n",
      "Losses {'ner': 6.905147696644177e-07}\n",
      "Losses {'ner': 0.32667487182393634}\n",
      "Losses {'ner': 1.7541310830002759}\n",
      "Losses {'ner': 1.9604469855875035}\n",
      "Losses {'ner': 1.9672032378824271}\n",
      "Losses {'ner': 1.9672088401915957}\n",
      "Losses {'ner': 1.9672088451611256}\n",
      "Losses {'ner': 1.9681734051246633}\n",
      "Losses {'ner': 1.9683957641943128}\n",
      "Losses {'ner': 1.9683994872604138}\n",
      "Losses {'ner': 1.989873309018763}\n",
      "Losses {'ner': 1.9945455871048974}\n",
      "Losses {'ner': 1.9945481657053112}\n",
      "Losses {'ner': 1.9945488343261442}\n",
      "Losses {'ner': 1.994549395934339}\n",
      "Losses {'ner': 1.998412447409922}\n",
      "Losses {'ner': 4.1201024490393845}\n",
      "Losses {'ner': 4.120105628119767}\n",
      "Losses {'ner': 4.1202470414907}\n",
      "Losses {'ner': 4.226939018371487}\n",
      "Losses {'ner': 4.236002441515573}\n",
      "Losses {'ner': 6.367812033562814}\n",
      "Losses {'ner': 9.662329127109462}\n",
      "Losses {'ner': 11.615862102558427}\n",
      "Losses {'ner': 12.624113863851413}\n",
      "Losses {'ner': 13.438378304476075}\n",
      "Losses {'ner': 13.438378305057054}\n",
      "Losses {'ner': 0.2395818735436134}\n",
      "Losses {'ner': 1.57849122290479}\n",
      "Losses {'ner': 1.5818056808959122}\n",
      "Losses {'ner': 1.583309608820586}\n",
      "Losses {'ner': 2.772502258596954}\n",
      "Losses {'ner': 2.7725033663047505}\n",
      "Losses {'ner': 2.7725036865421653}\n",
      "Losses {'ner': 2.7725040604157645}\n",
      "Losses {'ner': 2.772512671609266}\n",
      "Losses {'ner': 3.045406609052779}\n",
      "Losses {'ner': 3.0454071277669117}\n",
      "Losses {'ner': 3.5533794934770686}\n",
      "Losses {'ner': 3.5533870282770166}\n",
      "Losses {'ner': 3.5547082118848867}\n",
      "Losses {'ner': 3.6801768550863527}\n",
      "Losses {'ner': 3.6810354172820765}\n",
      "Losses {'ner': 5.814483350849847}\n",
      "Losses {'ner': 5.822292991792182}\n",
      "Losses {'ner': 5.822292995155512}\n",
      "Losses {'ner': 7.122655508820899}\n",
      "Losses {'ner': 7.123118589763064}\n",
      "Losses {'ner': 7.151908032220443}\n",
      "Losses {'ner': 7.35760669165253}\n",
      "Losses {'ner': 9.761776317956482}\n",
      "Losses {'ner': 9.761776380240775}\n",
      "Losses {'ner': 9.76177670461166}\n",
      "Losses {'ner': 9.761776708207769}\n",
      "Losses {'ner': 4.638708339213546e-06}\n",
      "Losses {'ner': 4.639424435331812e-06}\n",
      "Losses {'ner': 0.00044784311524402023}\n",
      "Losses {'ner': 1.487634403859078}\n",
      "Losses {'ner': 1.4934161014682115}\n",
      "Losses {'ner': 2.2866533911933344}\n",
      "Losses {'ner': 2.300224179355225}\n",
      "Losses {'ner': 2.3266078466329425}\n",
      "Losses {'ner': 2.326608309295811}\n",
      "Losses {'ner': 2.328410143083408}\n",
      "Losses {'ner': 4.636397143363277}\n",
      "Losses {'ner': 5.771760108231028}\n",
      "Losses {'ner': 5.7717607086109375}\n",
      "Losses {'ner': 5.772219782964585}\n",
      "Losses {'ner': 5.772220441365878}\n",
      "Losses {'ner': 5.772383078773956}\n",
      "Losses {'ner': 8.115424843378678}\n",
      "Losses {'ner': 8.116715193125716}\n",
      "Losses {'ner': 10.047059783450045}\n",
      "Losses {'ner': 10.047175995885858}\n",
      "Losses {'ner': 10.047185772827069}\n",
      "Losses {'ner': 16.719769460596012}\n",
      "Losses {'ner': 17.493322836345804}\n",
      "Losses {'ner': 17.905110315825432}\n",
      "Losses {'ner': 19.520677935066853}\n",
      "Losses {'ner': 19.70291679143913}\n",
      "Losses {'ner': 19.702917649025657}\n",
      "Losses {'ner': 0.9187894767603769}\n",
      "Losses {'ner': 0.9191305916243843}\n",
      "Losses {'ner': 5.428682554334958}\n",
      "Losses {'ner': 5.4292684981303365}\n",
      "Losses {'ner': 5.429271523242271}\n",
      "Losses {'ner': 5.4292832733039855}\n",
      "Losses {'ner': 5.429284100861023}\n",
      "Losses {'ner': 7.518127524407995}\n",
      "Losses {'ner': 7.521989006614609}\n",
      "Losses {'ner': 9.696393658868448}\n",
      "Losses {'ner': 9.696394008314277}\n",
      "Losses {'ner': 11.522413785893026}\n",
      "Losses {'ner': 14.977508237746765}\n",
      "Losses {'ner': 14.978728974418654}\n",
      "Losses {'ner': 15.015726201170922}\n",
      "Losses {'ner': 15.016277275648225}\n",
      "Losses {'ner': 15.672922918089832}\n",
      "Losses {'ner': 15.673006772002372}\n",
      "Losses {'ner': 15.678935282667783}\n",
      "Losses {'ner': 17.269720179959272}\n",
      "Losses {'ner': 17.269804491348143}\n",
      "Losses {'ner': 17.26980463538904}\n",
      "Losses {'ner': 19.269545030187984}\n",
      "Losses {'ner': 19.90802433850421}\n",
      "Losses {'ner': 19.908643589494595}\n",
      "Losses {'ner': 19.908647286643575}\n",
      "Losses {'ner': 20.44182386644452}\n",
      "Losses {'ner': 7.293658319886887e-07}\n",
      "Losses {'ner': 1.6182904430900837}\n",
      "Losses {'ner': 1.6184820793976524}\n",
      "Losses {'ner': 1.61888004109808}\n",
      "Losses {'ner': 1.6189462553597542}\n",
      "Losses {'ner': 3.1301349821655386}\n",
      "Losses {'ner': 3.4169451600569465}\n",
      "Losses {'ner': 3.929366624138815}\n",
      "Losses {'ner': 3.9296218764442856}\n",
      "Losses {'ner': 3.9528122759504476}\n",
      "Losses {'ner': 3.95281394258838}\n",
      "Losses {'ner': 3.9528172081443396}\n",
      "Losses {'ner': 3.9528182390578057}\n",
      "Losses {'ner': 5.714274913001398}\n",
      "Losses {'ner': 5.714306489026546}\n",
      "Losses {'ner': 7.713546121412851}\n",
      "Losses {'ner': 7.71354616315289}\n",
      "Losses {'ner': 7.844915620054116}\n",
      "Losses {'ner': 7.8451431346427345}\n",
      "Losses {'ner': 7.853224032228118}\n",
      "Losses {'ner': 7.853262201430544}\n",
      "Losses {'ner': 7.853381742630632}\n",
      "Losses {'ner': 7.853718584531685}\n",
      "Losses {'ner': 7.853718624919894}\n",
      "Losses {'ner': 7.853741046846774}\n",
      "Losses {'ner': 9.069535247052247}\n",
      "Losses {'ner': 9.069538934977457}\n",
      "Losses {'ner': 9.773495129584845e-05}\n",
      "Losses {'ner': 9.778371815851454e-05}\n",
      "Losses {'ner': 1.1103026502724596}\n",
      "Losses {'ner': 1.110302678387717}\n",
      "Losses {'ner': 1.1103110056335723}\n",
      "Losses {'ner': 1.11159401009403}\n",
      "Losses {'ner': 1.111594011513393}\n",
      "Losses {'ner': 1.1116072307472185}\n",
      "Losses {'ner': 1.1300538645284897}\n",
      "Losses {'ner': 1.4641683878417904}\n",
      "Losses {'ner': 1.4684553200206802}\n",
      "Losses {'ner': 1.5351206092378056}\n",
      "Losses {'ner': 1.5352428292416926}\n",
      "Losses {'ner': 2.2343229324295995}\n",
      "Losses {'ner': 2.2393384250325896}\n",
      "Losses {'ner': 2.2393485620466764}\n",
      "Losses {'ner': 2.2393495198230235}\n",
      "Losses {'ner': 2.268840317441486}\n",
      "Losses {'ner': 2.2696379656856003}\n",
      "Losses {'ner': 2.2706561807999726}\n",
      "Losses {'ner': 2.326238196576662}\n",
      "Losses {'ner': 2.3272939008548463}\n",
      "Losses {'ner': 2.3290526724471086}\n",
      "Losses {'ner': 2.329052675407967}\n",
      "Losses {'ner': 2.3296407734755307}\n",
      "Losses {'ner': 2.53016829565571}\n",
      "Losses {'ner': 2.530168295655719}\n",
      "Losses {'ner': 0.2855424046427612}\n",
      "Losses {'ner': 0.3865936319979657}\n",
      "Losses {'ner': 2.4024990406823425}\n",
      "Losses {'ner': 2.408291381498781}\n",
      "Losses {'ner': 2.4083055254304164}\n",
      "Losses {'ner': 2.408307076246957}\n",
      "Losses {'ner': 2.408307084393382}\n",
      "Losses {'ner': 4.3954545239512335}\n",
      "Losses {'ner': 4.398251395705297}\n",
      "Losses {'ner': 4.398251438730002}\n",
      "Losses {'ner': 4.398399726323649}\n",
      "Losses {'ner': 4.398399727434685}\n",
      "Losses {'ner': 5.816481142377311}\n",
      "Losses {'ner': 5.941915936875185}\n",
      "Losses {'ner': 5.941920468429358}\n",
      "Losses {'ner': 6.6277109060025206}\n",
      "Losses {'ner': 6.6282139345967135}\n",
      "Losses {'ner': 6.628619044844214}\n",
      "Losses {'ner': 6.628619827039956}\n",
      "Losses {'ner': 6.643191331034516}\n",
      "Losses {'ner': 6.805366553971098}\n",
      "Losses {'ner': 8.21284107070739}\n",
      "Losses {'ner': 9.009339073328515}\n",
      "Losses {'ner': 9.048619544554468}\n",
      "Losses {'ner': 9.051307108347242}\n",
      "Losses {'ner': 9.051316407599685}\n",
      "Losses {'ner': 9.053459298101906}\n",
      "Losses {'ner': 0.11814606242241416}\n",
      "Losses {'ner': 0.1618834628114555}\n",
      "Losses {'ner': 0.16188787926346965}\n",
      "Losses {'ner': 0.17094586481923635}\n",
      "Losses {'ner': 0.17102360862925417}\n",
      "Losses {'ner': 0.17102780897379213}\n",
      "Losses {'ner': 0.1710303336642532}\n",
      "Losses {'ner': 0.17103056994086618}\n",
      "Losses {'ner': 0.17103094781065553}\n",
      "Losses {'ner': 0.17118030764630676}\n",
      "Losses {'ner': 2.196721063324866}\n",
      "Losses {'ner': 2.196739886298967}\n",
      "Losses {'ner': 2.1967399113521524}\n",
      "Losses {'ner': 2.4362305793502124}\n",
      "Losses {'ner': 2.4363149258790684}\n",
      "Losses {'ner': 2.43631558961756}\n",
      "Losses {'ner': 4.008598519908458}\n",
      "Losses {'ner': 5.728505629210164}\n",
      "Losses {'ner': 5.728510714914103}\n",
      "Losses {'ner': 5.7285410867783595}\n",
      "Losses {'ner': 5.728541553212577}\n",
      "Losses {'ner': 5.728541643191823}\n",
      "Losses {'ner': 5.754141647752157}\n",
      "Losses {'ner': 7.333449045008807}\n",
      "Losses {'ner': 7.418293453218129}\n",
      "Losses {'ner': 7.4182935668823555}\n",
      "Losses {'ner': 7.422924946971758}\n",
      "Losses {'ner': 0.023782685817850015}\n",
      "Losses {'ner': 0.023782904237568007}\n",
      "Losses {'ner': 1.6158429844793691}\n",
      "Losses {'ner': 1.615844236673296}\n",
      "Losses {'ner': 1.6158446129011033}\n",
      "Losses {'ner': 1.6183425327759522}\n",
      "Losses {'ner': 1.6253059461949837}\n",
      "Losses {'ner': 1.6262534224241452}\n",
      "Losses {'ner': 1.6262536322281607}\n",
      "Losses {'ner': 1.8533078008210824}\n",
      "Losses {'ner': 1.8533079385367976}\n",
      "Losses {'ner': 2.8248827481747916}\n",
      "Losses {'ner': 2.8260713176064525}\n",
      "Losses {'ner': 2.826071321092898}\n",
      "Losses {'ner': 3.173270499271949}\n",
      "Losses {'ner': 3.1732721189905977}\n",
      "Losses {'ner': 3.173272148244544}\n",
      "Losses {'ner': 3.1732756367247577}\n",
      "Losses {'ner': 3.173275696328425}\n",
      "Losses {'ner': 3.2040801663621186}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3.204081343451203}\n",
      "Losses {'ner': 5.188544607653236}\n",
      "Losses {'ner': 5.188878084928048}\n",
      "Losses {'ner': 5.188878086755618}\n",
      "Losses {'ner': 5.188878433016058}\n",
      "Losses {'ner': 5.188878474429021}\n",
      "Losses {'ner': 5.188878483486412}\n",
      "Losses {'ner': 0.0038357253661067737}\n",
      "Losses {'ner': 0.0038358306885117512}\n",
      "Losses {'ner': 0.008392262221401466}\n",
      "Losses {'ner': 0.011592893564797195}\n",
      "Losses {'ner': 0.011595269610220066}\n",
      "Losses {'ner': 0.011962185456313789}\n",
      "Losses {'ner': 0.18110482960992302}\n",
      "Losses {'ner': 1.9010306900106082}\n",
      "Losses {'ner': 1.9072132598951619}\n",
      "Losses {'ner': 2.0625530151549762}\n",
      "Losses {'ner': 2.062561191951586}\n",
      "Losses {'ner': 4.517439080547922}\n",
      "Losses {'ner': 4.517439167646839}\n",
      "Losses {'ner': 4.518383768492361}\n",
      "Losses {'ner': 4.518383801451549}\n",
      "Losses {'ner': 6.227183242641842}\n",
      "Losses {'ner': 6.41506006677747}\n",
      "Losses {'ner': 7.651276612781393}\n",
      "Losses {'ner': 7.675030557877978}\n",
      "Losses {'ner': 7.781995554377545}\n",
      "Losses {'ner': 7.781997817062349}\n",
      "Losses {'ner': 7.782013773205101}\n",
      "Losses {'ner': 7.788463027716124}\n",
      "Losses {'ner': 7.788463750717704}\n",
      "Losses {'ner': 7.788540735062933}\n",
      "Losses {'ner': 8.699666852353605}\n",
      "Losses {'ner': 8.699666852658588}\n",
      "Losses {'ner': 0.13669807875682485}\n",
      "Losses {'ner': 0.13716454252798352}\n",
      "Losses {'ner': 2.1439947313419787}\n",
      "Losses {'ner': 2.1439947383226508}\n",
      "Losses {'ner': 3.8420638945653542}\n",
      "Losses {'ner': 5.707129374584833}\n",
      "Losses {'ner': 5.7080508180001255}\n",
      "Losses {'ner': 7.055830208935333}\n",
      "Losses {'ner': 7.366891133431978}\n",
      "Losses {'ner': 7.501896509949854}\n",
      "Losses {'ner': 7.501896559601546}\n",
      "Losses {'ner': 7.501901839537105}\n",
      "Losses {'ner': 7.99439815209587}\n",
      "Losses {'ner': 8.03200625662929}\n",
      "Losses {'ner': 8.791503331778262}\n",
      "Losses {'ner': 8.791503587651569}\n",
      "Losses {'ner': 10.720412058087476}\n",
      "Losses {'ner': 12.748524226585076}\n",
      "Losses {'ner': 12.762374200614685}\n",
      "Losses {'ner': 12.762384649677331}\n",
      "Losses {'ner': 12.796638104282781}\n",
      "Losses {'ner': 12.800842558231489}\n",
      "Losses {'ner': 12.800842559164675}\n",
      "Losses {'ner': 12.800842784351499}\n",
      "Losses {'ner': 12.801038340413882}\n",
      "Losses {'ner': 13.220778165439569}\n",
      "Losses {'ner': 13.220778165459933}\n",
      "Losses {'ner': 1.0470628746014508}\n",
      "Losses {'ner': 1.0470629263707547}\n",
      "Losses {'ner': 1.0477339233492902}\n",
      "Losses {'ner': 1.049594621996598}\n",
      "Losses {'ner': 1.0616002713097963}\n",
      "Losses {'ner': 1.0616270968387878}\n",
      "Losses {'ner': 1.0616301632984662}\n",
      "Losses {'ner': 1.0616305060257156}\n",
      "Losses {'ner': 1.0618457652787674}\n",
      "Losses {'ner': 1.0809635619279359}\n",
      "Losses {'ner': 1.0811026787932623}\n",
      "Losses {'ner': 1.0811064833772153}\n",
      "Losses {'ner': 1.0811065857512552}\n",
      "Losses {'ner': 5.23280943151341}\n",
      "Losses {'ner': 5.233175194897452}\n",
      "Losses {'ner': 5.233175447813951}\n",
      "Losses {'ner': 5.233225722214178}\n",
      "Losses {'ner': 5.2332648853823995}\n",
      "Losses {'ner': 5.233264885519616}\n",
      "Losses {'ner': 5.916815093068149}\n",
      "Losses {'ner': 5.917188324561854}\n",
      "Losses {'ner': 6.096876772703263}\n",
      "Losses {'ner': 6.096878606983725}\n",
      "Losses {'ner': 6.096879075647481}\n",
      "Losses {'ner': 6.276392728620102}\n",
      "Losses {'ner': 8.276797155405108}\n",
      "Losses {'ner': 8.276798086532027}\n",
      "Losses {'ner': 0.06306183740521867}\n",
      "Losses {'ner': 0.06306218009816203}\n",
      "Losses {'ner': 0.3245520989425199}\n",
      "Losses {'ner': 0.32455209919473976}\n",
      "Losses {'ner': 0.32459912444035843}\n",
      "Losses {'ner': 2.303508696556935}\n",
      "Losses {'ner': 2.303524107567661}\n",
      "Losses {'ner': 2.3035368803349523}\n",
      "Losses {'ner': 2.303552189230338}\n",
      "Losses {'ner': 2.3036427058608093}\n",
      "Losses {'ner': 4.203091111065029}\n",
      "Losses {'ner': 4.2032036226860905}\n",
      "Losses {'ner': 4.203207265421268}\n",
      "Losses {'ner': 4.203207265513579}\n",
      "Losses {'ner': 6.893434398442354}\n",
      "Losses {'ner': 6.8934343985055735}\n",
      "Losses {'ner': 6.893526758889519}\n",
      "Losses {'ner': 6.893526759529351}\n",
      "Losses {'ner': 6.893532537687339}\n",
      "Losses {'ner': 6.893532540543575}\n",
      "Losses {'ner': 6.8935325612157445}\n",
      "Losses {'ner': 6.893532580249231}\n",
      "Losses {'ner': 6.893532900216315}\n",
      "Losses {'ner': 8.435486225752621}\n",
      "Losses {'ner': 8.435486435705664}\n",
      "Losses {'ner': 8.435486440570667}\n",
      "Losses {'ner': 8.435486440587011}\n",
      "Losses {'ner': 0.6174014129128098}\n",
      "Losses {'ner': 0.6683069659220257}\n",
      "Losses {'ner': 0.6683098717310016}\n",
      "Losses {'ner': 0.6683147436034758}\n",
      "Losses {'ner': 0.6683282629192112}\n",
      "Losses {'ner': 0.6683283174402248}\n",
      "Losses {'ner': 0.6683283176799051}\n",
      "Losses {'ner': 0.6802825870181131}\n",
      "Losses {'ner': 0.6802831891050407}\n",
      "Losses {'ner': 0.6802852663240971}\n",
      "Losses {'ner': 0.6802853526642932}\n",
      "Losses {'ner': 0.6807509751668515}\n",
      "Losses {'ner': 0.6807765246122824}\n",
      "Losses {'ner': 0.6807873974064256}\n",
      "Losses {'ner': 0.7296650985874394}\n",
      "Losses {'ner': 0.7296651000979834}\n",
      "Losses {'ner': 0.7296669062126807}\n",
      "Losses {'ner': 0.8290669156966194}\n",
      "Losses {'ner': 0.8290670110025643}\n",
      "Losses {'ner': 0.8290735879518599}\n",
      "Losses {'ner': 0.8290773442309896}\n",
      "Losses {'ner': 0.8290773700306543}\n",
      "Losses {'ner': 0.866252565889965}\n",
      "Losses {'ner': 0.8687533359721109}\n",
      "Losses {'ner': 0.8687958330462349}\n",
      "Losses {'ner': 0.8687961343862772}\n",
      "Losses {'ner': 0.8687998831518698}\n",
      "Losses {'ner': 1.7458163949366198e-07}\n",
      "Losses {'ner': 2.2098389379213584e-07}\n",
      "Losses {'ner': 1.1480784360717632e-06}\n",
      "Losses {'ner': 0.05395805164218346}\n",
      "Losses {'ner': 0.8046376700471336}\n",
      "Losses {'ner': 0.8046376700819348}\n",
      "Losses {'ner': 1.102625253735869}\n",
      "Losses {'ner': 1.102627007289373}\n",
      "Losses {'ner': 1.1026270774485532}\n",
      "Losses {'ner': 1.1034562234890568}\n",
      "Losses {'ner': 1.1034569144652993}\n",
      "Losses {'ner': 2.0129175477318304}\n",
      "Losses {'ner': 2.0129176477119786}\n",
      "Losses {'ner': 2.8212303888380523}\n",
      "Losses {'ner': 2.821231335411694}\n",
      "Losses {'ner': 2.821267423387234}\n",
      "Losses {'ner': 3.128066781088113}\n",
      "Losses {'ner': 3.128278080402713}\n",
      "Losses {'ner': 3.128283275559031}\n",
      "Losses {'ner': 3.1283026232781794}\n",
      "Losses {'ner': 3.8695160752019953}\n",
      "Losses {'ner': 3.869516082451523}\n",
      "Losses {'ner': 5.749474636325505}\n",
      "Losses {'ner': 7.089081681458267}\n",
      "Losses {'ner': 7.116032294864683}\n",
      "Losses {'ner': 7.43283428036304}\n",
      "Losses {'ner': 7.432834280363047}\n",
      "Losses {'ner': 1.1508860625137522e-05}\n",
      "Losses {'ner': 1.160554710525638e-05}\n",
      "Losses {'ner': 2.3796778159288736e-05}\n",
      "Losses {'ner': 0.04872788890077038}\n",
      "Losses {'ner': 0.048762630519355546}\n",
      "Losses {'ner': 0.04876658155766538}\n",
      "Losses {'ner': 0.048790335883358085}\n",
      "Losses {'ner': 1.1942989678794294}\n",
      "Losses {'ner': 1.1943647003253848}\n",
      "Losses {'ner': 1.4614904208853448}\n",
      "Losses {'ner': 1.4616533337672832}\n",
      "Losses {'ner': 1.4677257491959992}\n",
      "Losses {'ner': 1.4677514647749876}\n",
      "Losses {'ner': 1.4678812357139865}\n",
      "Losses {'ner': 1.4684891737444765}\n",
      "Losses {'ner': 1.4684891852865098}\n",
      "Losses {'ner': 1.469829958511536}\n",
      "Losses {'ner': 1.4719511867170387}\n",
      "Losses {'ner': 1.471951294065501}\n",
      "Losses {'ner': 2.1722452948526203}\n",
      "Losses {'ner': 4.080604130535929}\n",
      "Losses {'ner': 4.0806041309426835}\n",
      "Losses {'ner': 4.080604591894927}\n",
      "Losses {'ner': 4.080604591897436}\n",
      "Losses {'ner': 4.080604603187559}\n",
      "Losses {'ner': 4.084480994282278}\n",
      "Losses {'ner': 4.084480994286368}\n",
      "Losses {'ner': 0.06627173880980418}\n",
      "Losses {'ner': 0.06627608271262922}\n",
      "Losses {'ner': 0.06627644306444658}\n",
      "Losses {'ner': 0.06627655615248422}\n",
      "Losses {'ner': 0.06627665820757668}\n",
      "Losses {'ner': 0.06629007441709941}\n",
      "Losses {'ner': 0.23435791141427567}\n",
      "Losses {'ner': 0.23435803217349033}\n",
      "Losses {'ner': 0.2355953964149814}\n",
      "Losses {'ner': 0.2367309168168898}\n",
      "Losses {'ner': 0.23673601080108353}\n",
      "Losses {'ner': 0.23673601263221844}\n",
      "Losses {'ner': 0.23673602253436107}\n",
      "Losses {'ner': 0.23676597211862294}\n",
      "Losses {'ner': 0.23677159328061154}\n",
      "Losses {'ner': 0.23677171028488767}\n",
      "Losses {'ner': 0.2367861248337617}\n",
      "Losses {'ner': 0.23678643678002004}\n",
      "Losses {'ner': 0.23679002184129072}\n",
      "Losses {'ner': 0.23722969298523208}\n",
      "Losses {'ner': 0.23722974220576626}\n",
      "Losses {'ner': 0.23723620434341935}\n",
      "Losses {'ner': 0.2372629376760259}\n",
      "Losses {'ner': 0.2427059285161008}\n",
      "Losses {'ner': 0.24823629568916245}\n",
      "Losses {'ner': 2.246329024370618}\n",
      "Losses {'ner': 2.2463385239263727}\n",
      "Losses {'ner': 0.0005173305101619813}\n",
      "Losses {'ner': 0.09808549632626506}\n",
      "Losses {'ner': 0.10597447714625495}\n",
      "Losses {'ner': 0.10597451420036888}\n",
      "Losses {'ner': 0.2101585975539324}\n",
      "Losses {'ner': 0.21015905574742363}\n",
      "Losses {'ner': 2.210097403852732}\n",
      "Losses {'ner': 2.2100974040336854}\n",
      "Losses {'ner': 2.21009797721352}\n",
      "Losses {'ner': 2.210097977813589}\n",
      "Losses {'ner': 2.210387718172888}\n",
      "Losses {'ner': 2.2103877228601}\n",
      "Losses {'ner': 2.2104061167452445}\n",
      "Losses {'ner': 2.2104061554407703}\n",
      "Losses {'ner': 2.210406705007921}\n",
      "Losses {'ner': 2.210406730142918}\n",
      "Losses {'ner': 2.2104067301471844}\n",
      "Losses {'ner': 2.2104595975193804}\n",
      "Losses {'ner': 3.23063335761313}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 5.254884411036806}\n",
      "Losses {'ner': 5.25491897177432}\n",
      "Losses {'ner': 5.2673290719084305}\n",
      "Losses {'ner': 6.057574878033074}\n",
      "Losses {'ner': 7.477402261059277}\n",
      "Losses {'ner': 7.477409328841381}\n",
      "Losses {'ner': 8.553914064798628}\n",
      "Losses {'ner': 8.553914064826229}\n",
      "Losses {'ner': 0.07208022355202374}\n",
      "Losses {'ner': 0.07208691162033183}\n",
      "Losses {'ner': 0.11672061343672892}\n",
      "Losses {'ner': 0.14608675708222274}\n",
      "Losses {'ner': 0.14609249324962184}\n",
      "Losses {'ner': 2.145247911788091}\n",
      "Losses {'ner': 2.1452479117905896}\n",
      "Losses {'ner': 2.147986172578822}\n",
      "Losses {'ner': 2.1481659998977864}\n",
      "Losses {'ner': 2.148510814049556}\n",
      "Losses {'ner': 2.1485187082091963}\n",
      "Losses {'ner': 2.2240491407323666}\n",
      "Losses {'ner': 2.335518372803431}\n",
      "Losses {'ner': 2.3355261426003335}\n",
      "Losses {'ner': 3.0827930439751317}\n",
      "Losses {'ner': 3.8281649668154336}\n",
      "Losses {'ner': 5.4979332519143815}\n",
      "Losses {'ner': 5.498611183393191}\n",
      "Losses {'ner': 5.504814890411906}\n",
      "Losses {'ner': 5.504954367690832}\n",
      "Losses {'ner': 5.50495436834481}\n",
      "Losses {'ner': 5.5049984696946535}\n",
      "Losses {'ner': 5.504998470090951}\n",
      "Losses {'ner': 5.5054035319607175}\n",
      "Losses {'ner': 5.505403531969174}\n",
      "Losses {'ner': 5.505586920225104}\n",
      "Losses {'ner': 5.526297766634306}\n",
      "Losses {'ner': 2.429125582176213e-08}\n",
      "Losses {'ner': 8.340923736717691e-05}\n",
      "Losses {'ner': 0.044394176032582416}\n",
      "Losses {'ner': 0.044395061790373655}\n",
      "Losses {'ner': 0.04441676742954588}\n",
      "Losses {'ner': 0.044573773466072344}\n",
      "Losses {'ner': 0.04987754613505672}\n",
      "Losses {'ner': 0.04987754675232163}\n",
      "Losses {'ner': 0.049903955613059645}\n",
      "Losses {'ner': 1.1043347684426674}\n",
      "Losses {'ner': 1.1043347958927647}\n",
      "Losses {'ner': 1.1043453215225327}\n",
      "Losses {'ner': 1.1043453503913485}\n",
      "Losses {'ner': 1.1045669750319689}\n",
      "Losses {'ner': 1.3714542719128748}\n",
      "Losses {'ner': 3.045808024792859}\n",
      "Losses {'ner': 3.0458790085439005}\n",
      "Losses {'ner': 3.045879008624411}\n",
      "Losses {'ner': 3.045879062630287}\n",
      "Losses {'ner': 3.0458791059985155}\n",
      "Losses {'ner': 5.414989839926665}\n",
      "Losses {'ner': 5.414989909511592}\n",
      "Losses {'ner': 5.415634781020154}\n",
      "Losses {'ner': 7.409700369913639}\n",
      "Losses {'ner': 7.409703696131568}\n",
      "Losses {'ner': 7.409703851660877}\n",
      "Losses {'ner': 7.409703851661589}\n",
      "Losses {'ner': 3.31926083256173e-07}\n",
      "Losses {'ner': 1.9228719324280434}\n",
      "Losses {'ner': 1.9228726959413458}\n",
      "Losses {'ner': 1.923093438564338}\n",
      "Losses {'ner': 1.924424205824137}\n",
      "Losses {'ner': 1.924424206172175}\n",
      "Losses {'ner': 1.9250289431711396}\n",
      "Losses {'ner': 1.9250289854494074}\n",
      "Losses {'ner': 1.925037180178679}\n",
      "Losses {'ner': 1.9256130014045951}\n",
      "Losses {'ner': 1.9256130014602204}\n",
      "Losses {'ner': 1.9256136497615575}\n",
      "Losses {'ner': 1.9256145274107614}\n",
      "Losses {'ner': 1.9256218528745943}\n",
      "Losses {'ner': 3.7031403636948763}\n",
      "Losses {'ner': 3.703140420155713}\n",
      "Losses {'ner': 3.7031584818934946}\n",
      "Losses {'ner': 5.646439253619867}\n",
      "Losses {'ner': 5.646746764685806}\n",
      "Losses {'ner': 5.646749634671981}\n",
      "Losses {'ner': 5.646749664905579}\n",
      "Losses {'ner': 7.570285657209974}\n",
      "Losses {'ner': 7.5702856582673705}\n",
      "Losses {'ner': 7.570285658279387}\n",
      "Losses {'ner': 7.570350773308737}\n",
      "Losses {'ner': 7.570351235173681}\n",
      "Losses {'ner': 7.570351235173681}\n",
      "Losses {'ner': 1.8262227442814812e-09}\n",
      "Losses {'ner': 1.748953766817139e-08}\n",
      "Losses {'ner': 4.208173788355106e-08}\n",
      "Losses {'ner': 0.004387642943376177}\n",
      "Losses {'ner': 0.004387716157567852}\n",
      "Losses {'ner': 0.004387716600745005}\n",
      "Losses {'ner': 0.005696627645478387}\n",
      "Losses {'ner': 0.019105338421826802}\n",
      "Losses {'ner': 0.019163796495374502}\n",
      "Losses {'ner': 0.01916397030431843}\n",
      "Losses {'ner': 0.019163987785928323}\n",
      "Losses {'ner': 1.9267825071538356}\n",
      "Losses {'ner': 2.9618617858342904}\n",
      "Losses {'ner': 2.9618620678650607}\n",
      "Losses {'ner': 2.96186206787497}\n",
      "Losses {'ner': 4.961811255118159}\n",
      "Losses {'ner': 4.972983520691767}\n",
      "Losses {'ner': 4.972983521054338}\n",
      "Losses {'ner': 4.972983522441272}\n",
      "Losses {'ner': 4.972983522923211}\n",
      "Losses {'ner': 5.9058842342221345}\n",
      "Losses {'ner': 5.954242637528278}\n",
      "Losses {'ner': 6.008695147752672}\n",
      "Losses {'ner': 6.053805492201904}\n",
      "Losses {'ner': 7.452045598004167}\n",
      "Losses {'ner': 7.452267922204954}\n",
      "Losses {'ner': 7.452267922431029}\n",
      "Losses {'ner': 1.209964902603275e-07}\n",
      "Losses {'ner': 0.00036062275783852333}\n",
      "Losses {'ner': 0.00037824884856907615}\n",
      "Losses {'ner': 0.004327821496028255}\n",
      "Losses {'ner': 0.00432788171275683}\n",
      "Losses {'ner': 0.005937469217132769}\n",
      "Losses {'ner': 4.20221482579771}\n",
      "Losses {'ner': 4.202214825919982}\n",
      "Losses {'ner': 4.2022148326602915}\n",
      "Losses {'ner': 4.202222912331661}\n",
      "Losses {'ner': 4.202226939263813}\n",
      "Losses {'ner': 4.202231792827117}\n",
      "Losses {'ner': 4.273845209819801}\n",
      "Losses {'ner': 4.273845423002386}\n",
      "Losses {'ner': 4.273851094720871}\n",
      "Losses {'ner': 4.273851096751768}\n",
      "Losses {'ner': 4.2738511016559455}\n",
      "Losses {'ner': 4.273851138887596}\n",
      "Losses {'ner': 4.276133172919496}\n",
      "Losses {'ner': 4.2761346588989}\n",
      "Losses {'ner': 4.276134690758314}\n",
      "Losses {'ner': 4.2762847381363}\n",
      "Losses {'ner': 4.3261642633086606}\n",
      "Losses {'ner': 6.297033877465756}\n",
      "Losses {'ner': 7.220116334177992}\n",
      "Losses {'ner': 7.220127445759043}\n",
      "Losses {'ner': 7.2201274457599975}\n",
      "Losses {'ner': 0.00012947875378483163}\n",
      "Losses {'ner': 0.00012947910319355317}\n",
      "Losses {'ner': 0.00013005545642009412}\n",
      "Losses {'ner': 0.0001300600602266876}\n",
      "Losses {'ner': 0.00013006029603805454}\n",
      "Losses {'ner': 0.0001300636263660881}\n",
      "Losses {'ner': 0.00013007398556704508}\n",
      "Losses {'ner': 0.00013018578166025104}\n",
      "Losses {'ner': 1.9919000998246852}\n",
      "Losses {'ner': 3.960096669106459}\n",
      "Losses {'ner': 5.948439787911928}\n",
      "Losses {'ner': 7.7809353767213025}\n",
      "Losses {'ner': 8.121128168680768}\n",
      "Losses {'ner': 8.121128169930536}\n",
      "Losses {'ner': 8.121128170216679}\n",
      "Losses {'ner': 8.12112820741238}\n",
      "Losses {'ner': 8.28916951978171}\n",
      "Losses {'ner': 9.437560914740267}\n",
      "Losses {'ner': 13.236186233330757}\n",
      "Losses {'ner': 13.23618649490259}\n",
      "Losses {'ner': 13.269898966296442}\n",
      "Losses {'ner': 13.542972835080505}\n",
      "Losses {'ner': 15.542303831846407}\n",
      "Losses {'ner': 15.54230383257792}\n",
      "Losses {'ner': 15.54231863467988}\n",
      "Losses {'ner': 15.542318634698486}\n",
      "Losses {'ner': 15.542318634920328}\n",
      "Losses {'ner': 1.7836044639288549}\n",
      "Losses {'ner': 1.7836044646566327}\n",
      "Losses {'ner': 4.010294470538666}\n",
      "Losses {'ner': 6.236634175931322}\n",
      "Losses {'ner': 6.236634192578328}\n",
      "Losses {'ner': 8.019426802314205}\n",
      "Losses {'ner': 8.01944669193717}\n",
      "Losses {'ner': 8.019447655737657}\n",
      "Losses {'ner': 8.01944766353103}\n",
      "Losses {'ner': 8.019447869297597}\n",
      "Losses {'ner': 8.062726311993734}\n",
      "Losses {'ner': 10.364462488429824}\n",
      "Losses {'ner': 10.364462488886184}\n",
      "Losses {'ner': 10.696601219495314}\n",
      "Losses {'ner': 10.696601220178184}\n",
      "Losses {'ner': 10.69663643492751}\n",
      "Losses {'ner': 10.696636459311227}\n",
      "Losses {'ner': 10.698990819202768}\n",
      "Losses {'ner': 12.698882283913008}\n",
      "Losses {'ner': 12.699339062069456}\n",
      "Losses {'ner': 12.69972580674143}\n",
      "Losses {'ner': 13.923486076459954}\n",
      "Losses {'ner': 14.040399101813232}\n",
      "Losses {'ner': 14.040399101867312}\n",
      "Losses {'ner': 14.040399242783733}\n",
      "Losses {'ner': 14.047221028148927}\n",
      "Losses {'ner': 14.047221028277882}\n",
      "Losses {'ner': 4.396672290097479e-08}\n",
      "Losses {'ner': 7.362043895747476e-08}\n",
      "Losses {'ner': 0.00018305097461313708}\n",
      "Losses {'ner': 2.508146392259314}\n",
      "Losses {'ner': 2.508146392289029}\n",
      "Losses {'ner': 2.5081632063427692}\n",
      "Losses {'ner': 2.620365144324309}\n",
      "Losses {'ner': 2.620367258527347}\n",
      "Losses {'ner': 2.620367667928852}\n",
      "Losses {'ner': 3.3787569776955353}\n",
      "Losses {'ner': 3.5255003634840696}\n",
      "Losses {'ner': 3.5255015734334973}\n",
      "Losses {'ner': 3.525501592942774}\n",
      "Losses {'ner': 3.548251452469155}\n",
      "Losses {'ner': 3.548251718704276}\n",
      "Losses {'ner': 3.5482518227039295}\n",
      "Losses {'ner': 3.555343979588785}\n",
      "Losses {'ner': 4.073092216724811}\n",
      "Losses {'ner': 4.073216230835087}\n",
      "Losses {'ner': 5.532393109612211}\n",
      "Losses {'ner': 5.5323934011951765}\n",
      "Losses {'ner': 5.532504564607387}\n",
      "Losses {'ner': 7.338654982846966}\n",
      "Losses {'ner': 7.338658373239365}\n",
      "Losses {'ner': 7.347408106491007}\n",
      "Losses {'ner': 7.348914813951288}\n",
      "Losses {'ner': 7.348914813951394}\n",
      "Losses {'ner': 2.6995656851489543e-05}\n",
      "Losses {'ner': 2.711284427340018e-05}\n",
      "Losses {'ner': 2.711487773841165e-05}\n",
      "Losses {'ner': 2.712570892591602e-05}\n",
      "Losses {'ner': 0.7410054406487555}\n",
      "Losses {'ner': 0.7410172959906}\n",
      "Losses {'ner': 2.7194709566134985}\n",
      "Losses {'ner': 2.7196567138683783}\n",
      "Losses {'ner': 2.9105174973650456}\n",
      "Losses {'ner': 2.910686638650839}\n",
      "Losses {'ner': 2.910693208021344}\n",
      "Losses {'ner': 2.9106933914434303}\n",
      "Losses {'ner': 3.127551429552959}\n",
      "Losses {'ner': 3.127551437845005}\n",
      "Losses {'ner': 3.8218051219769937}\n",
      "Losses {'ner': 4.05441373979734}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 4.650751426890973}\n",
      "Losses {'ner': 4.650751435572345}\n",
      "Losses {'ner': 4.650800109381115}\n",
      "Losses {'ner': 4.652084705368814}\n",
      "Losses {'ner': 4.652087195441463}\n",
      "Losses {'ner': 4.652087401381336}\n",
      "Losses {'ner': 4.652087521645617}\n",
      "Losses {'ner': 5.497266935810818}\n",
      "Losses {'ner': 5.497267210675323}\n",
      "Losses {'ner': 5.497267211552402}\n",
      "Losses {'ner': 5.497267211552402}\n",
      "Losses {'ner': 1.9681317775631817}\n",
      "Losses {'ner': 1.9681317775898057}\n",
      "Losses {'ner': 1.9682041818546046}\n",
      "Losses {'ner': 1.9690774898690642}\n",
      "Losses {'ner': 1.9692584913371896}\n",
      "Losses {'ner': 1.9692807500053726}\n",
      "Losses {'ner': 2.0875019454027006}\n",
      "Losses {'ner': 4.0327288339332785}\n",
      "Losses {'ner': 4.032890573429111}\n",
      "Losses {'ner': 4.032890573488066}\n",
      "Losses {'ner': 4.032892502092044}\n",
      "Losses {'ner': 4.363856710615418}\n",
      "Losses {'ner': 4.363857148736426}\n",
      "Losses {'ner': 4.3638571516721285}\n",
      "Losses {'ner': 6.175475193957606}\n",
      "Losses {'ner': 6.175475265581533}\n",
      "Losses {'ner': 7.85868760931525}\n",
      "Losses {'ner': 7.858689114959492}\n",
      "Losses {'ner': 7.858693448987043}\n",
      "Losses {'ner': 7.8591326611688}\n",
      "Losses {'ner': 9.316642482903578}\n",
      "Losses {'ner': 9.316642482948545}\n",
      "Losses {'ner': 9.320265046046499}\n",
      "Losses {'ner': 9.35234113841398}\n",
      "Losses {'ner': 9.352341139282004}\n",
      "Losses {'ner': 9.35251868229097}\n",
      "Losses {'ner': 9.352518746076402}\n",
      "Losses {'ner': 1.2649502895160927}\n",
      "Losses {'ner': 1.2649504360777322}\n",
      "Losses {'ner': 1.2649929006291565}\n",
      "Losses {'ner': 1.2659236963392424}\n",
      "Losses {'ner': 1.2659330253886778}\n",
      "Losses {'ner': 1.2659330255543142}\n",
      "Losses {'ner': 3.318009702505474}\n",
      "Losses {'ner': 3.318557468249936}\n",
      "Losses {'ner': 3.318573643684354}\n",
      "Losses {'ner': 6.303535630232211}\n",
      "Losses {'ner': 7.584560497407085}\n",
      "Losses {'ner': 8.589547022244936}\n",
      "Losses {'ner': 8.589547030396588}\n",
      "Losses {'ner': 8.58968561552324}\n",
      "Losses {'ner': 8.589685643226803}\n",
      "Losses {'ner': 8.589686386439812}\n",
      "Losses {'ner': 8.590301492516495}\n",
      "Losses {'ner': 8.591136937387747}\n",
      "Losses {'ner': 8.591139780763163}\n",
      "Losses {'ner': 8.658526637428592}\n",
      "Losses {'ner': 8.83368977607208}\n",
      "Losses {'ner': 8.83369073925364}\n",
      "Losses {'ner': 8.83621876766699}\n",
      "Losses {'ner': 8.836218775221846}\n",
      "Losses {'ner': 9.068690370207731}\n",
      "Losses {'ner': 9.090238221111573}\n",
      "Losses {'ner': 9.090238221112294}\n",
      "Losses {'ner': 3.0281965696190545}\n",
      "Losses {'ner': 3.0281966125791278}\n",
      "Losses {'ner': 4.777253097116276}\n",
      "Losses {'ner': 4.942172103645069}\n",
      "Losses {'ner': 5.316902283288663}\n",
      "Losses {'ner': 5.316902283325987}\n",
      "Losses {'ner': 5.3169230740992575}\n",
      "Losses {'ner': 5.328991972177003}\n",
      "Losses {'ner': 5.355535131115869}\n",
      "Losses {'ner': 5.355535131148947}\n",
      "Losses {'ner': 5.355535162635188}\n",
      "Losses {'ner': 5.356594123696578}\n",
      "Losses {'ner': 5.3567765736958375}\n",
      "Losses {'ner': 5.356776573750457}\n",
      "Losses {'ner': 5.361286682389664}\n",
      "Losses {'ner': 5.361286683758289}\n",
      "Losses {'ner': 5.361446137848859}\n",
      "Losses {'ner': 5.3614462817327615}\n",
      "Losses {'ner': 5.40414087168279}\n",
      "Losses {'ner': 5.404140872861102}\n",
      "Losses {'ner': 6.112222552298966}\n",
      "Losses {'ner': 6.112548933653536}\n",
      "Losses {'ner': 9.069835584448205}\n",
      "Losses {'ner': 9.06984424524247}\n",
      "Losses {'ner': 9.069844245941052}\n",
      "Losses {'ner': 10.414114941429508}\n",
      "Losses {'ner': 10.414119592225221}\n",
      "Losses {'ner': 5.120243756514503e-07}\n",
      "Losses {'ner': 2.9303997518838916e-06}\n",
      "Losses {'ner': 2.9305642724871332e-06}\n",
      "Losses {'ner': 0.00023050502906455625}\n",
      "Losses {'ner': 0.00023097326946062914}\n",
      "Losses {'ner': 0.00023152735914159443}\n",
      "Losses {'ner': 0.003080379347128073}\n",
      "Losses {'ner': 0.0030803798559048442}\n",
      "Losses {'ner': 0.003122498966678992}\n",
      "Losses {'ner': 0.0031303065295739934}\n",
      "Losses {'ner': 0.003130310059536579}\n",
      "Losses {'ner': 0.0031303101327239506}\n",
      "Losses {'ner': 0.003183589802563521}\n",
      "Losses {'ner': 0.003183684500121026}\n",
      "Losses {'ner': 1.7240792326166146}\n",
      "Losses {'ner': 2.0504499803125746}\n",
      "Losses {'ner': 2.0505972272307376}\n",
      "Losses {'ner': 4.034361319599957}\n",
      "Losses {'ner': 4.034362093462349}\n",
      "Losses {'ner': 4.034364617341762}\n",
      "Losses {'ner': 4.034364759503032}\n",
      "Losses {'ner': 4.0343647798590645}\n",
      "Losses {'ner': 4.034411876306444}\n",
      "Losses {'ner': 7.084347083423103}\n",
      "Losses {'ner': 9.000275232569136}\n",
      "Losses {'ner': 10.846787802605839}\n",
      "Losses {'ner': 12.543074957961966}\n",
      "Losses {'ner': 7.569578168248896e-06}\n",
      "Losses {'ner': 7.569602058481927e-06}\n",
      "Losses {'ner': 7.570089020532764e-06}\n",
      "Losses {'ner': 7.601921178004904e-06}\n",
      "Losses {'ner': 7.620502002775274e-06}\n",
      "Losses {'ner': 7.620620097257378e-06}\n",
      "Losses {'ner': 7.767873522307488e-06}\n",
      "Losses {'ner': 7.767943924720053e-06}\n",
      "Losses {'ner': 7.767945557386284e-06}\n",
      "Losses {'ner': 9.631260415035991e-06}\n",
      "Losses {'ner': 0.923704920298794}\n",
      "Losses {'ner': 0.923704954935322}\n",
      "Losses {'ner': 0.923712182860455}\n",
      "Losses {'ner': 3.171755529222543}\n",
      "Losses {'ner': 3.1717555399956394}\n",
      "Losses {'ner': 3.171755540325357}\n",
      "Losses {'ner': 4.881137299015392}\n",
      "Losses {'ner': 4.881137299369497}\n",
      "Losses {'ner': 4.881138275595683}\n",
      "Losses {'ner': 4.899118597888107}\n",
      "Losses {'ner': 4.899120712013963}\n",
      "Losses {'ner': 4.944030852705475}\n",
      "Losses {'ner': 4.944030893527129}\n",
      "Losses {'ner': 5.058264885975285}\n",
      "Losses {'ner': 5.058264910324875}\n",
      "Losses {'ner': 5.058267614145796}\n",
      "Losses {'ner': 5.058267614146035}\n",
      "Losses {'ner': 6.475940013073604e-06}\n",
      "Losses {'ner': 6.476517925429561e-06}\n",
      "Losses {'ner': 0.12890211787627495}\n",
      "Losses {'ner': 0.12891101142611724}\n",
      "Losses {'ner': 2.128520124329659}\n",
      "Losses {'ner': 2.1285352651519305}\n",
      "Losses {'ner': 4.124175276464467}\n",
      "Losses {'ner': 4.124175317576263}\n",
      "Losses {'ner': 4.124175355624778}\n",
      "Losses {'ner': 4.124202220302386}\n",
      "Losses {'ner': 4.125177998134913}\n",
      "Losses {'ner': 4.132040285682696}\n",
      "Losses {'ner': 4.1320417403949365}\n",
      "Losses {'ner': 4.825285283091476}\n",
      "Losses {'ner': 4.825285283192548}\n",
      "Losses {'ner': 4.825285284304796}\n",
      "Losses {'ner': 4.825285284305153}\n",
      "Losses {'ner': 7.822927133443055}\n",
      "Losses {'ner': 7.823662501267178}\n",
      "Losses {'ner': 8.867035960526016}\n",
      "Losses {'ner': 8.867035994005002}\n",
      "Losses {'ner': 8.867036177591654}\n",
      "Losses {'ner': 8.86703831991484}\n",
      "Losses {'ner': 10.117084378806142}\n",
      "Losses {'ner': 10.278343187537454}\n",
      "Losses {'ner': 10.278343189529291}\n",
      "Losses {'ner': 10.278343189529295}\n",
      "Losses {'ner': 0.00016414718499461981}\n",
      "Losses {'ner': 0.0001641688241879114}\n",
      "Losses {'ner': 0.00016420549798489025}\n",
      "Losses {'ner': 0.00016556184442964323}\n",
      "Losses {'ner': 0.0001656485730850021}\n",
      "Losses {'ner': 0.00018212178110728954}\n",
      "Losses {'ner': 0.023151707294001494}\n",
      "Losses {'ner': 0.023597639446102887}\n",
      "Losses {'ner': 0.02359769638811196}\n",
      "Losses {'ner': 0.02359769639329442}\n",
      "Losses {'ner': 0.03320011746610378}\n",
      "Losses {'ner': 0.03320011899755515}\n",
      "Losses {'ner': 0.033200119144117336}\n",
      "Losses {'ner': 0.033200119171106275}\n",
      "Losses {'ner': 0.033200119175743385}\n",
      "Losses {'ner': 0.11321319818947394}\n",
      "Losses {'ner': 0.11321327926698954}\n",
      "Losses {'ner': 2.0403255504878786}\n",
      "Losses {'ner': 2.040327392845056}\n",
      "Losses {'ner': 3.679632092411177}\n",
      "Losses {'ner': 3.6796320929303636}\n",
      "Losses {'ner': 3.6796322217887165}\n",
      "Losses {'ner': 4.490086713336464}\n",
      "Losses {'ner': 4.490086810265694}\n",
      "Losses {'ner': 4.4900868194190595}\n",
      "Losses {'ner': 4.490086939912718}\n",
      "Losses {'ner': 4.490086939922708}\n",
      "Losses {'ner': 1.3612817691557446}\n",
      "Losses {'ner': 1.3714764401062713}\n",
      "Losses {'ner': 1.371503571269261}\n",
      "Losses {'ner': 1.3717407517759546}\n",
      "Losses {'ner': 1.3731691745430599}\n",
      "Losses {'ner': 3.3731675073432767}\n",
      "Losses {'ner': 3.3731711925538392}\n",
      "Losses {'ner': 3.373175448320676}\n",
      "Losses {'ner': 3.3890463843442746}\n",
      "Losses {'ner': 3.3890463902683603}\n",
      "Losses {'ner': 7.240896900788703}\n",
      "Losses {'ner': 7.240900771692554}\n",
      "Losses {'ner': 7.240900771723592}\n",
      "Losses {'ner': 7.240900813613083}\n",
      "Losses {'ner': 7.240900841986006}\n",
      "Losses {'ner': 7.240900842147627}\n",
      "Losses {'ner': 9.0886320193854}\n",
      "Losses {'ner': 9.123869820564728}\n",
      "Losses {'ner': 9.12386983300215}\n",
      "Losses {'ner': 9.12386985256761}\n",
      "Losses {'ner': 9.138420108985267}\n",
      "Losses {'ner': 9.13842013514778}\n",
      "Losses {'ner': 9.138460485410244}\n",
      "Losses {'ner': 9.138564014128864}\n",
      "Losses {'ner': 9.164686506285006}\n",
      "Losses {'ner': 9.164686917808831}\n",
      "Losses {'ner': 9.164686918367527}\n",
      "Losses {'ner': 1.027041827478394e-08}\n",
      "Losses {'ner': 0.0004591116015830922}\n",
      "Losses {'ner': 0.0004823967676998476}\n",
      "Losses {'ner': 0.0004824031989252773}\n",
      "Losses {'ner': 0.44708731360290666}\n",
      "Losses {'ner': 0.488042608923451}\n",
      "Losses {'ner': 0.48804260902439006}\n",
      "Losses {'ner': 0.4880426097237007}\n",
      "Losses {'ner': 1.1481791320418882}\n",
      "Losses {'ner': 1.1487343677979713}\n",
      "Losses {'ner': 1.1487361791733113}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1.148736179340223}\n",
      "Losses {'ner': 1.1487470672732636}\n",
      "Losses {'ner': 1.1487470672860407}\n",
      "Losses {'ner': 1.1487647106657382}\n",
      "Losses {'ner': 1.1487647190813057}\n",
      "Losses {'ner': 2.420572049416585}\n",
      "Losses {'ner': 2.4205764639365284}\n",
      "Losses {'ner': 2.420576463972871}\n",
      "Losses {'ner': 2.42061909904909}\n",
      "Losses {'ner': 2.433879317649355}\n",
      "Losses {'ner': 4.433879362812423}\n",
      "Losses {'ner': 4.561915141605763}\n",
      "Losses {'ner': 4.56191514161015}\n",
      "Losses {'ner': 4.561915141650293}\n",
      "Losses {'ner': 4.564653355937261}\n",
      "Losses {'ner': 4.564653355937261}\n",
      "Losses {'ner': 3.474509826489205e-09}\n",
      "Losses {'ner': 2.4326586883158667e-06}\n",
      "Losses {'ner': 0.0002674393156288137}\n",
      "Losses {'ner': 0.00026746833417162996}\n",
      "Losses {'ner': 0.0002675961656048404}\n",
      "Losses {'ner': 0.8595732611005253}\n",
      "Losses {'ner': 0.8596508353274284}\n",
      "Losses {'ner': 0.859650854643734}\n",
      "Losses {'ner': 0.8596512707800461}\n",
      "Losses {'ner': 0.859651460717616}\n",
      "Losses {'ner': 0.8596514608059552}\n",
      "Losses {'ner': 1.038568239259133}\n",
      "Losses {'ner': 1.038568366896172}\n",
      "Losses {'ner': 1.0385684049291557}\n",
      "Losses {'ner': 1.038680858406164}\n",
      "Losses {'ner': 1.0386856483354054}\n",
      "Losses {'ner': 1.0386857837549357}\n",
      "Losses {'ner': 2.040879564646841}\n",
      "Losses {'ner': 2.0409001373465734}\n",
      "Losses {'ner': 2.0424014103373334}\n",
      "Losses {'ner': 2.0424014107155384}\n",
      "Losses {'ner': 2.0424044533896626}\n",
      "Losses {'ner': 2.042404453392884}\n",
      "Losses {'ner': 2.0424044544866744}\n",
      "Losses {'ner': 2.079328987705911}\n",
      "Losses {'ner': 2.0793289881283163}\n",
      "Losses {'ner': 2.079328988247306}\n",
      "Losses {'ner': 0.0008529775213635036}\n",
      "Losses {'ner': 0.000852986865796251}\n",
      "Losses {'ner': 0.0017098736305173044}\n",
      "Losses {'ner': 0.0017110743689821805}\n",
      "Losses {'ner': 0.0017110753970477387}\n",
      "Losses {'ner': 0.001711075398038534}\n",
      "Losses {'ner': 0.00186505083383401}\n",
      "Losses {'ner': 0.0018650508433163192}\n",
      "Losses {'ner': 0.04713816871260856}\n",
      "Losses {'ner': 0.0471383905900188}\n",
      "Losses {'ner': 0.047138392639712166}\n",
      "Losses {'ner': 0.047138395964428564}\n",
      "Losses {'ner': 0.04719847893821759}\n",
      "Losses {'ner': 0.04719852148900801}\n",
      "Losses {'ner': 0.04726674267262884}\n",
      "Losses {'ner': 0.04726675668489982}\n",
      "Losses {'ner': 0.12082394362504069}\n",
      "Losses {'ner': 0.44577982046724224}\n",
      "Losses {'ner': 3.793108483450796}\n",
      "Losses {'ner': 3.7931084834518387}\n",
      "Losses {'ner': 3.793422518826688}\n",
      "Losses {'ner': 3.864169146007748}\n",
      "Losses {'ner': 3.8642372874055133}\n",
      "Losses {'ner': 3.8772263569811214}\n",
      "Losses {'ner': 3.87722638984149}\n",
      "Losses {'ner': 5.86350468542441}\n",
      "Losses {'ner': 5.86350468542441}\n",
      "Losses {'ner': 0.02678207680586188}\n",
      "Losses {'ner': 0.026782076808135623}\n",
      "Losses {'ner': 0.026782086541545944}\n",
      "Losses {'ner': 0.026788386131204274}\n",
      "Losses {'ner': 0.026788395842612275}\n",
      "Losses {'ner': 0.026788395913633602}\n",
      "Losses {'ner': 0.02678858473553904}\n",
      "Losses {'ner': 0.026788584890053358}\n",
      "Losses {'ner': 0.02678858489643226}\n",
      "Losses {'ner': 0.026821337591425784}\n",
      "Losses {'ner': 0.03352883601819974}\n",
      "Losses {'ner': 0.033529890719627635}\n",
      "Losses {'ner': 0.03368930137076745}\n",
      "Losses {'ner': 0.03368932475959285}\n",
      "Losses {'ner': 0.033689325285238644}\n",
      "Losses {'ner': 0.033837668304831016}\n",
      "Losses {'ner': 0.03386000850379115}\n",
      "Losses {'ner': 0.035927286182441905}\n",
      "Losses {'ner': 1.871893473536833}\n",
      "Losses {'ner': 1.8718934743515732}\n",
      "Losses {'ner': 3.848926123061788}\n",
      "Losses {'ner': 3.9607475420229066}\n",
      "Losses {'ner': 3.961379146715077}\n",
      "Losses {'ner': 3.961409058240829}\n",
      "Losses {'ner': 5.701483980176807}\n",
      "Losses {'ner': 5.701484041205434}\n",
      "Losses {'ner': 5.7015285813401295}\n",
      "Losses {'ner': 9.007971666771731e-05}\n",
      "Losses {'ner': 9.194138214944968e-05}\n",
      "Losses {'ner': 9.194327640669645e-05}\n",
      "Losses {'ner': 9.194327955120721e-05}\n",
      "Losses {'ner': 0.00015991474684125205}\n",
      "Losses {'ner': 0.00015991785626875456}\n",
      "Losses {'ner': 0.00015993103404714855}\n",
      "Losses {'ner': 0.00015993103446539753}\n",
      "Losses {'ner': 0.00016001589702347956}\n",
      "Losses {'ner': 0.00016001861276232935}\n",
      "Losses {'ner': 0.00016002481347083544}\n",
      "Losses {'ner': 0.04283192906427971}\n",
      "Losses {'ner': 0.04283192906429714}\n",
      "Losses {'ner': 2.0090539919414327}\n",
      "Losses {'ner': 2.0090622945207968}\n",
      "Losses {'ner': 2.0090692860089807}\n",
      "Losses {'ner': 2.0090692861935593}\n",
      "Losses {'ner': 2.131701343809253}\n",
      "Losses {'ner': 2.1317133438787046}\n",
      "Losses {'ner': 4.056888945317567}\n",
      "Losses {'ner': 4.056888975293425}\n",
      "Losses {'ner': 4.056889395917996}\n",
      "Losses {'ner': 4.05700362072877}\n",
      "Losses {'ner': 4.057003622681869}\n",
      "Losses {'ner': 4.057003664162199}\n",
      "Losses {'ner': 4.05703524911529}\n",
      "Losses {'ner': 4.05703524911529}\n",
      "Losses {'ner': 6.841143676328145e-13}\n",
      "Losses {'ner': 0.0035147988496177446}\n",
      "Losses {'ner': 0.003514887822290823}\n",
      "Losses {'ner': 0.004442183416791816}\n",
      "Losses {'ner': 0.004442252311275659}\n",
      "Losses {'ner': 0.004442835295355168}\n",
      "Losses {'ner': 0.004442836170090163}\n",
      "Losses {'ner': 0.004442836174010624}\n",
      "Losses {'ner': 1.4507067555157316}\n",
      "Losses {'ner': 1.4525074036285424}\n",
      "Losses {'ner': 1.4525074036541992}\n",
      "Losses {'ner': 1.452507404284821}\n",
      "Losses {'ner': 1.4540272412089301}\n",
      "Losses {'ner': 1.454090568440059}\n",
      "Losses {'ner': 1.4621454028081493}\n",
      "Losses {'ner': 1.4621854987208027}\n",
      "Losses {'ner': 1.4621854989674574}\n",
      "Losses {'ner': 1.4621854990474303}\n",
      "Losses {'ner': 1.5145704530065698}\n",
      "Losses {'ner': 1.5145704533208821}\n",
      "Losses {'ner': 1.5145704533563702}\n",
      "Losses {'ner': 1.7517455519523124}\n",
      "Losses {'ner': 3.3456486588578995}\n",
      "Losses {'ner': 8.578941849756456}\n",
      "Losses {'ner': 8.578941849781357}\n",
      "Losses {'ner': 8.584554500327492}\n",
      "Losses {'ner': 8.584554500327492}\n",
      "Losses {'ner': 5.898648288744361e-15}\n",
      "Losses {'ner': 0.020336216857366193}\n",
      "Losses {'ner': 0.020336227974731107}\n",
      "Losses {'ner': 2.0202188101354603}\n",
      "Losses {'ner': 2.0202188105443084}\n",
      "Losses {'ner': 2.0202188109734105}\n",
      "Losses {'ner': 2.020218883381193}\n",
      "Losses {'ner': 2.0202188834153536}\n",
      "Losses {'ner': 2.020218883416221}\n",
      "Losses {'ner': 2.020218922291617}\n",
      "Losses {'ner': 2.032844762575439}\n",
      "Losses {'ner': 2.0328450488233236}\n",
      "Losses {'ner': 2.03284505084991}\n",
      "Losses {'ner': 4.032213202653194}\n",
      "Losses {'ner': 4.032217924585549}\n",
      "Losses {'ner': 4.032228521697678}\n",
      "Losses {'ner': 4.051481258444035}\n",
      "Losses {'ner': 4.0514812585389866}\n",
      "Losses {'ner': 4.051523791867055}\n",
      "Losses {'ner': 4.051913000794529}\n",
      "Losses {'ner': 4.051939853695063}\n",
      "Losses {'ner': 4.051939855286836}\n",
      "Losses {'ner': 4.052669254891477}\n",
      "Losses {'ner': 4.0546039667912925}\n",
      "Losses {'ner': 4.054603968661479}\n",
      "Losses {'ner': 4.054604442471839}\n",
      "Losses {'ner': 4.0546044424718435}\n",
      "Losses {'ner': 2.583256057268951e-12}\n",
      "Losses {'ner': 1.9428395373257368}\n",
      "Losses {'ner': 1.942847855933264}\n",
      "Losses {'ner': 1.942847858778031}\n",
      "Losses {'ner': 1.9428478729925758}\n",
      "Losses {'ner': 1.9428478743441184}\n",
      "Losses {'ner': 1.9428478743578088}\n",
      "Losses {'ner': 1.9428478744091384}\n",
      "Losses {'ner': 1.942847874409435}\n",
      "Losses {'ner': 1.9428478755516376}\n",
      "Losses {'ner': 1.9455564615971437}\n",
      "Losses {'ner': 3.846506345963896}\n",
      "Losses {'ner': 3.8465223699969964}\n",
      "Losses {'ner': 3.9967336183081623}\n",
      "Losses {'ner': 3.9967336183764965}\n",
      "Losses {'ner': 3.996733618376639}\n",
      "Losses {'ner': 3.9967336704830663}\n",
      "Losses {'ner': 3.9967336710132306}\n",
      "Losses {'ner': 3.997981203894875}\n",
      "Losses {'ner': 3.9979812150395633}\n",
      "Losses {'ner': 4.012960445147469}\n",
      "Losses {'ner': 4.012960445152011}\n",
      "Losses {'ner': 4.01296044843885}\n",
      "Losses {'ner': 4.012960455625605}\n",
      "Losses {'ner': 4.012960457428892}\n",
      "Losses {'ner': 5.602281013397262}\n",
      "Losses {'ner': 5.602281013397716}\n",
      "Losses {'ner': 6.243930309294758e-09}\n",
      "Losses {'ner': 6.392970693753912e-09}\n",
      "Losses {'ner': 6.520961079888608e-09}\n",
      "Losses {'ner': 6.524834580031411e-09}\n",
      "Losses {'ner': 6.599296663720406e-09}\n",
      "Losses {'ner': 7.466860115548992e-09}\n",
      "Losses {'ner': 0.027097187968744323}\n",
      "Losses {'ner': 0.028124431588001264}\n",
      "Losses {'ner': 0.028124431632083835}\n",
      "Losses {'ner': 0.038257032048360444}\n",
      "Losses {'ner': 0.038257533359542364}\n",
      "Losses {'ner': 0.038257547651447274}\n",
      "Losses {'ner': 0.038257698031840884}\n",
      "Losses {'ner': 0.03825773758658298}\n",
      "Losses {'ner': 0.03828445270004151}\n",
      "Losses {'ner': 0.038287178062686354}\n",
      "Losses {'ner': 0.03828717806270763}\n",
      "Losses {'ner': 0.04748152979996622}\n",
      "Losses {'ner': 0.04748177400694137}\n",
      "Losses {'ner': 0.047481774465094315}\n",
      "Losses {'ner': 0.047483121908955046}\n",
      "Losses {'ner': 0.05649991156696004}\n",
      "Losses {'ner': 0.0564999121045842}\n",
      "Losses {'ner': 0.056502592049316694}\n",
      "Losses {'ner': 0.08049912279150923}\n",
      "Losses {'ner': 0.08049913283293401}\n",
      "Losses {'ner': 0.08049913283293428}\n",
      "Losses {'ner': 0.008477296979808903}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 0.009461153101818026}\n",
      "Losses {'ner': 0.010570503348628654}\n",
      "Losses {'ner': 0.010570507475689514}\n",
      "Losses {'ner': 0.010570507519448437}\n",
      "Losses {'ner': 0.010570538847698855}\n",
      "Losses {'ner': 0.010570968636598906}\n",
      "Losses {'ner': 0.010571086917506536}\n",
      "Losses {'ner': 0.010571087946113516}\n",
      "Losses {'ner': 0.010573572944936143}\n",
      "Losses {'ner': 0.010573626454781554}\n",
      "Losses {'ner': 0.010574390901141083}\n",
      "Losses {'ner': 0.010574390905530539}\n",
      "Losses {'ner': 0.013693730805705549}\n",
      "Losses {'ner': 0.01369787463903249}\n",
      "Losses {'ner': 0.013697874714255613}\n",
      "Losses {'ner': 0.013697874718653165}\n",
      "Losses {'ner': 0.01373541564656937}\n",
      "Losses {'ner': 0.013741662997832735}\n",
      "Losses {'ner': 2.0001253395271195}\n",
      "Losses {'ner': 6.034430062696989}\n",
      "Losses {'ner': 6.034438369439143}\n",
      "Losses {'ner': 6.034438373411843}\n",
      "Losses {'ner': 6.034438398419071}\n",
      "Losses {'ner': 6.035753373507985}\n",
      "Losses {'ner': 6.035753373545893}\n",
      "Losses {'ner': 6.139209368054099}\n",
      "Losses {'ner': 5.4773838185985124e-06}\n",
      "Losses {'ner': 5.477392624056522e-06}\n",
      "Losses {'ner': 5.507533278065829e-06}\n",
      "Losses {'ner': 5.509535797435697e-06}\n",
      "Losses {'ner': 5.570980069363708e-06}\n",
      "Losses {'ner': 2.399589970853937e-05}\n",
      "Losses {'ner': 2.500873915617114e-05}\n",
      "Losses {'ner': 2.5027372823469094e-05}\n",
      "Losses {'ner': 0.0024881724549027833}\n",
      "Losses {'ner': 0.00450798477320871}\n",
      "Losses {'ner': 0.0045079848943078565}\n",
      "Losses {'ner': 0.004529077210015796}\n",
      "Losses {'ner': 0.0055021137211293866}\n",
      "Losses {'ner': 0.005502113721525177}\n",
      "Losses {'ner': 0.005502113873983339}\n",
      "Losses {'ner': 0.005502113876467252}\n",
      "Losses {'ner': 0.0055021138790312205}\n",
      "Losses {'ner': 0.005502478432534369}\n",
      "Losses {'ner': 0.005571195028362644}\n",
      "Losses {'ner': 0.005646579973691954}\n",
      "Losses {'ner': 0.005646756200061811}\n",
      "Losses {'ner': 0.005647404699055541}\n",
      "Losses {'ner': 0.005647409046201806}\n",
      "Losses {'ner': 0.005647411311510598}\n",
      "Losses {'ner': 0.005647411311515815}\n",
      "Losses {'ner': 0.005661719568601692}\n",
      "Losses {'ner': 0.005661719568602028}\n",
      "Losses {'ner': 0.00012385551243585845}\n",
      "Losses {'ner': 1.6989384863712413}\n",
      "Losses {'ner': 1.7556603379517906}\n",
      "Losses {'ner': 1.7556603379520617}\n",
      "Losses {'ner': 1.7556603383256668}\n",
      "Losses {'ner': 1.7556606211055654}\n",
      "Losses {'ner': 1.7556606266529795}\n",
      "Losses {'ner': 1.7557764881608238}\n",
      "Losses {'ner': 1.7580848236244393}\n",
      "Losses {'ner': 1.7688369196185325}\n",
      "Losses {'ner': 1.7688369196627953}\n",
      "Losses {'ner': 1.7688369197497924}\n",
      "Losses {'ner': 1.7688369198511926}\n",
      "Losses {'ner': 1.7688369477197778}\n",
      "Losses {'ner': 1.7688479102041197}\n",
      "Losses {'ner': 2.0487900353975355}\n",
      "Losses {'ner': 2.0487900365454284}\n",
      "Losses {'ner': 2.0488394560899406}\n",
      "Losses {'ner': 2.0488394561000147}\n",
      "Losses {'ner': 2.048839503340959}\n",
      "Losses {'ner': 2.0488395535429875}\n",
      "Losses {'ner': 2.0488462369993754}\n",
      "Losses {'ner': 2.04884676922928}\n",
      "Losses {'ner': 2.048846795610039}\n",
      "Losses {'ner': 2.0488467956388563}\n",
      "Losses {'ner': 2.0488543030996116}\n",
      "Losses {'ner': 2.0488543030996116}\n",
      "Losses {'ner': 5.182053885105184e-13}\n",
      "Losses {'ner': 0.1346775642569767}\n",
      "Losses {'ner': 0.1346795972723137}\n",
      "Losses {'ner': 0.13467975506577956}\n",
      "Losses {'ner': 0.13467975780981692}\n",
      "Losses {'ner': 0.6303395272370438}\n",
      "Losses {'ner': 0.6303397554456315}\n",
      "Losses {'ner': 0.6303397554471797}\n",
      "Losses {'ner': 0.6303397560034661}\n",
      "Losses {'ner': 0.6303397835817053}\n",
      "Losses {'ner': 0.6390134059237426}\n",
      "Losses {'ner': 0.6390134112452165}\n",
      "Losses {'ner': 0.6390134112519739}\n",
      "Losses {'ner': 0.6390134113034883}\n",
      "Losses {'ner': 0.6390134125502714}\n",
      "Losses {'ner': 0.6601876056333474}\n",
      "Losses {'ner': 0.6601876367045927}\n",
      "Losses {'ner': 0.6604425437086654}\n",
      "Losses {'ner': 0.6604425437101588}\n",
      "Losses {'ner': 0.6604433062321836}\n",
      "Losses {'ner': 0.6604644713337983}\n",
      "Losses {'ner': 0.6604984445612261}\n",
      "Losses {'ner': 0.6604984467289643}\n",
      "Losses {'ner': 0.6605234112924151}\n",
      "Losses {'ner': 0.6607186610779159}\n",
      "Losses {'ner': 0.6607186611103887}\n",
      "Losses {'ner': 0.6607186611132121}\n",
      "Losses {'ner': 5.01270445397082e-07}\n",
      "Losses {'ner': 6.441390589123559e-06}\n",
      "Losses {'ner': 1.1567857656599058e-05}\n",
      "Losses {'ner': 1.1598701788866733e-05}\n",
      "Losses {'ner': 1.1598754046213466e-05}\n",
      "Losses {'ner': 1.159876660507085e-05}\n",
      "Losses {'ner': 1.1599181271781745e-05}\n",
      "Losses {'ner': 1.1599181337900059e-05}\n",
      "Losses {'ner': 7.591068390415473e-05}\n",
      "Losses {'ner': 7.591177208272997e-05}\n",
      "Losses {'ner': 7.881883537440679e-05}\n",
      "Losses {'ner': 0.2316885492454126}\n",
      "Losses {'ner': 2.1086793443843606}\n",
      "Losses {'ner': 4.083613675591775}\n",
      "Losses {'ner': 5.707578732728737}\n",
      "Losses {'ner': 5.7075787327353}\n",
      "Losses {'ner': 5.707578732901075}\n",
      "Losses {'ner': 5.709000740033872}\n",
      "Losses {'ner': 5.709001236615471}\n",
      "Losses {'ner': 5.709001265953931}\n",
      "Losses {'ner': 5.734415544711451}\n",
      "Losses {'ner': 5.734415617380333}\n",
      "Losses {'ner': 5.734415617380439}\n",
      "Losses {'ner': 6.524218528146923}\n",
      "Losses {'ner': 6.566726221418242}\n",
      "Losses {'ner': 6.566726221418254}\n",
      "Losses {'ner': 6.566726221418258}\n",
      "Losses {'ner': 7.628383227148613e-06}\n",
      "Losses {'ner': 7.628574591052465e-06}\n",
      "Losses {'ner': 2.9555273707133805e-05}\n",
      "Losses {'ner': 2.1189530555223657}\n",
      "Losses {'ner': 2.118953062789568}\n",
      "Losses {'ner': 2.1189530628734174}\n",
      "Losses {'ner': 2.1189530628734228}\n",
      "Losses {'ner': 2.1189530628912454}\n",
      "Losses {'ner': 2.1189530629113627}\n",
      "Losses {'ner': 2.1189530630075324}\n",
      "Losses {'ner': 2.118953063044882}\n",
      "Losses {'ner': 2.118953063044888}\n",
      "Losses {'ner': 2.11895306316396}\n",
      "Losses {'ner': 2.118953063393174}\n",
      "Losses {'ner': 2.1189530804905656}\n",
      "Losses {'ner': 2.1189531498942595}\n",
      "Losses {'ner': 2.1189534435957844}\n",
      "Losses {'ner': 2.119030528017314}\n",
      "Losses {'ner': 2.1190305280173156}\n",
      "Losses {'ner': 2.1190305280963644}\n",
      "Losses {'ner': 2.119030528098241}\n",
      "Losses {'ner': 2.120711639796665}\n",
      "Losses {'ner': 2.120711639796707}\n",
      "Losses {'ner': 2.1207116469368645}\n",
      "Losses {'ner': 2.120711647039218}\n",
      "Losses {'ner': 2.1207160995569097}\n",
      "Losses {'ner': 2.1429116582493712}\n",
      "Losses {'ner': 1.1211626297805966}\n",
      "Losses {'ner': 1.1211626317762469}\n",
      "Losses {'ner': 1.1211626318709507}\n",
      "Losses {'ner': 1.1211626330475533}\n",
      "Losses {'ner': 1.1233023132800852}\n",
      "Losses {'ner': 1.123302313366885}\n",
      "Losses {'ner': 1.1233023142070198}\n",
      "Losses {'ner': 1.1233102926929937}\n",
      "Losses {'ner': 1.1233309585772024}\n",
      "Losses {'ner': 1.1233309666618494}\n",
      "Losses {'ner': 1.2146907802355857}\n",
      "Losses {'ner': 1.2146907808126783}\n",
      "Losses {'ner': 1.214690780902735}\n",
      "Losses {'ner': 1.2146983015624584}\n",
      "Losses {'ner': 1.2197156366237671}\n",
      "Losses {'ner': 1.2197156366238169}\n",
      "Losses {'ner': 1.21971563662382}\n",
      "Losses {'ner': 1.2343084568246976}\n",
      "Losses {'ner': 1.2343084649921816}\n",
      "Losses {'ner': 1.2343084649929121}\n",
      "Losses {'ner': 1.234308522865553}\n",
      "Losses {'ner': 1.2343085234576425}\n",
      "Losses {'ner': 1.234308658838034}\n",
      "Losses {'ner': 1.2343086588401746}\n",
      "Losses {'ner': 1.2343086589082006}\n",
      "Losses {'ner': 1.234308660730933}\n",
      "Losses {'ner': 1.234308660730933}\n",
      "Losses {'ner': 7.549572168383159e-11}\n",
      "Losses {'ner': 2.4598617802156273e-08}\n",
      "Losses {'ner': 0.03428125950331168}\n",
      "Losses {'ner': 0.03428128103726665}\n",
      "Losses {'ner': 0.03428128918352855}\n",
      "Losses {'ner': 0.0343040134080532}\n",
      "Losses {'ner': 0.034304013409520766}\n",
      "Losses {'ner': 0.03430401359156353}\n",
      "Losses {'ner': 0.03430401823262328}\n",
      "Losses {'ner': 0.034304018232893155}\n",
      "Losses {'ner': 0.03430918384189907}\n",
      "Losses {'ner': 0.03432586774352687}\n",
      "Losses {'ner': 0.03432586774370258}\n",
      "Losses {'ner': 0.034326144388714196}\n",
      "Losses {'ner': 0.03432614438877843}\n",
      "Losses {'ner': 0.034326148296565204}\n",
      "Losses {'ner': 0.034326148358414986}\n",
      "Losses {'ner': 0.034326148394463685}\n",
      "Losses {'ner': 0.03432743887943551}\n",
      "Losses {'ner': 0.034327438880377904}\n",
      "Losses {'ner': 0.44611391191042593}\n",
      "Losses {'ner': 0.44611391327513295}\n",
      "Losses {'ner': 0.44662130297004016}\n",
      "Losses {'ner': 0.4466213029716882}\n",
      "Losses {'ner': 0.4466216609065308}\n",
      "Losses {'ner': 0.45137363382030493}\n",
      "Losses {'ner': 0.45137363382030493}\n",
      "Losses {'ner': 0.0030708715800442776}\n",
      "Losses {'ner': 0.0030708720927813665}\n",
      "Losses {'ner': 0.003070872095422945}\n",
      "Losses {'ner': 0.003070872103407678}\n",
      "Losses {'ner': 0.003070872103512903}\n",
      "Losses {'ner': 0.0032565316231409807}\n",
      "Losses {'ner': 2.2516611537908697}\n",
      "Losses {'ner': 2.2516611537908697}\n",
      "Losses {'ner': 2.251661153791166}\n",
      "Losses {'ner': 2.251695673554384}\n",
      "Losses {'ner': 2.2516956735989933}\n",
      "Losses {'ner': 2.2516956806882087}\n",
      "Losses {'ner': 2.2516956816684437}\n",
      "Losses {'ner': 2.254433513680573}\n",
      "Losses {'ner': 2.2544335136832343}\n",
      "Losses {'ner': 2.254433513713048}\n",
      "Losses {'ner': 2.2544349043310286}\n",
      "Losses {'ner': 2.2544349236195402}\n",
      "Losses {'ner': 2.2758011957412214}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 2.275849354346027}\n",
      "Losses {'ner': 2.6032892884167045}\n",
      "Losses {'ner': 2.8523677895269532}\n",
      "Losses {'ner': 2.8523959393556173}\n",
      "Losses {'ner': 2.852395939850062}\n",
      "Losses {'ner': 2.8523959519931172}\n",
      "Losses {'ner': 2.852395957655359}\n",
      "Losses {'ner': 2.852395957659182}\n",
      "Losses {'ner': 2.658261428271811e-05}\n",
      "Losses {'ner': 0.044012893432842044}\n",
      "Losses {'ner': 0.05183437138255492}\n",
      "Losses {'ner': 0.05183639898929203}\n",
      "Losses {'ner': 0.055810885599691724}\n",
      "Losses {'ner': 0.055810885599762286}\n",
      "Losses {'ner': 0.055810885599840133}\n",
      "Losses {'ner': 0.05581088576469595}\n",
      "Losses {'ner': 0.05581088577521013}\n",
      "Losses {'ner': 0.05581088577593555}\n",
      "Losses {'ner': 0.05581425268428712}\n",
      "Losses {'ner': 0.05581430408051453}\n",
      "Losses {'ner': 2.040420733761991}\n",
      "Losses {'ner': 2.0405354118914407}\n",
      "Losses {'ner': 2.0405354118940524}\n",
      "Losses {'ner': 2.040535418930357}\n",
      "Losses {'ner': 2.0405356095865383}\n",
      "Losses {'ner': 4.2762962019946915}\n",
      "Losses {'ner': 4.2762962028961}\n",
      "Losses {'ner': 4.277189994517766}\n",
      "Losses {'ner': 4.277192246730768}\n",
      "Losses {'ner': 4.2771922476166235}\n",
      "Losses {'ner': 4.277192247620291}\n",
      "Losses {'ner': 4.331408484413462}\n",
      "Losses {'ner': 4.33140848441356}\n",
      "Losses {'ner': 4.331408484864984}\n",
      "Losses {'ner': 4.331408484864985}\n",
      "Losses {'ner': 9.730703636808251e-06}\n",
      "Losses {'ner': 9.730704996784062e-06}\n",
      "Losses {'ner': 9.739873960887075e-06}\n",
      "Losses {'ner': 9.743128561788261e-06}\n",
      "Losses {'ner': 9.74312890735394e-06}\n",
      "Losses {'ner': 9.758130373025749e-06}\n",
      "Losses {'ner': 9.758131091453094e-06}\n",
      "Losses {'ner': 9.759423258420253e-06}\n",
      "Losses {'ner': 0.02207582922507142}\n",
      "Losses {'ner': 0.022075829483829235}\n",
      "Losses {'ner': 0.024699678043062487}\n",
      "Losses {'ner': 0.02469969271150028}\n",
      "Losses {'ner': 0.024699692852047907}\n",
      "Losses {'ner': 2.024699396233505}\n",
      "Losses {'ner': 2.3934764337236145}\n",
      "Losses {'ner': 2.3935487193962133}\n",
      "Losses {'ner': 2.3935516167479336}\n",
      "Losses {'ner': 2.3935516169979194}\n",
      "Losses {'ner': 2.393551677356826}\n",
      "Losses {'ner': 3.2177954017591706}\n",
      "Losses {'ner': 3.217795401768623}\n",
      "Losses {'ner': 3.2177954017720953}\n",
      "Losses {'ner': 3.2177957814863016}\n",
      "Losses {'ner': 3.2178077192860273}\n",
      "Losses {'ner': 3.2178077192982832}\n",
      "Losses {'ner': 3.217807784940321}\n",
      "Losses {'ner': 3.217807784940948}\n",
      "Losses {'ner': 2.542904427298565e-13}\n",
      "Losses {'ner': 3.9318158059083894e-13}\n",
      "Losses {'ner': 1.2936051926861233e-11}\n",
      "Losses {'ner': 2.9189804400803278e-08}\n",
      "Losses {'ner': 2.787138525248662e-07}\n",
      "Losses {'ner': 1.8353887017483421}\n",
      "Losses {'ner': 1.8353890994924258}\n",
      "Losses {'ner': 1.8353890995193807}\n",
      "Losses {'ner': 2.70601829091811}\n",
      "Losses {'ner': 2.7060434745809676}\n",
      "Losses {'ner': 2.7075666029491754}\n",
      "Losses {'ner': 2.7075666030466623}\n",
      "Losses {'ner': 2.7075683307148206}\n",
      "Losses {'ner': 3.3433350780880136}\n",
      "Losses {'ner': 3.3603753683850752}\n",
      "Losses {'ner': 3.3603753683852196}\n",
      "Losses {'ner': 3.5301406165783042}\n",
      "Losses {'ner': 3.5378680484284803}\n",
      "Losses {'ner': 5.260971334907026}\n",
      "Losses {'ner': 5.260971334907106}\n",
      "Losses {'ner': 5.260971338336226}\n",
      "Losses {'ner': 5.2609817870530575}\n",
      "Losses {'ner': 5.260981793171888}\n",
      "Losses {'ner': 5.260981793196098}\n",
      "Losses {'ner': 5.2609818003019635}\n",
      "Losses {'ner': 5.260981813506996}\n",
      "Losses {'ner': 5.260981813507434}\n",
      "Losses {'ner': 4.408301129690401e-05}\n",
      "Losses {'ner': 4.413089445375188e-05}\n",
      "Losses {'ner': 4.413261351777726e-05}\n",
      "Losses {'ner': 4.4271563089785e-05}\n",
      "Losses {'ner': 4.4271569185066826e-05}\n",
      "Losses {'ner': 4.4702191869906444e-05}\n",
      "Losses {'ner': 4.478691089660444e-05}\n",
      "Losses {'ner': 0.00010581471918371998}\n",
      "Losses {'ner': 0.03107468972358542}\n",
      "Losses {'ner': 0.0310746967629925}\n",
      "Losses {'ner': 0.031079926898351123}\n",
      "Losses {'ner': 0.031081113061629858}\n",
      "Losses {'ner': 0.24909164459572666}\n",
      "Losses {'ner': 2.233564873561472}\n",
      "Losses {'ner': 2.2335648736018627}\n",
      "Losses {'ner': 2.2335665220101393}\n",
      "Losses {'ner': 2.234556628969349}\n",
      "Losses {'ner': 2.2346142790239543}\n",
      "Losses {'ner': 2.2346142807433798}\n",
      "Losses {'ner': 2.2346142846336496}\n",
      "Losses {'ner': 2.2346142898168373}\n",
      "Losses {'ner': 2.23461429225435}\n",
      "Losses {'ner': 2.234933641165653}\n",
      "Losses {'ner': 2.23493364116734}\n",
      "Losses {'ner': 2.234933838123934}\n",
      "Losses {'ner': 2.234934125960175}\n",
      "Losses {'ner': 2.2349341259601756}\n",
      "Losses {'ner': 0.001995538887029554}\n",
      "Losses {'ner': 0.0019964356363279764}\n",
      "Losses {'ner': 0.0020386074998951613}\n",
      "Losses {'ner': 0.0020831304137029845}\n",
      "Losses {'ner': 0.002083130413726018}\n",
      "Losses {'ner': 1.4974058281633584}\n",
      "Losses {'ner': 1.4975160339633553}\n",
      "Losses {'ner': 1.497554525027066}\n",
      "Losses {'ner': 1.4988755961015416}\n",
      "Losses {'ner': 1.4988755986890152}\n",
      "Losses {'ner': 1.500272002729262}\n",
      "Losses {'ner': 1.500336053360116}\n",
      "Losses {'ner': 1.5003360864953472}\n",
      "Losses {'ner': 1.5003360864969182}\n",
      "Losses {'ner': 1.5003360864989672}\n",
      "Losses {'ner': 1.5003384361071705}\n",
      "Losses {'ner': 1.5003741607418768}\n",
      "Losses {'ner': 1.5003741609185621}\n",
      "Losses {'ner': 1.5190697035329717}\n",
      "Losses {'ner': 3.318235429247827}\n",
      "Losses {'ner': 3.3182354307694126}\n",
      "Losses {'ner': 3.3182354308015376}\n",
      "Losses {'ner': 3.31823543094715}\n",
      "Losses {'ner': 3.3186547250068537}\n",
      "Losses {'ner': 3.3186547251027623}\n",
      "Losses {'ner': 3.3186554043421546}\n",
      "Losses {'ner': 3.318655404358634}\n",
      "Losses {'ner': 0.0014902917711383274}\n",
      "Losses {'ner': 0.0014902923876911995}\n",
      "Losses {'ner': 1.9961291905003506}\n",
      "Losses {'ner': 1.9961291905003722}\n",
      "Losses {'ner': 1.996163145855978}\n",
      "Losses {'ner': 1.9961909633929777}\n",
      "Losses {'ner': 1.996191283218555}\n",
      "Losses {'ner': 1.9961912876466776}\n",
      "Losses {'ner': 1.9961912876468622}\n",
      "Losses {'ner': 1.9961912876822485}\n",
      "Losses {'ner': 1.9962045212461175}\n",
      "Losses {'ner': 1.9962045212852164}\n",
      "Losses {'ner': 1.9962050029580731}\n",
      "Losses {'ner': 3.9612946846127635}\n",
      "Losses {'ner': 3.961294738074116}\n",
      "Losses {'ner': 3.9612948499871528}\n",
      "Losses {'ner': 3.9612948731778745}\n",
      "Losses {'ner': 3.961294873181267}\n",
      "Losses {'ner': 5.961291410266222}\n",
      "Losses {'ner': 7.029619790971863}\n",
      "Losses {'ner': 7.0296213711532465}\n",
      "Losses {'ner': 7.02964593490924}\n",
      "Losses {'ner': 7.0296499621586195}\n",
      "Losses {'ner': 7.029650006242861}\n",
      "Losses {'ner': 7.029652930724047}\n",
      "Losses {'ner': 7.029652940574666}\n",
      "Losses {'ner': 7.029652940584276}\n",
      "Losses {'ner': 1.6703167764002853e-09}\n",
      "Losses {'ner': 2.744803113262977}\n",
      "Losses {'ner': 2.7448031132629795}\n",
      "Losses {'ner': 2.7448031132631474}\n",
      "Losses {'ner': 2.7448031132631865}\n",
      "Losses {'ner': 2.7448045679657223}\n",
      "Losses {'ner': 2.7448045679662303}\n",
      "Losses {'ner': 3.272194571369587}\n",
      "Losses {'ner': 3.272194571369587}\n",
      "Losses {'ner': 3.27219457144519}\n",
      "Losses {'ner': 3.2721945750605634}\n",
      "Losses {'ner': 3.2828933703839285}\n",
      "Losses {'ner': 3.2828934318393848}\n",
      "Losses {'ner': 3.2828934359159363}\n",
      "Losses {'ner': 3.282943438751404}\n",
      "Losses {'ner': 3.2829434389956487}\n",
      "Losses {'ner': 3.2829434654179286}\n",
      "Losses {'ner': 3.2829434663477075}\n",
      "Losses {'ner': 3.28339624468777}\n",
      "Losses {'ner': 3.2835030786877577}\n",
      "Losses {'ner': 3.2835030792615747}\n",
      "Losses {'ner': 3.2835032039868453}\n",
      "Losses {'ner': 3.2835033215693596}\n",
      "Losses {'ner': 3.2888379584935024}\n",
      "Losses {'ner': 5.285854773056387}\n",
      "Losses {'ner': 5.285854773075575}\n",
      "Losses {'ner': 5.2858549475292085}\n",
      "Losses {'ner': 3.683143642292431e-08}\n",
      "Losses {'ner': 0.8049457451950609}\n",
      "Losses {'ner': 0.8049457453506187}\n",
      "Losses {'ner': 0.8049689449661169}\n",
      "Losses {'ner': 0.8049689449661601}\n",
      "Losses {'ner': 0.8049689449841461}\n",
      "Losses {'ner': 0.8049689813434782}\n",
      "Losses {'ner': 0.804980441357186}\n",
      "Losses {'ner': 2.8009574854628143}\n",
      "Losses {'ner': 2.8009574855654296}\n",
      "Losses {'ner': 2.8030535960361624}\n",
      "Losses {'ner': 2.8030535960363987}\n",
      "Losses {'ner': 2.8030535960374827}\n",
      "Losses {'ner': 2.803053640331999}\n",
      "Losses {'ner': 2.8030536435702778}\n",
      "Losses {'ner': 2.803075030954852}\n",
      "Losses {'ner': 2.8666693293709424}\n",
      "Losses {'ner': 2.8666695150101544}\n",
      "Losses {'ner': 3.244517207273043}\n",
      "Losses {'ner': 3.244517244981484}\n",
      "Losses {'ner': 3.2445191229846047}\n",
      "Losses {'ner': 3.3350345412540348}\n",
      "Losses {'ner': 3.5146856094648204}\n",
      "Losses {'ner': 3.514685609488214}\n",
      "Losses {'ner': 3.514685609488246}\n",
      "Losses {'ner': 3.5146856254163295}\n",
      "Losses {'ner': 3.5146856254163295}\n",
      "Losses {'ner': 1.3375010354577534e-12}\n",
      "Losses {'ner': 2.324982289993738e-10}\n",
      "Losses {'ner': 0.004973169189261055}\n",
      "Losses {'ner': 0.005144508040773415}\n",
      "Losses {'ner': 0.00514451191727559}\n",
      "Losses {'ner': 0.06334964867529333}\n",
      "Losses {'ner': 0.06334965438510336}\n",
      "Losses {'ner': 0.06492004341249125}\n",
      "Losses {'ner': 0.06492004341348714}\n",
      "Losses {'ner': 0.0649203698942343}\n",
      "Losses {'ner': 0.06499864505368076}\n",
      "Losses {'ner': 2.0197584814043306}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 2.019758481409096}\n",
      "Losses {'ner': 2.019758481410396}\n",
      "Losses {'ner': 2.0197586454958105}\n",
      "Losses {'ner': 2.019758645496726}\n",
      "Losses {'ner': 2.019758645496729}\n",
      "Losses {'ner': 2.0197586454970295}\n",
      "Losses {'ner': 2.0198761947298642}\n",
      "Losses {'ner': 2.0198761954537443}\n",
      "Losses {'ner': 2.019876196603581}\n",
      "Losses {'ner': 2.0198761966198933}\n",
      "Losses {'ner': 2.0198772567571486}\n",
      "Losses {'ner': 3.9822563551673937}\n",
      "Losses {'ner': 3.982256355167396}\n",
      "Losses {'ner': 3.9822563551849424}\n",
      "Losses {'ner': 3.9822563551849424}\n",
      "Losses {'ner': 2.8801470454193577e-07}\n",
      "Losses {'ner': 2.8814477280237063e-07}\n",
      "Losses {'ner': 4.013492817021962e-07}\n",
      "Losses {'ner': 4.1252881565177586e-07}\n",
      "Losses {'ner': 4.1254619786925856e-07}\n",
      "Losses {'ner': 1.9998938472011631}\n",
      "Losses {'ner': 1.9998940440612496}\n",
      "Losses {'ner': 2.4720502706972507}\n",
      "Losses {'ner': 2.4720502715746404}\n",
      "Losses {'ner': 2.472050271665881}\n",
      "Losses {'ner': 3.7570368315281373}\n",
      "Losses {'ner': 3.7570368315400144}\n",
      "Losses {'ner': 5.6012858418995135}\n",
      "Losses {'ner': 5.601285841899515}\n",
      "Losses {'ner': 5.601285841899527}\n",
      "Losses {'ner': 8.345180420044557}\n",
      "Losses {'ner': 8.346981929593658}\n",
      "Losses {'ner': 8.346981929596454}\n",
      "Losses {'ner': 8.346982012529127}\n",
      "Losses {'ner': 8.346982012529127}\n",
      "Losses {'ner': 9.25488097990698}\n",
      "Losses {'ner': 9.25488097990982}\n",
      "Losses {'ner': 9.254880980165956}\n",
      "Losses {'ner': 9.384108052175007}\n",
      "Losses {'ner': 9.384108776572717}\n",
      "Losses {'ner': 9.384108778841185}\n",
      "Losses {'ner': 9.384108783319387}\n",
      "Losses {'ner': 1.819371429807441}\n",
      "Losses {'ner': 1.8196228548664002}\n",
      "Losses {'ner': 1.8196263343089742}\n",
      "Losses {'ner': 1.8196263343381913}\n",
      "Losses {'ner': 1.8196263348724477}\n",
      "Losses {'ner': 1.819626334872988}\n",
      "Losses {'ner': 1.8216287026470142}\n",
      "Losses {'ner': 1.8216287026518287}\n",
      "Losses {'ner': 3.5794819341802677}\n",
      "Losses {'ner': 3.5806702998785718}\n",
      "Losses {'ner': 3.580670299886121}\n",
      "Losses {'ner': 5.570468651126637}\n",
      "Losses {'ner': 5.570468651161015}\n",
      "Losses {'ner': 5.570468651162418}\n",
      "Losses {'ner': 5.735354708541245}\n",
      "Losses {'ner': 5.735354708562915}\n",
      "Losses {'ner': 5.735357102226976}\n",
      "Losses {'ner': 5.735419248843982}\n",
      "Losses {'ner': 5.735419248857495}\n",
      "Losses {'ner': 7.685247878419717}\n",
      "Losses {'ner': 9.431755584751649}\n",
      "Losses {'ner': 9.431755585465465}\n",
      "Losses {'ner': 11.424480294209165}\n",
      "Losses {'ner': 11.424480294209324}\n",
      "Losses {'ner': 11.424480327551077}\n",
      "Losses {'ner': 12.811511172037626}\n",
      "Losses {'ner': 12.811511172037626}\n",
      "Losses {'ner': 1.9997334504370023}\n",
      "Losses {'ner': 1.9997334506285727}\n",
      "Losses {'ner': 1.9997335014658637}\n",
      "Losses {'ner': 1.99973350252482}\n",
      "Losses {'ner': 2.00060287382964}\n",
      "Losses {'ner': 2.0006028739002177}\n",
      "Losses {'ner': 2.0006134532300135}\n",
      "Losses {'ner': 2.0006134532362507}\n",
      "Losses {'ner': 2.0006134540982043}\n",
      "Losses {'ner': 2.120537316893673}\n",
      "Losses {'ner': 2.1205373327275217}\n",
      "Losses {'ner': 2.1205373344591396}\n",
      "Losses {'ner': 2.1205373344594123}\n",
      "Losses {'ner': 2.120537334641853}\n",
      "Losses {'ner': 4.117798207496979}\n",
      "Losses {'ner': 4.117924930912651}\n",
      "Losses {'ner': 4.117927593351716}\n",
      "Losses {'ner': 4.117927898625102}\n",
      "Losses {'ner': 4.11792803556003}\n",
      "Losses {'ner': 5.643906481567408}\n",
      "Losses {'ner': 5.643906730770413}\n",
      "Losses {'ner': 5.643906767410147}\n",
      "Losses {'ner': 5.643906767418644}\n",
      "Losses {'ner': 5.643942994382376}\n",
      "Losses {'ner': 5.6439468671171715}\n",
      "Losses {'ner': 5.643946867133522}\n",
      "Losses {'ner': 5.643946867133526}\n",
      "Losses {'ner': 1.1074097685974509e-13}\n",
      "Losses {'ner': 2.2035135497475495e-11}\n",
      "Losses {'ner': 1.0964575524236792e-08}\n",
      "Losses {'ner': 0.0003070186738087417}\n",
      "Losses {'ner': 0.0003070235079216957}\n",
      "Losses {'ner': 0.0003070237218526759}\n",
      "Losses {'ner': 0.0003070264630368556}\n",
      "Losses {'ner': 0.018043299564072118}\n",
      "Losses {'ner': 0.018043299564111267}\n",
      "Losses {'ner': 0.018043299564355658}\n",
      "Losses {'ner': 0.018044697723804386}\n",
      "Losses {'ner': 0.01804469773457663}\n",
      "Losses {'ner': 0.17001264044932796}\n",
      "Losses {'ner': 0.17001264044951722}\n",
      "Losses {'ner': 0.1700126535187073}\n",
      "Losses {'ner': 0.1749642484562498}\n",
      "Losses {'ner': 0.1749642900704406}\n",
      "Losses {'ner': 0.17496442145951985}\n",
      "Losses {'ner': 0.17505013571821282}\n",
      "Losses {'ner': 0.17505054915918034}\n",
      "Losses {'ner': 0.17505054916455232}\n",
      "Losses {'ner': 0.17505054916581272}\n",
      "Losses {'ner': 0.17505054916582946}\n",
      "Losses {'ner': 0.17505054917068236}\n",
      "Losses {'ner': 0.1750506241066554}\n",
      "Losses {'ner': 0.1750506441467132}\n",
      "Losses {'ner': 0.17505064414673774}\n",
      "Losses {'ner': 0.36663270002141224}\n",
      "Losses {'ner': 0.3666327025293158}\n",
      "Losses {'ner': 0.3666390021180724}\n",
      "Losses {'ner': 0.3880188231581183}\n",
      "Losses {'ner': 0.38801882450307157}\n",
      "Losses {'ner': 0.3880188245057044}\n",
      "Losses {'ner': 0.3880188253748622}\n",
      "Losses {'ner': 0.3880188254007195}\n",
      "Losses {'ner': 0.3880188256558056}\n",
      "Losses {'ner': 0.9114502633102386}\n",
      "Losses {'ner': 0.9114502633489725}\n",
      "Losses {'ner': 0.9114502634014575}\n",
      "Losses {'ner': 0.9114502634014952}\n",
      "Losses {'ner': 0.9114509283595209}\n",
      "Losses {'ner': 0.9114509309329505}\n",
      "Losses {'ner': 0.911450930933404}\n",
      "Losses {'ner': 0.9114566647722322}\n",
      "Losses {'ner': 2.2024836459474213}\n",
      "Losses {'ner': 2.2024836460156747}\n",
      "Losses {'ner': 4.238561257188029}\n",
      "Losses {'ner': 4.238561257188029}\n",
      "Losses {'ner': 4.238561257261773}\n",
      "Losses {'ner': 4.238561257397977}\n",
      "Losses {'ner': 4.238561668611749}\n",
      "Losses {'ner': 4.238561774575442}\n",
      "Losses {'ner': 4.238561774575499}\n",
      "Losses {'ner': 4.238561774701176}\n",
      "Losses {'ner': 6.356661109240445e-06}\n",
      "Losses {'ner': 6.359973106802448e-06}\n",
      "Losses {'ner': 1.3428367795746194}\n",
      "Losses {'ner': 1.342836847044261}\n",
      "Losses {'ner': 1.342836847044343}\n",
      "Losses {'ner': 1.34283725623361}\n",
      "Losses {'ner': 2.805881184389443}\n",
      "Losses {'ner': 2.8058811843925846}\n",
      "Losses {'ner': 2.805881184392668}\n",
      "Losses {'ner': 2.8058811844007487}\n",
      "Losses {'ner': 2.805881184402319}\n",
      "Losses {'ner': 2.8059034749510743}\n",
      "Losses {'ner': 2.805903474966233}\n",
      "Losses {'ner': 4.805513422171057}\n",
      "Losses {'ner': 4.805513423974606}\n",
      "Losses {'ner': 4.805513424960551}\n",
      "Losses {'ner': 4.844769133419927}\n",
      "Losses {'ner': 4.8447691334218295}\n",
      "Losses {'ner': 4.900563126596859}\n",
      "Losses {'ner': 4.90056312667474}\n",
      "Losses {'ner': 4.900563126676305}\n",
      "Losses {'ner': 4.900563126696607}\n",
      "Losses {'ner': 5.480837887763513}\n",
      "Losses {'ner': 5.480837887812553}\n",
      "Losses {'ner': 5.48083788781262}\n",
      "Losses {'ner': 5.480837891416348}\n",
      "Losses {'ner': 5.480837891416348}\n",
      "Losses {'ner': 1.103364805023901e-09}\n",
      "Losses {'ner': 0.000809877125620893}\n",
      "Losses {'ner': 0.0008098772426581137}\n",
      "Losses {'ner': 0.0008098772567173843}\n",
      "Losses {'ner': 0.0008098772598151721}\n",
      "Losses {'ner': 0.0008098824234068218}\n",
      "Losses {'ner': 0.000809882484278235}\n",
      "Losses {'ner': 0.0008098824842796912}\n",
      "Losses {'ner': 0.16622297374522133}\n",
      "Losses {'ner': 0.17326837869751283}\n",
      "Losses {'ner': 0.17326837906880663}\n",
      "Losses {'ner': 0.1732683790797769}\n",
      "Losses {'ner': 0.1732683799350601}\n",
      "Losses {'ner': 0.17326838039685655}\n",
      "Losses {'ner': 0.17326838088128574}\n",
      "Losses {'ner': 0.1732683993965526}\n",
      "Losses {'ner': 0.17326841539152982}\n",
      "Losses {'ner': 0.17326841649409386}\n",
      "Losses {'ner': 0.17610593364894706}\n",
      "Losses {'ner': 0.17610593365781604}\n",
      "Losses {'ner': 0.17610613012526782}\n",
      "Losses {'ner': 0.17612384563983657}\n",
      "Losses {'ner': 0.17612384567586467}\n",
      "Losses {'ner': 0.1761238456765241}\n",
      "Losses {'ner': 0.18320756657489465}\n",
      "Losses {'ner': 0.18464518488046353}\n",
      "Losses {'ner': 0.18464518488046386}\n",
      "Losses {'ner': 3.6552425569291736e-09}\n",
      "Losses {'ner': 3.658861990060351e-09}\n",
      "Losses {'ner': 4.6250002457647825e-09}\n",
      "Losses {'ner': 1.0495982442931074e-08}\n",
      "Losses {'ner': 1.062678176849457e-08}\n",
      "Losses {'ner': 1.0626782863940108e-08}\n",
      "Losses {'ner': 1.0738702615189118e-08}\n",
      "Losses {'ner': 0.00012230356472654245}\n",
      "Losses {'ner': 0.00012230356475070254}\n",
      "Losses {'ner': 0.00014044101809568887}\n",
      "Losses {'ner': 0.0001404410197364441}\n",
      "Losses {'ner': 0.00014044288391579546}\n",
      "Losses {'ner': 1.1315614128470044}\n",
      "Losses {'ner': 1.131561412848453}\n",
      "Losses {'ner': 1.1758781715100837}\n",
      "Losses {'ner': 1.1759566541924729}\n",
      "Losses {'ner': 1.1778662021034048}\n",
      "Losses {'ner': 1.1778662021062871}\n",
      "Losses {'ner': 1.1779333946437014}\n",
      "Losses {'ner': 1.1779334004170898}\n",
      "Losses {'ner': 1.178513823923777}\n",
      "Losses {'ner': 1.1785138293180035}\n",
      "Losses {'ner': 1.17851382931891}\n",
      "Losses {'ner': 1.178516983632415}\n",
      "Losses {'ner': 1.1785170119269364}\n",
      "Losses {'ner': 1.1785320601389793}\n",
      "Losses {'ner': 1.1785323315242782}\n",
      "Losses {'ner': 7.205866844883814e-07}\n",
      "Losses {'ner': 7.206830331851224e-07}\n",
      "Losses {'ner': 7.206830339427973e-07}\n",
      "Losses {'ner': 0.002652541576461643}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 0.00464086392760229}\n",
      "Losses {'ner': 0.004710335484262985}\n",
      "Losses {'ner': 0.004711197741631615}\n",
      "Losses {'ner': 0.00471591492805249}\n",
      "Losses {'ner': 0.0047159149280719655}\n",
      "Losses {'ner': 0.004715917477527822}\n",
      "Losses {'ner': 0.00485207889407347}\n",
      "Losses {'ner': 0.00660856584266697}\n",
      "Losses {'ner': 0.006648706702849374}\n",
      "Losses {'ner': 0.00664870670401505}\n",
      "Losses {'ner': 0.014833970427166106}\n",
      "Losses {'ner': 0.014833970427166164}\n",
      "Losses {'ner': 0.014833970429342725}\n",
      "Losses {'ner': 0.014833976789492453}\n",
      "Losses {'ner': 0.8419839601524273}\n",
      "Losses {'ner': 0.8419846399744956}\n",
      "Losses {'ner': 0.8419846399745754}\n",
      "Losses {'ner': 0.8419846817140921}\n",
      "Losses {'ner': 0.8419846850850529}\n",
      "Losses {'ner': 0.8419846856399851}\n",
      "Losses {'ner': 0.8419846856673106}\n",
      "Losses {'ner': 0.8419846869236727}\n",
      "Losses {'ner': 0.8419846869236727}\n",
      "Losses {'ner': 4.8375317419428615e-05}\n",
      "Losses {'ner': 4.837963456083325e-05}\n",
      "Losses {'ner': 4.837963456115456e-05}\n",
      "Losses {'ner': 4.8379640515092874e-05}\n",
      "Losses {'ner': 4.8379642879546065e-05}\n",
      "Losses {'ner': 4.838632417199737e-05}\n",
      "Losses {'ner': 4.846618426589865e-05}\n",
      "Losses {'ner': 5.014137139021987e-05}\n",
      "Losses {'ner': 5.014139135187227e-05}\n",
      "Losses {'ner': 0.07123272043212438}\n",
      "Losses {'ner': 0.07123272047679141}\n",
      "Losses {'ner': 0.07127246432114892}\n",
      "Losses {'ner': 0.07127246620492463}\n",
      "Losses {'ner': 0.07127246620492503}\n",
      "Losses {'ner': 0.07127328510952108}\n",
      "Losses {'ner': 0.07127633858707239}\n",
      "Losses {'ner': 0.07127633874747154}\n",
      "Losses {'ner': 0.07144985782822724}\n",
      "Losses {'ner': 0.0714500241215366}\n",
      "Losses {'ner': 0.07145002439854083}\n",
      "Losses {'ner': 0.07145002439884593}\n",
      "Losses {'ner': 0.07145002608197915}\n",
      "Losses {'ner': 0.15619568705396608}\n",
      "Losses {'ner': 0.15619571685674014}\n",
      "Losses {'ner': 0.15619571685674016}\n",
      "Losses {'ner': 0.1561957168567734}\n",
      "Losses {'ner': 0.1561957168567738}\n",
      "Losses {'ner': 0.003124918005647319}\n",
      "Losses {'ner': 0.003124918005647321}\n",
      "Losses {'ner': 0.003124918057491095}\n",
      "Losses {'ner': 0.06431877604854007}\n",
      "Losses {'ner': 0.06434079161982201}\n",
      "Losses {'ner': 0.06434086082195878}\n",
      "Losses {'ner': 0.06434086092050864}\n",
      "Losses {'ner': 0.06434235966253728}\n",
      "Losses {'ner': 0.06434235978657045}\n",
      "Losses {'ner': 0.06434349405379464}\n",
      "Losses {'ner': 0.064343494276659}\n",
      "Losses {'ner': 0.06434349427665909}\n",
      "Losses {'ner': 0.2389405770462999}\n",
      "Losses {'ner': 0.23894057704687047}\n",
      "Losses {'ner': 0.23894267952140436}\n",
      "Losses {'ner': 0.2389426795214821}\n",
      "Losses {'ner': 0.23894268558854795}\n",
      "Losses {'ner': 0.2402808933190926}\n",
      "Losses {'ner': 0.24028089333634858}\n",
      "Losses {'ner': 0.24028089333634864}\n",
      "Losses {'ner': 0.2402808936440377}\n",
      "Losses {'ner': 0.2402808942617423}\n",
      "Losses {'ner': 0.24028089426178295}\n",
      "Losses {'ner': 0.24028101810303823}\n",
      "Losses {'ner': 0.24028101814402875}\n",
      "Losses {'ner': 0.2402810186594554}\n",
      "Losses {'ner': 0.24028101865946216}\n",
      "Losses {'ner': 4.70625693054018e-07}\n",
      "Losses {'ner': 4.7576374721706565e-07}\n",
      "Losses {'ner': 0.0036389931314511183}\n",
      "Losses {'ner': 0.003638993131876156}\n",
      "Losses {'ner': 0.003638993146965375}\n",
      "Losses {'ner': 1.726189624908097}\n",
      "Losses {'ner': 1.726189624939241}\n",
      "Losses {'ner': 1.7261896249458564}\n",
      "Losses {'ner': 1.7261899404574275}\n",
      "Losses {'ner': 1.7261899455786882}\n",
      "Losses {'ner': 1.7261899485663705}\n",
      "Losses {'ner': 1.7262165025777252}\n",
      "Losses {'ner': 1.726842735877424}\n",
      "Losses {'ner': 1.7268427359967546}\n",
      "Losses {'ner': 1.7268427359967604}\n",
      "Losses {'ner': 1.7268531631244937}\n",
      "Losses {'ner': 1.7268716295635311}\n",
      "Losses {'ner': 1.7268917434275113}\n",
      "Losses {'ner': 1.7268917434275113}\n",
      "Losses {'ner': 1.7268917434277906}\n",
      "Losses {'ner': 1.7268917434612385}\n",
      "Losses {'ner': 1.7268917441484606}\n",
      "Losses {'ner': 1.726891744610915}\n",
      "Losses {'ner': 1.7268917446138625}\n",
      "Losses {'ner': 1.726891755492517}\n",
      "Losses {'ner': 1.727260649261206}\n",
      "Losses {'ner': 1.727260649261206}\n",
      "Losses {'ner': 3.965542611048526e-09}\n",
      "Losses {'ner': 1.9932720700838422}\n",
      "Losses {'ner': 3.0962827611150967}\n",
      "Losses {'ner': 3.0962827611156474}\n",
      "Losses {'ner': 3.0962828111873653}\n",
      "Losses {'ner': 3.0962828194501086}\n",
      "Losses {'ner': 3.0962828194504617}\n",
      "Losses {'ner': 3.0962828194505825}\n",
      "Losses {'ner': 3.096294589581678}\n",
      "Losses {'ner': 3.0962945895861846}\n",
      "Losses {'ner': 3.097931353260618}\n",
      "Losses {'ner': 3.0979313532733532}\n",
      "Losses {'ner': 3.098631839049839}\n",
      "Losses {'ner': 3.0986318390498395}\n",
      "Losses {'ner': 3.0986318391894345}\n",
      "Losses {'ner': 3.098631839189435}\n",
      "Losses {'ner': 3.098631843110962}\n",
      "Losses {'ner': 3.099289398179049}\n",
      "Losses {'ner': 3.0992893982040868}\n",
      "Losses {'ner': 3.0992893982461673}\n",
      "Losses {'ner': 3.0992893989090704}\n",
      "Losses {'ner': 3.0992893989091534}\n",
      "Losses {'ner': 3.099289400921941}\n",
      "Losses {'ner': 3.0992894009219416}\n",
      "Losses {'ner': 3.902256355197804}\n",
      "Losses {'ner': 3.9022563551978817}\n",
      "Losses {'ner': 3.9022563551978817}\n",
      "Losses {'ner': 0.0016868272381255164}\n",
      "Losses {'ner': 0.0016868272381255257}\n",
      "Losses {'ner': 0.0016868272381283399}\n",
      "Losses {'ner': 0.0017207694554430926}\n",
      "Losses {'ner': 0.0017207909973194612}\n",
      "Losses {'ner': 0.0017207919039388018}\n",
      "Losses {'ner': 0.0017211952239278962}\n",
      "Losses {'ner': 0.001721195223927927}\n",
      "Losses {'ner': 0.0017212297172181455}\n",
      "Losses {'ner': 0.008046679658167702}\n",
      "Losses {'ner': 0.008046679658202263}\n",
      "Losses {'ner': 0.02217692026858682}\n",
      "Losses {'ner': 0.02217692026907848}\n",
      "Losses {'ner': 0.022176923240828292}\n",
      "Losses {'ner': 1.9746968168870687}\n",
      "Losses {'ner': 1.9746968168870813}\n",
      "Losses {'ner': 1.9749098782882424}\n",
      "Losses {'ner': 1.9749098782888654}\n",
      "Losses {'ner': 1.9749101576064942}\n",
      "Losses {'ner': 1.974910157606495}\n",
      "Losses {'ner': 1.974910160753852}\n",
      "Losses {'ner': 1.974910161036842}\n",
      "Losses {'ner': 1.9749101613220779}\n",
      "Losses {'ner': 1.9749101613222932}\n",
      "Losses {'ner': 4.126970166296893}\n",
      "Losses {'ner': 4.126970166487762}\n",
      "Losses {'ner': 4.126970166487762}\n",
      "Losses {'ner': 5.685909936913796e-10}\n",
      "Losses {'ner': 3.413424243591049e-08}\n",
      "Losses {'ner': 6.343435300324076e-08}\n",
      "Losses {'ner': 7.100457411015618e-05}\n",
      "Losses {'ner': 7.15844526243911e-05}\n",
      "Losses {'ner': 1.9995248035379936}\n",
      "Losses {'ner': 1.9995248035379944}\n",
      "Losses {'ner': 1.9995248035380093}\n",
      "Losses {'ner': 1.9995248035391842}\n",
      "Losses {'ner': 1.9995248035420967}\n",
      "Losses {'ner': 1.9995248035420972}\n",
      "Losses {'ner': 1.9995248035422037}\n",
      "Losses {'ner': 1.9995248048418959}\n",
      "Losses {'ner': 1.999524804841957}\n",
      "Losses {'ner': 1.9996699314858297}\n",
      "Losses {'ner': 1.99966993148583}\n",
      "Losses {'ner': 1.99966996929148}\n",
      "Losses {'ner': 2.0008696604434553}\n",
      "Losses {'ner': 2.259797330660277}\n",
      "Losses {'ner': 2.2597973306602794}\n",
      "Losses {'ner': 2.2597973306892976}\n",
      "Losses {'ner': 2.259797333089609}\n",
      "Losses {'ner': 2.25979733308961}\n",
      "Losses {'ner': 2.259797333189872}\n",
      "Losses {'ner': 4.259650993116352}\n",
      "Losses {'ner': 4.259650993116367}\n",
      "Losses {'ner': 4.259650993116367}\n",
      "Losses {'ner': 0.0003935656317644503}\n",
      "Losses {'ner': 0.00039357247991191714}\n",
      "Losses {'ner': 1.8507671726254658}\n",
      "Losses {'ner': 1.850767172645922}\n",
      "Losses {'ner': 1.8509965422555363}\n",
      "Losses {'ner': 1.850996542939172}\n",
      "Losses {'ner': 1.850996580098993}\n",
      "Losses {'ner': 1.8509965800989936}\n",
      "Losses {'ner': 1.8509969299954279}\n",
      "Losses {'ner': 1.8509983612873782}\n",
      "Losses {'ner': 1.8509983763438764}\n",
      "Losses {'ner': 1.851016319107286}\n",
      "Losses {'ner': 1.9127340703542468}\n",
      "Losses {'ner': 1.9127340704867148}\n",
      "Losses {'ner': 1.9127444454179838}\n",
      "Losses {'ner': 1.912744445418026}\n",
      "Losses {'ner': 1.9127444546487073}\n",
      "Losses {'ner': 1.9127448441216568}\n",
      "Losses {'ner': 1.9127448446073188}\n",
      "Losses {'ner': 1.9128079557198796}\n",
      "Losses {'ner': 1.9128079557199387}\n",
      "Losses {'ner': 1.912808805062611}\n",
      "Losses {'ner': 1.9128088050626568}\n",
      "Losses {'ner': 1.9128088050626906}\n",
      "Losses {'ner': 1.9128088058536157}\n",
      "Losses {'ner': 1.9128088058545485}\n",
      "Losses {'ner': 1.912808805854578}\n",
      "Losses {'ner': 1.1523070925683694e-14}\n",
      "Losses {'ner': 1.7441224451155654e-08}\n",
      "Losses {'ner': 1.885566877988972e-08}\n",
      "Losses {'ner': 4.427063796739765e-08}\n",
      "Losses {'ner': 3.1605378894970277e-07}\n",
      "Losses {'ner': 3.1720453308781285e-07}\n",
      "Losses {'ner': 3.172046384611564e-07}\n",
      "Losses {'ner': 6.616548531317909e-05}\n",
      "Losses {'ner': 7.85945517466292e-05}\n",
      "Losses {'ner': 7.859455176667924e-05}\n",
      "Losses {'ner': 7.859458548089625e-05}\n",
      "Losses {'ner': 7.859458561510608e-05}\n",
      "Losses {'ner': 8.106585026037737e-05}\n",
      "Losses {'ner': 8.106631676407039e-05}\n",
      "Losses {'ner': 0.0002199908321062879}\n",
      "Losses {'ner': 0.00021999083256607184}\n",
      "Losses {'ner': 0.0002199908357711424}\n",
      "Losses {'ner': 0.00024604233307437}\n",
      "Losses {'ner': 0.0002460424385361037}\n",
      "Losses {'ner': 0.6633410953179173}\n",
      "Losses {'ner': 0.6633438153792146}\n",
      "Losses {'ner': 0.6633438319438585}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 0.6633439712864012}\n",
      "Losses {'ner': 0.663343973230452}\n",
      "Losses {'ner': 0.6633439733254834}\n",
      "Losses {'ner': 0.6633439733568286}\n",
      "Losses {'ner': 0.6633439733568289}\n",
      "Losses {'ner': 8.798433427929278e-07}\n",
      "Losses {'ner': 2.3572056172915332e-05}\n",
      "Losses {'ner': 0.8986602294042819}\n",
      "Losses {'ner': 0.8986604247308385}\n",
      "Losses {'ner': 0.8986604248405174}\n",
      "Losses {'ner': 0.8986604595897368}\n",
      "Losses {'ner': 0.8986604597060273}\n",
      "Losses {'ner': 0.8986613809615187}\n",
      "Losses {'ner': 0.8986613809754028}\n",
      "Losses {'ner': 0.8986617829300385}\n",
      "Losses {'ner': 0.8986617830208741}\n",
      "Losses {'ner': 2.887962272460756}\n",
      "Losses {'ner': 2.8879622724882226}\n",
      "Losses {'ner': 2.8879622725108356}\n",
      "Losses {'ner': 2.888608628990013}\n",
      "Losses {'ner': 2.8886086289909487}\n",
      "Losses {'ner': 2.888608634811167}\n",
      "Losses {'ner': 2.8886086348112747}\n",
      "Losses {'ner': 2.888848008007819}\n",
      "Losses {'ner': 2.8888498342703617}\n",
      "Losses {'ner': 2.8888498342703617}\n",
      "Losses {'ner': 2.888849834270665}\n",
      "Losses {'ner': 2.8898566341603824}\n",
      "Losses {'ner': 8.867978987751014}\n",
      "Losses {'ner': 8.867978987751018}\n",
      "Losses {'ner': 8.867978987751018}\n",
      "Losses {'ner': 8.867978987751302}\n",
      "Losses {'ner': 2.696470960023227e-16}\n",
      "Losses {'ner': 0.37623292623429316}\n",
      "Losses {'ner': 0.37624419364037354}\n",
      "Losses {'ner': 0.3763115267783614}\n",
      "Losses {'ner': 0.37631152781983634}\n",
      "Losses {'ner': 0.3763115282457522}\n",
      "Losses {'ner': 0.37691821334495645}\n",
      "Losses {'ner': 0.3777691113204887}\n",
      "Losses {'ner': 0.37776960985027247}\n",
      "Losses {'ner': 0.3777696100706275}\n",
      "Losses {'ner': 0.46099971687999836}\n",
      "Losses {'ner': 0.46099971690111746}\n",
      "Losses {'ner': 0.46099971690279185}\n",
      "Losses {'ner': 0.4609997169437029}\n",
      "Losses {'ner': 0.460999851727778}\n",
      "Losses {'ner': 0.46121187658828583}\n",
      "Losses {'ner': 0.4612119119126938}\n",
      "Losses {'ner': 0.4612119119131548}\n",
      "Losses {'ner': 0.4612119119134605}\n",
      "Losses {'ner': 0.46121191191815336}\n",
      "Losses {'ner': 0.46121191192185645}\n",
      "Losses {'ner': 0.4612119119357725}\n",
      "Losses {'ner': 0.4612119873187964}\n",
      "Losses {'ner': 0.4612119874726565}\n",
      "Losses {'ner': 0.46121199356876597}\n",
      "Losses {'ner': 0.46745659882421425}\n",
      "Losses {'ner': 0.46745659882421425}\n",
      "Losses {'ner': 1.4672960042953493}\n",
      "Losses {'ner': 1.4672960042953604}\n",
      "Losses {'ner': 1.467296008568359}\n",
      "Losses {'ner': 1.4672960085980795}\n",
      "Losses {'ner': 1.4672960087089215}\n",
      "Losses {'ner': 1.46729600979133}\n",
      "Losses {'ner': 1.4672960238834236}\n",
      "Losses {'ner': 1.46729602400206}\n",
      "Losses {'ner': 1.4672960240020603}\n",
      "Losses {'ner': 1.4695034243017497}\n",
      "Losses {'ner': 1.4695034243431093}\n",
      "Losses {'ner': 1.469930723067534}\n",
      "Losses {'ner': 1.4699307246241295}\n",
      "Losses {'ner': 1.4699307250681346}\n",
      "Losses {'ner': 3.150803448712562}\n",
      "Losses {'ner': 3.1508034487125673}\n",
      "Losses {'ner': 3.150803488332796}\n",
      "Losses {'ner': 3.1508034883328184}\n",
      "Losses {'ner': 3.1508057452851017}\n",
      "Losses {'ner': 6.313274000023121}\n",
      "Losses {'ner': 6.313274000570463}\n",
      "Losses {'ner': 6.313358032121902}\n",
      "Losses {'ner': 6.313358032121903}\n",
      "Losses {'ner': 6.313358038589833}\n",
      "Losses {'ner': 6.313358039449991}\n",
      "Losses {'ner': 6.313358039449992}\n",
      "Losses {'ner': 6.313358039449994}\n",
      "Losses {'ner': 4.976118709609858e-09}\n",
      "Losses {'ner': 5.564886939926757e-09}\n",
      "Losses {'ner': 5.564977409123938e-09}\n",
      "Losses {'ner': 0.09337252849289161}\n",
      "Losses {'ner': 0.09337253308334738}\n",
      "Losses {'ner': 0.09337253315705302}\n",
      "Losses {'ner': 0.09337253317016968}\n",
      "Losses {'ner': 0.09337253323932772}\n",
      "Losses {'ner': 0.09375822909850567}\n",
      "Losses {'ner': 0.09375824838909821}\n",
      "Losses {'ner': 0.09375824839021034}\n",
      "Losses {'ner': 0.09375826488399984}\n",
      "Losses {'ner': 0.09375828604876617}\n",
      "Losses {'ner': 0.09375828604886846}\n",
      "Losses {'ner': 0.09375828608597289}\n",
      "Losses {'ner': 0.24785506917927508}\n",
      "Losses {'ner': 0.2482346655213877}\n",
      "Losses {'ner': 0.24825043389290555}\n",
      "Losses {'ner': 0.24840369688422764}\n",
      "Losses {'ner': 0.24840369775102586}\n",
      "Losses {'ner': 0.2484037022742044}\n",
      "Losses {'ner': 0.24840370281658364}\n",
      "Losses {'ner': 0.2625595954960544}\n",
      "Losses {'ner': 0.2625601680220099}\n",
      "Losses {'ner': 0.2625601680291239}\n",
      "Losses {'ner': 0.26256016802921506}\n",
      "Losses {'ner': 0.2625601680292153}\n",
      "Losses {'ner': 4.2672878804711666e-16}\n",
      "Losses {'ner': 0.0003099942760628152}\n",
      "Losses {'ner': 0.0003111918186045875}\n",
      "Losses {'ner': 0.0003111932551772964}\n",
      "Losses {'ner': 1.991920258117355}\n",
      "Losses {'ner': 1.9919202581174191}\n",
      "Losses {'ner': 1.9919202581174247}\n",
      "Losses {'ner': 1.991920304810662}\n",
      "Losses {'ner': 1.9919203052198164}\n",
      "Losses {'ner': 1.991920404337039}\n",
      "Losses {'ner': 1.9919210317178118}\n",
      "Losses {'ner': 1.9919212500711014}\n",
      "Losses {'ner': 2.815697137569935}\n",
      "Losses {'ner': 2.815697137569935}\n",
      "Losses {'ner': 2.815697137570825}\n",
      "Losses {'ner': 2.8156971640207904}\n",
      "Losses {'ner': 2.8222352672542597}\n",
      "Losses {'ner': 2.8222352703310225}\n",
      "Losses {'ner': 2.8222352703550277}\n",
      "Losses {'ner': 2.8222431325529476}\n",
      "Losses {'ner': 2.8222431325530777}\n",
      "Losses {'ner': 2.8222431325532176}\n",
      "Losses {'ner': 2.8222431488088566}\n",
      "Losses {'ner': 2.822243148920809}\n",
      "Losses {'ner': 2.822243149365788}\n",
      "Losses {'ner': 2.8223247141555166}\n",
      "Losses {'ner': 2.8223247141555166}\n",
      "Losses {'ner': 1.8492483598412127e-07}\n",
      "Losses {'ner': 1.8498513475452127e-07}\n",
      "Losses {'ner': 4.827040848526109e-07}\n",
      "Losses {'ner': 7.070051032537655e-07}\n",
      "Losses {'ner': 7.103998404484774e-07}\n",
      "Losses {'ner': 0.0007510019131743137}\n",
      "Losses {'ner': 0.5079754478327864}\n",
      "Losses {'ner': 0.5079754478334859}\n",
      "Losses {'ner': 0.5079754478338777}\n",
      "Losses {'ner': 0.5080408367694291}\n",
      "Losses {'ner': 0.5080408367943554}\n",
      "Losses {'ner': 0.5080420042946663}\n",
      "Losses {'ner': 0.5081580812857431}\n",
      "Losses {'ner': 2.543725306269267}\n",
      "Losses {'ner': 2.543725608434221}\n",
      "Losses {'ner': 5.124644874181295}\n",
      "Losses {'ner': 5.124644879049107}\n",
      "Losses {'ner': 5.124644882115322}\n",
      "Losses {'ner': 5.1246448821202}\n",
      "Losses {'ner': 5.124644882127142}\n",
      "Losses {'ner': 5.130361771162113}\n",
      "Losses {'ner': 5.130361777244819}\n",
      "Losses {'ner': 5.130361777255409}\n",
      "Losses {'ner': 5.13036177725541}\n",
      "Losses {'ner': 5.13036178082887}\n",
      "Losses {'ner': 5.1303617808299435}\n",
      "Losses {'ner': 5.130361783086629}\n",
      "Losses {'ner': 2.6137262742687633e-14}\n",
      "Losses {'ner': 9.282934624353845e-12}\n",
      "Losses {'ner': 4.054831088487109e-09}\n",
      "Losses {'ner': 4.057088123249991e-09}\n",
      "Losses {'ner': 4.057904628674821e-09}\n",
      "Losses {'ner': 4.0598302533032e-09}\n",
      "Losses {'ner': 9.109677803448103e-07}\n",
      "Losses {'ner': 9.209793854410395e-07}\n",
      "Losses {'ner': 1.439769611122757}\n",
      "Losses {'ner': 1.4397696111284752}\n",
      "Losses {'ner': 1.4438819650949903}\n",
      "Losses {'ner': 1.4438819650950077}\n",
      "Losses {'ner': 1.4438819783806267}\n",
      "Losses {'ner': 1.4438819783826546}\n",
      "Losses {'ner': 1.4441380928792749}\n",
      "Losses {'ner': 1.4933358077642087}\n",
      "Losses {'ner': 1.4933528006548247}\n",
      "Losses {'ner': 1.7584645713103344}\n",
      "Losses {'ner': 1.7595123974788245}\n",
      "Losses {'ner': 1.7595123974844573}\n",
      "Losses {'ner': 2.4622522508211575}\n",
      "Losses {'ner': 2.4622540353881455}\n",
      "Losses {'ner': 2.677917140791483}\n",
      "Losses {'ner': 2.67791715402795}\n",
      "Losses {'ner': 2.677917154027953}\n",
      "Losses {'ner': 2.677917154027961}\n",
      "Losses {'ner': 2.677917154027962}\n",
      "Losses {'ner': 1.6989651491751277e-11}\n",
      "Losses {'ner': 2.581292477147057e-07}\n",
      "Losses {'ner': 2.5812981608640315e-07}\n",
      "Losses {'ner': 2.5812981806004677e-07}\n",
      "Losses {'ner': 0.0001669587214929998}\n",
      "Losses {'ner': 0.000166958721598006}\n",
      "Losses {'ner': 0.00016695919727905992}\n",
      "Losses {'ner': 0.0001669591974867162}\n",
      "Losses {'ner': 0.00016695919799979692}\n",
      "Losses {'ner': 0.0001670181468683595}\n",
      "Losses {'ner': 0.00016701815871782605}\n",
      "Losses {'ner': 0.0001670181643392562}\n",
      "Losses {'ner': 1.2650122439577918}\n",
      "Losses {'ner': 1.2650722795735774}\n",
      "Losses {'ner': 1.2651212283422681}\n",
      "Losses {'ner': 1.265121249170203}\n",
      "Losses {'ner': 1.3561913647665638}\n",
      "Losses {'ner': 1.3561913743005365}\n",
      "Losses {'ner': 1.4691578000203676}\n",
      "Losses {'ner': 1.4691578000275642}\n",
      "Losses {'ner': 1.4691578016530444}\n",
      "Losses {'ner': 1.469250241653746}\n",
      "Losses {'ner': 1.4692502416544047}\n",
      "Losses {'ner': 2.2301456741525265}\n",
      "Losses {'ner': 2.2301456741749526}\n",
      "Losses {'ner': 2.230145674177562}\n",
      "Losses {'ner': 2.2301456741775625}\n",
      "Losses {'ner': 1.3106617445965113e-10}\n",
      "Losses {'ner': 1.5310571195005844}\n",
      "Losses {'ner': 1.5310571195009728}\n",
      "Losses {'ner': 1.5310572543731402}\n",
      "Losses {'ner': 1.531057308526152}\n",
      "Losses {'ner': 3.1959390249667017}\n",
      "Losses {'ner': 3.195939024966998}\n",
      "Losses {'ner': 3.195939024967666}\n",
      "Losses {'ner': 3.195939048699004}\n",
      "Losses {'ner': 3.2242486505604755}\n",
      "Losses {'ner': 3.2242486505620254}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3.2242490870548437}\n",
      "Losses {'ner': 3.2242490874494836}\n",
      "Losses {'ner': 3.224249110326901}\n",
      "Losses {'ner': 3.613425657854223}\n",
      "Losses {'ner': 3.6134256578552795}\n",
      "Losses {'ner': 3.6134256580552786}\n",
      "Losses {'ner': 4.973241771829013}\n",
      "Losses {'ner': 4.973241771835484}\n",
      "Losses {'ner': 5.602521742967063}\n",
      "Losses {'ner': 5.60252174296711}\n",
      "Losses {'ner': 5.603065601854395}\n",
      "Losses {'ner': 5.733059794928814}\n",
      "Losses {'ner': 5.733059799573407}\n",
      "Losses {'ner': 5.733059799580036}\n",
      "Losses {'ner': 5.7330597995800465}\n",
      "Losses {'ner': 5.733059799580076}\n",
      "Losses {'ner': 3.789113916511299e-13}\n",
      "Losses {'ner': 1.2239666769132126e-08}\n",
      "Losses {'ner': 1.2251491054036673e-08}\n",
      "Losses {'ner': 1.2264777659164477e-08}\n",
      "Losses {'ner': 1.2266875550258427e-08}\n",
      "Losses {'ner': 2.0240326157048613e-08}\n",
      "Losses {'ner': 1.9482373348311561}\n",
      "Losses {'ner': 1.9482374418464508}\n",
      "Losses {'ner': 1.948237441846471}\n",
      "Losses {'ner': 1.948238762347465}\n",
      "Losses {'ner': 1.9482387623915753}\n",
      "Losses {'ner': 1.9482387746882481}\n",
      "Losses {'ner': 2.093550158372103}\n",
      "Losses {'ner': 2.0935569185750755}\n",
      "Losses {'ner': 2.0935569185783023}\n",
      "Losses {'ner': 4.00375401365484}\n",
      "Losses {'ner': 4.0037862026175315}\n",
      "Losses {'ner': 4.003786202619807}\n",
      "Losses {'ner': 4.016901732526042}\n",
      "Losses {'ner': 4.016920276370057}\n",
      "Losses {'ner': 4.016920276854667}\n",
      "Losses {'ner': 4.016920276855718}\n",
      "Losses {'ner': 4.016921135750064}\n",
      "Losses {'ner': 4.016921135750064}\n",
      "Losses {'ner': 6.422719149485631}\n",
      "Losses {'ner': 6.422719149815698}\n",
      "Losses {'ner': 6.422719149830036}\n",
      "Losses {'ner': 2.264247268173725e-08}\n",
      "Losses {'ner': 0.0002627931413497622}\n",
      "Losses {'ner': 0.0002627931413545855}\n",
      "Losses {'ner': 0.0002627931496372024}\n",
      "Losses {'ner': 0.03684926405889421}\n",
      "Losses {'ner': 0.03684953190843432}\n",
      "Losses {'ner': 0.0368495319090215}\n",
      "Losses {'ner': 0.03684953191293268}\n",
      "Losses {'ner': 0.03684953194565599}\n",
      "Losses {'ner': 0.036849532628090924}\n",
      "Losses {'ner': 0.03684953263101832}\n",
      "Losses {'ner': 0.036849536676046186}\n",
      "Losses {'ner': 0.03684953692045292}\n",
      "Losses {'ner': 0.036849536938088304}\n",
      "Losses {'ner': 0.03684953693809531}\n",
      "Losses {'ner': 0.036849536938095714}\n",
      "Losses {'ner': 0.03684953694922406}\n",
      "Losses {'ner': 0.036849536955993425}\n",
      "Losses {'ner': 0.03684955580543841}\n",
      "Losses {'ner': 0.03684955639550716}\n",
      "Losses {'ner': 0.036849556395507965}\n",
      "Losses {'ner': 0.03704146692481128}\n",
      "Losses {'ner': 0.0370507444060541}\n",
      "Losses {'ner': 0.037050744528424065}\n",
      "Losses {'ner': 0.037050744528536475}\n",
      "Losses {'ner': 0.037050750502983634}\n",
      "Losses {'ner': 0.03705075050299057}\n",
      "Losses {'ner': 6.372316545733286e-13}\n",
      "Losses {'ner': 8.085915538139293e-09}\n",
      "Losses {'ner': 8.112663570165847e-09}\n",
      "Losses {'ner': 8.821781043890992e-09}\n",
      "Losses {'ner': 0.00046041676860529644}\n",
      "Losses {'ner': 0.00046041676860556613}\n",
      "Losses {'ner': 0.00046041689116904666}\n",
      "Losses {'ner': 0.0004604168911702966}\n",
      "Losses {'ner': 0.04005664673766651}\n",
      "Losses {'ner': 0.04005664675048203}\n",
      "Losses {'ner': 0.04005665327762681}\n",
      "Losses {'ner': 0.040056653296979654}\n",
      "Losses {'ner': 0.04005665329703677}\n",
      "Losses {'ner': 0.040057310970952595}\n",
      "Losses {'ner': 0.04005731110400386}\n",
      "Losses {'ner': 0.044493647575842295}\n",
      "Losses {'ner': 0.04449364758104301}\n",
      "Losses {'ner': 0.044493647581055076}\n",
      "Losses {'ner': 0.044543522473339266}\n",
      "Losses {'ner': 0.044543522630179674}\n",
      "Losses {'ner': 0.044543522630352876}\n",
      "Losses {'ner': 0.04457970812491189}\n",
      "Losses {'ner': 0.044609938008036934}\n",
      "Losses {'ner': 0.044609938009172026}\n",
      "Losses {'ner': 0.044609938012740255}\n",
      "Losses {'ner': 0.044609938015325375}\n",
      "Losses {'ner': 0.044609938015325375}\n",
      "Losses {'ner': 2.613565803436294e-13}\n",
      "Losses {'ner': 0.22672525049128167}\n",
      "Losses {'ner': 0.22672525312328848}\n",
      "Losses {'ner': 0.22672525379976743}\n",
      "Losses {'ner': 0.22672525846854147}\n",
      "Losses {'ner': 0.22672525850303554}\n",
      "Losses {'ner': 0.22672525851390918}\n",
      "Losses {'ner': 0.22672525866657917}\n",
      "Losses {'ner': 0.226725258698028}\n",
      "Losses {'ner': 0.226725272061593}\n",
      "Losses {'ner': 0.2267253037006157}\n",
      "Losses {'ner': 0.22672693894301876}\n",
      "Losses {'ner': 0.22672708810139175}\n",
      "Losses {'ner': 0.22672711636218634}\n",
      "Losses {'ner': 0.2267271163635037}\n",
      "Losses {'ner': 0.22672711638764917}\n",
      "Losses {'ner': 0.2267271163876632}\n",
      "Losses {'ner': 0.226727116389424}\n",
      "Losses {'ner': 0.2267271163923086}\n",
      "Losses {'ner': 0.22672711639233226}\n",
      "Losses {'ner': 0.22672711880423685}\n",
      "Losses {'ner': 0.6960154459603454}\n",
      "Losses {'ner': 0.6960169418816228}\n",
      "Losses {'ner': 2.6957099781581766}\n",
      "Losses {'ner': 2.6957099781581784}\n",
      "Losses {'ner': 2.695709978158197}\n",
      "Losses {'ner': 2.6957099785195755}\n",
      "Losses {'ner': 4.483741743356966e-05}\n",
      "Losses {'ner': 4.484209967965643e-05}\n",
      "Losses {'ner': 0.034403380786919464}\n",
      "Losses {'ner': 0.034403380821404594}\n",
      "Losses {'ner': 0.03440379577386224}\n",
      "Losses {'ner': 0.03440379580186686}\n",
      "Losses {'ner': 0.03440380617979361}\n",
      "Losses {'ner': 0.034403806235052806}\n",
      "Losses {'ner': 0.03440380623626766}\n",
      "Losses {'ner': 0.03440383766950425}\n",
      "Losses {'ner': 0.03440388977014351}\n",
      "Losses {'ner': 0.03506828568313029}\n",
      "Losses {'ner': 0.035068285683130376}\n",
      "Losses {'ner': 0.035068288411697957}\n",
      "Losses {'ner': 0.03506828841304666}\n",
      "Losses {'ner': 0.035068507778659004}\n",
      "Losses {'ner': 0.035068507798600594}\n",
      "Losses {'ner': 0.035068507805326485}\n",
      "Losses {'ner': 0.038138093667148834}\n",
      "Losses {'ner': 0.9663150300870741}\n",
      "Losses {'ner': 0.9663150301005177}\n",
      "Losses {'ner': 0.9663213236144707}\n",
      "Losses {'ner': 0.9663213236193413}\n",
      "Losses {'ner': 0.966483422891584}\n",
      "Losses {'ner': 0.9664834228919679}\n",
      "Losses {'ner': 0.966483422895551}\n",
      "Losses {'ner': 0.966483422895551}\n",
      "Losses {'ner': 0.009733001031047399}\n",
      "Losses {'ner': 0.009733001031064914}\n",
      "Losses {'ner': 0.009733001031079772}\n",
      "Losses {'ner': 0.009733001046181656}\n",
      "Losses {'ner': 0.2772663629579609}\n",
      "Losses {'ner': 0.27726655399522027}\n",
      "Losses {'ner': 0.2865990202908351}\n",
      "Losses {'ner': 2.2712343639990635}\n",
      "Losses {'ner': 2.30657303526027}\n",
      "Losses {'ner': 2.3067054232009965}\n",
      "Losses {'ner': 2.3067054232012922}\n",
      "Losses {'ner': 5.815763302649252}\n",
      "Losses {'ner': 5.815763302649477}\n",
      "Losses {'ner': 5.8157633026494775}\n",
      "Losses {'ner': 5.8157633026494775}\n",
      "Losses {'ner': 5.815763302649484}\n",
      "Losses {'ner': 5.815763334577429}\n",
      "Losses {'ner': 5.815763334577738}\n",
      "Losses {'ner': 10.72832353668553}\n",
      "Losses {'ner': 10.728324075257046}\n",
      "Losses {'ner': 12.706022402657464}\n",
      "Losses {'ner': 12.70602240771133}\n",
      "Losses {'ner': 12.70616596858235}\n",
      "Losses {'ner': 12.70616596858235}\n",
      "Losses {'ner': 12.711603539079338}\n",
      "Losses {'ner': 12.71169048376061}\n",
      "Losses {'ner': 12.71169048376061}\n",
      "Losses {'ner': 3.4559624948444344e-11}\n",
      "Losses {'ner': 0.006487867829492399}\n",
      "Losses {'ner': 0.006487867829612697}\n",
      "Losses {'ner': 0.00648786782962712}\n",
      "Losses {'ner': 0.006487867829627536}\n",
      "Losses {'ner': 0.006488659534975717}\n",
      "Losses {'ner': 0.006488659534990858}\n",
      "Losses {'ner': 0.006488659545167557}\n",
      "Losses {'ner': 0.006488847968667038}\n",
      "Losses {'ner': 0.006488847968687455}\n",
      "Losses {'ner': 0.00651718066130566}\n",
      "Losses {'ner': 1.80870676920807}\n",
      "Losses {'ner': 1.8087067692111027}\n",
      "Losses {'ner': 1.8087067692111234}\n",
      "Losses {'ner': 1.8087067692111236}\n",
      "Losses {'ner': 1.808741776004733}\n",
      "Losses {'ner': 1.8087417912057526}\n",
      "Losses {'ner': 1.8087480531480817}\n",
      "Losses {'ner': 1.8087480531480908}\n",
      "Losses {'ner': 1.8088257690200904}\n",
      "Losses {'ner': 1.80882576909338}\n",
      "Losses {'ner': 3.8088257690933833}\n",
      "Losses {'ner': 3.808825769093436}\n",
      "Losses {'ner': 3.8088257690934935}\n",
      "Losses {'ner': 3.80882576910137}\n",
      "Losses {'ner': 3.8088257691712304}\n",
      "Losses {'ner': 4.141006060585491}\n",
      "Losses {'ner': 9.82955825748055e-05}\n",
      "Losses {'ner': 9.829660917454373e-05}\n",
      "Losses {'ner': 9.829668003387125e-05}\n",
      "Losses {'ner': 2.0000989163918153}\n",
      "Losses {'ner': 2.0000989163936818}\n",
      "Losses {'ner': 2.0009823657217454}\n",
      "Losses {'ner': 2.000982365721935}\n",
      "Losses {'ner': 2.000982365739257}\n",
      "Losses {'ner': 2.000982365739258}\n",
      "Losses {'ner': 2.0009823657393087}\n",
      "Losses {'ner': 2.0009823657408505}\n",
      "Losses {'ner': 2.000982365740965}\n",
      "Losses {'ner': 2.000982740938795}\n",
      "Losses {'ner': 2.0009827409394787}\n",
      "Losses {'ner': 2.0010100380636024}\n",
      "Losses {'ner': 2.0010100380651648}\n",
      "Losses {'ner': 2.001011004271302}\n",
      "Losses {'ner': 2.0010110042713065}\n",
      "Losses {'ner': 2.001136873239598}\n",
      "Losses {'ner': 2.026876580292372}\n",
      "Losses {'ner': 2.0268765856163533}\n",
      "Losses {'ner': 2.0268765856229094}\n",
      "Losses {'ner': 2.026876585626}\n",
      "Losses {'ner': 2.026876585626003}\n",
      "Losses {'ner': 2.026876586001684}\n",
      "Losses {'ner': 2.026876586001703}\n",
      "Losses {'ner': 2.026876586001703}\n",
      "Losses {'ner': 1.5320033686979405e-15}\n",
      "Losses {'ner': 4.162640553119043e-09}\n",
      "Losses {'ner': 4.162648383438182e-09}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 4.162668371324447e-09}\n",
      "Losses {'ner': 4.849752169974724e-06}\n",
      "Losses {'ner': 4.849756044635103e-06}\n",
      "Losses {'ner': 4.947704707011078e-06}\n",
      "Losses {'ner': 4.947771429317107e-06}\n",
      "Losses {'ner': 4.947847870941519e-06}\n",
      "Losses {'ner': 0.004868930089655166}\n",
      "Losses {'ner': 0.028918338823510158}\n",
      "Losses {'ner': 0.028918338835539903}\n",
      "Losses {'ner': 0.02891877039270857}\n",
      "Losses {'ner': 0.028918770401572234}\n",
      "Losses {'ner': 0.028921458061585737}\n",
      "Losses {'ner': 0.028921458061589213}\n",
      "Losses {'ner': 0.028921458061610898}\n",
      "Losses {'ner': 0.028921458484680782}\n",
      "Losses {'ner': 0.02892520166909522}\n",
      "Losses {'ner': 0.028925201784817806}\n",
      "Losses {'ner': 0.02892520345569317}\n",
      "Losses {'ner': 0.028925203455934263}\n",
      "Losses {'ner': 0.02892520347223736}\n",
      "Losses {'ner': 0.4082980005298886}\n",
      "Losses {'ner': 0.4082980005303716}\n",
      "Losses {'ner': 0.4082980005303716}\n",
      "Losses {'ner': 0.40829800053037196}\n",
      "Losses {'ner': 3.916536351985578e-13}\n",
      "Losses {'ner': 2.1062748098564329e-10}\n",
      "Losses {'ner': 2.1062748114357856e-10}\n",
      "Losses {'ner': 4.320157060742817e-09}\n",
      "Losses {'ner': 4.320462150971755e-09}\n",
      "Losses {'ner': 4.3204626160147454e-09}\n",
      "Losses {'ner': 4.3208856952097245e-09}\n",
      "Losses {'ner': 4.320885891844714e-09}\n",
      "Losses {'ner': 2.5046413261129352e-06}\n",
      "Losses {'ner': 0.003056211348377845}\n",
      "Losses {'ner': 0.0030578630124896084}\n",
      "Losses {'ner': 0.00324359017478845}\n",
      "Losses {'ner': 0.003243590174788465}\n",
      "Losses {'ner': 0.0032437020598439165}\n",
      "Losses {'ner': 0.003243702059857685}\n",
      "Losses {'ner': 0.003243702061660936}\n",
      "Losses {'ner': 0.0032437020625154258}\n",
      "Losses {'ner': 0.004699253192282973}\n",
      "Losses {'ner': 0.004699253204562503}\n",
      "Losses {'ner': 0.004699253204562602}\n",
      "Losses {'ner': 0.004699253205392317}\n",
      "Losses {'ner': 0.004699253208119772}\n",
      "Losses {'ner': 0.0046992532081506135}\n",
      "Losses {'ner': 0.004699253208182482}\n",
      "Losses {'ner': 0.004699253404458002}\n",
      "Losses {'ner': 0.0046992534049147185}\n",
      "Losses {'ner': 0.0046992534049163205}\n",
      "Losses {'ner': 1.3915872151903015e-16}\n",
      "Losses {'ner': 1.7183832381941103e-10}\n",
      "Losses {'ner': 1.7183833003865684e-10}\n",
      "Losses {'ner': 1.7183840795122303e-10}\n",
      "Losses {'ner': 1.7184491389770214e-10}\n",
      "Losses {'ner': 3.0616943462897725e-09}\n",
      "Losses {'ner': 3.0628840941424894e-09}\n",
      "Losses {'ner': 3.0628917570876744e-09}\n",
      "Losses {'ner': 0.11437928237852753}\n",
      "Losses {'ner': 0.11437928364115439}\n",
      "Losses {'ner': 0.12844926561001416}\n",
      "Losses {'ner': 0.1284492656100144}\n",
      "Losses {'ner': 0.41750433995918657}\n",
      "Losses {'ner': 0.4175043399591967}\n",
      "Losses {'ner': 0.4175043399591967}\n",
      "Losses {'ner': 0.41750434010103776}\n",
      "Losses {'ner': 0.4175043401666256}\n",
      "Losses {'ner': 0.41750434016663285}\n",
      "Losses {'ner': 0.5471670336754793}\n",
      "Losses {'ner': 0.5471670337021155}\n",
      "Losses {'ner': 0.5471670337021202}\n",
      "Losses {'ner': 0.5471670355495686}\n",
      "Losses {'ner': 0.5471670537755903}\n",
      "Losses {'ner': 0.5471670537756079}\n",
      "Losses {'ner': 0.5471670537756094}\n",
      "Losses {'ner': 2.552320744822499}\n",
      "Losses {'ner': 2.552320744822499}\n",
      "Losses {'ner': 1.4505779805873858e-09}\n",
      "Losses {'ner': 2.9329732034731968e-05}\n",
      "Losses {'ner': 2.9329732823402794e-05}\n",
      "Losses {'ner': 2.9329732846716325e-05}\n",
      "Losses {'ner': 2.000010733106261}\n",
      "Losses {'ner': 2.00001132423171}\n",
      "Losses {'ner': 2.0000113242321396}\n",
      "Losses {'ner': 2.0001294346120195}\n",
      "Losses {'ner': 2.0006910058600185}\n",
      "Losses {'ner': 2.000691005860063}\n",
      "Losses {'ner': 2.0221581412060297}\n",
      "Losses {'ner': 2.0221609151972886}\n",
      "Losses {'ner': 2.02216091635249}\n",
      "Losses {'ner': 2.022160963936397}\n",
      "Losses {'ner': 2.0221609639395774}\n",
      "Losses {'ner': 2.022161017221469}\n",
      "Losses {'ner': 2.022161017223309}\n",
      "Losses {'ner': 2.022168899317961}\n",
      "Losses {'ner': 2.022168899675141}\n",
      "Losses {'ner': 2.0221688996762404}\n",
      "Losses {'ner': 2.022168899676242}\n",
      "Losses {'ner': 2.0932487871263286}\n",
      "Losses {'ner': 2.093248787126342}\n",
      "Losses {'ner': 2.093509109044519}\n",
      "Losses {'ner': 2.093509109044556}\n",
      "Losses {'ner': 2.093509109044559}\n",
      "Losses {'ner': 2.093509109044559}\n",
      "Losses {'ner': 4.808473077978608e-16}\n",
      "Losses {'ner': 4.844703103224971e-16}\n",
      "Losses {'ner': 5.384234323592615e-16}\n",
      "Losses {'ner': 2.8158324025316385e-14}\n",
      "Losses {'ner': 8.700096303801642e-12}\n",
      "Losses {'ner': 8.727596129484068e-12}\n",
      "Losses {'ner': 5.367574705530913e-07}\n",
      "Losses {'ner': 5.367654118599891e-07}\n",
      "Losses {'ner': 0.019986970674058198}\n",
      "Losses {'ner': 0.019986970674566836}\n",
      "Losses {'ner': 0.01998697067709149}\n",
      "Losses {'ner': 0.019986983530888708}\n",
      "Losses {'ner': 0.019986983531948693}\n",
      "Losses {'ner': 2.0146672000532604}\n",
      "Losses {'ner': 2.0146672013136167}\n",
      "Losses {'ner': 2.014667205347462}\n",
      "Losses {'ner': 2.014667205358398}\n",
      "Losses {'ner': 2.0146672053809076}\n",
      "Losses {'ner': 2.014667205611647}\n",
      "Losses {'ner': 2.014667205926892}\n",
      "Losses {'ner': 2.0146672304388877}\n",
      "Losses {'ner': 2.014667230687136}\n",
      "Losses {'ner': 2.0146672306945987}\n",
      "Losses {'ner': 2.0147503667758286}\n",
      "Losses {'ner': 2.015708239376985}\n",
      "Losses {'ner': 2.0157082393788777}\n",
      "Losses {'ner': 2.0157087612245324}\n",
      "Losses {'ner': 5.876905525437143e-17}\n",
      "Losses {'ner': 3.6586919343462646e-15}\n",
      "Losses {'ner': 1.1784511073309314e-13}\n",
      "Losses {'ner': 1.2813462739006801e-08}\n",
      "Losses {'ner': 1.2813576590141045e-08}\n",
      "Losses {'ner': 1.5356432222478763e-07}\n",
      "Losses {'ner': 1.5357356977964835e-07}\n",
      "Losses {'ner': 1.5358887649596306e-07}\n",
      "Losses {'ner': 4.6154675652544835e-06}\n",
      "Losses {'ner': 4.658920649971481e-06}\n",
      "Losses {'ner': 4.671884533972802e-06}\n",
      "Losses {'ner': 4.6718899621664105e-06}\n",
      "Losses {'ner': 4.672649517450936e-06}\n",
      "Losses {'ner': 4.6726495750532164e-06}\n",
      "Losses {'ner': 3.996463416609867}\n",
      "Losses {'ner': 3.9967402032226076}\n",
      "Losses {'ner': 5.4654011539127865}\n",
      "Losses {'ner': 5.465401321385066}\n",
      "Losses {'ner': 5.465401321385077}\n",
      "Losses {'ner': 5.465401321385104}\n",
      "Losses {'ner': 5.514810770485917}\n",
      "Losses {'ner': 5.514815028968967}\n",
      "Losses {'ner': 5.514815028978152}\n",
      "Losses {'ner': 5.514815046591551}\n",
      "Losses {'ner': 5.514815046593448}\n",
      "Losses {'ner': 5.51942647520296}\n",
      "Losses {'ner': 5.51942647520296}\n",
      "Losses {'ner': 9.446988277810829e-10}\n",
      "Losses {'ner': 1.9891791354277624}\n",
      "Losses {'ner': 1.9891791359504891}\n",
      "Losses {'ner': 1.9891791361957376}\n",
      "Losses {'ner': 1.9891791388468638}\n",
      "Losses {'ner': 1.9891792674557316}\n",
      "Losses {'ner': 1.989180687407333}\n",
      "Losses {'ner': 1.989180704563199}\n",
      "Losses {'ner': 1.9891807045632213}\n",
      "Losses {'ner': 1.9891848842146564}\n",
      "Losses {'ner': 1.989184921377462}\n",
      "Losses {'ner': 1.989184921377689}\n",
      "Losses {'ner': 1.9891855825361262}\n",
      "Losses {'ner': 1.989185582540954}\n",
      "Losses {'ner': 2.0400524173608034}\n",
      "Losses {'ner': 2.040053176649161}\n",
      "Losses {'ner': 2.040053176649239}\n",
      "Losses {'ner': 2.0465096567359686}\n",
      "Losses {'ner': 2.046509656736152}\n",
      "Losses {'ner': 2.0465096567361583}\n",
      "Losses {'ner': 2.046509663756799}\n",
      "Losses {'ner': 2.047530083300112}\n",
      "Losses {'ner': 2.0475300833151184}\n",
      "Losses {'ner': 2.0475382268368847}\n",
      "Losses {'ner': 2.047866777687072}\n",
      "Losses {'ner': 2.047866777687072}\n",
      "Losses {'ner': 2.047866777687072}\n",
      "Losses {'ner': 2.152426386518026e-09}\n",
      "Losses {'ner': 2.1524300708660398e-09}\n",
      "Losses {'ner': 0.00029099633560567554}\n",
      "Losses {'ner': 0.0002909963432946149}\n",
      "Losses {'ner': 0.00029100434158638416}\n",
      "Losses {'ner': 0.0002910241512543258}\n",
      "Losses {'ner': 0.0002910242622770229}\n",
      "Losses {'ner': 0.00029102426227747553}\n",
      "Losses {'ner': 0.9108657629282559}\n",
      "Losses {'ner': 0.9108657629284618}\n",
      "Losses {'ner': 0.9108657768558464}\n",
      "Losses {'ner': 0.9108657768567237}\n",
      "Losses {'ner': 0.9108658136641739}\n",
      "Losses {'ner': 0.9108658137074209}\n",
      "Losses {'ner': 0.9108658137226034}\n",
      "Losses {'ner': 0.9108658138816708}\n",
      "Losses {'ner': 0.9108658898763664}\n",
      "Losses {'ner': 0.9113799167352136}\n",
      "Losses {'ner': 0.9113799175186097}\n",
      "Losses {'ner': 0.9113799175195179}\n",
      "Losses {'ner': 0.9113799218405932}\n",
      "Losses {'ner': 0.9113799218407861}\n",
      "Losses {'ner': 0.9113799644418626}\n",
      "Losses {'ner': 0.9114233267424084}\n",
      "Losses {'ner': 0.9114233270622647}\n",
      "Losses {'ner': 0.9114233270622647}\n",
      "Losses {'ner': 0.9114233270622647}\n",
      "Losses {'ner': 3.962743608905248e-09}\n",
      "Losses {'ner': 4.161539503781385e-09}\n",
      "Losses {'ner': 4.829557744054479e-09}\n",
      "Losses {'ner': 5.811703463240943e-09}\n",
      "Losses {'ner': 3.361131321171033e-08}\n",
      "Losses {'ner': 3.3611318088484084e-08}\n",
      "Losses {'ner': 3.3611318845717774e-08}\n",
      "Losses {'ner': 3.3622868776617385e-08}\n",
      "Losses {'ner': 3.6882856430024705e-08}\n",
      "Losses {'ner': 1.8347324758820327e-06}\n",
      "Losses {'ner': 1.8349843462684325e-06}\n",
      "Losses {'ner': 3.431355439556668e-06}\n",
      "Losses {'ner': 3.4313554397736538e-06}\n",
      "Losses {'ner': 3.4313556240288616e-06}\n",
      "Losses {'ner': 5.496911399912566e-06}\n",
      "Losses {'ner': 5.496911400947813e-06}\n",
      "Losses {'ner': 5.4969175268061535e-06}\n",
      "Losses {'ner': 5.4970309347703975e-06}\n",
      "Losses {'ner': 5.936055168852625e-06}\n",
      "Losses {'ner': 5.9360551690607e-06}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 6.8257033901878985e-06}\n",
      "Losses {'ner': 6.825703892470601e-06}\n",
      "Losses {'ner': 3.3083125660609847}\n",
      "Losses {'ner': 3.308312582929375}\n",
      "Losses {'ner': 3.3083132958168497}\n",
      "Losses {'ner': 3.3083132958169443}\n",
      "Losses {'ner': 3.3083132958169443}\n",
      "Losses {'ner': 1.7889716741515065}\n",
      "Losses {'ner': 1.7889716741515105}\n",
      "Losses {'ner': 1.7889716741515107}\n",
      "Losses {'ner': 1.7889716745193378}\n",
      "Losses {'ner': 1.7889716745211588}\n",
      "Losses {'ner': 1.7889716759314502}\n",
      "Losses {'ner': 3.684248699247766}\n",
      "Losses {'ner': 3.6842486992477976}\n",
      "Losses {'ner': 3.684248699344116}\n",
      "Losses {'ner': 3.6842487052666453}\n",
      "Losses {'ner': 3.6842487669804354}\n",
      "Losses {'ner': 3.684248766981375}\n",
      "Losses {'ner': 3.6842488245318017}\n",
      "Losses {'ner': 6.471598883121391}\n",
      "Losses {'ner': 6.471598883121407}\n",
      "Losses {'ner': 6.471609101254371}\n",
      "Losses {'ner': 6.471609101254375}\n",
      "Losses {'ner': 6.471609101254878}\n",
      "Losses {'ner': 6.471609148175927}\n",
      "Losses {'ner': 6.471609148177564}\n",
      "Losses {'ner': 7.1990922228483525}\n",
      "Losses {'ner': 7.199092222848492}\n",
      "Losses {'ner': 7.199092222868088}\n",
      "Losses {'ner': 7.199376114691359}\n",
      "Losses {'ner': 7.725574245411927}\n",
      "Losses {'ner': 7.725574245714813}\n",
      "Losses {'ner': 11.64516006972757}\n",
      "Losses {'ner': 4.841442984757536e-12}\n",
      "Losses {'ner': 2.9659839578200107e-10}\n",
      "Losses {'ner': 8.477981698593335e-10}\n",
      "Losses {'ner': 8.477982616359303e-10}\n",
      "Losses {'ner': 1.5755654470900348e-09}\n",
      "Losses {'ner': 1.1982169320117129}\n",
      "Losses {'ner': 1.198216932011722}\n",
      "Losses {'ner': 1.1982169367873803}\n",
      "Losses {'ner': 3.1913708668127168}\n",
      "Losses {'ner': 3.191370867277496}\n",
      "Losses {'ner': 3.1913708678804333}\n",
      "Losses {'ner': 3.191473305812693}\n",
      "Losses {'ner': 3.191473305915881}\n",
      "Losses {'ner': 3.1915866552655885}\n",
      "Losses {'ner': 5.004842814636376}\n",
      "Losses {'ner': 5.004845772009702}\n",
      "Losses {'ner': 5.004846026919134}\n",
      "Losses {'ner': 5.004846026919155}\n",
      "Losses {'ner': 5.0048460269194655}\n",
      "Losses {'ner': 5.004846240932821}\n",
      "Losses {'ner': 5.043382809803789}\n",
      "Losses {'ner': 5.04338280980415}\n",
      "Losses {'ner': 5.043382809804286}\n",
      "Losses {'ner': 5.043382817902813}\n",
      "Losses {'ner': 5.043382817903673}\n",
      "Losses {'ner': 5.0433828680592425}\n",
      "Losses {'ner': 5.043382868271972}\n",
      "Losses {'ner': 4.241892916386008e-15}\n",
      "Losses {'ner': 3.228313615807153e-10}\n",
      "Losses {'ner': 3.2305957819311986e-10}\n",
      "Losses {'ner': 3.2375241421595654e-10}\n",
      "Losses {'ner': 1.1068954182933535}\n",
      "Losses {'ner': 1.1068955167304777}\n",
      "Losses {'ner': 3.1043607780971008}\n",
      "Losses {'ner': 3.1043607781909075}\n",
      "Losses {'ner': 3.1043607781922447}\n",
      "Losses {'ner': 3.1043607781922504}\n",
      "Losses {'ner': 3.1043607782735605}\n",
      "Losses {'ner': 5.041018207512911}\n",
      "Losses {'ner': 5.041018207513366}\n",
      "Losses {'ner': 5.04101820764256}\n",
      "Losses {'ner': 6.977029532294106}\n",
      "Losses {'ner': 6.977029533067985}\n",
      "Losses {'ner': 6.977029533511863}\n",
      "Losses {'ner': 6.977029533514369}\n",
      "Losses {'ner': 6.977029533514375}\n",
      "Losses {'ner': 6.977029534655543}\n",
      "Losses {'ner': 6.977746688803032}\n",
      "Losses {'ner': 10.931435311091292}\n",
      "Losses {'ner': 11.171369313478719}\n",
      "Losses {'ner': 11.171706299339206}\n",
      "Losses {'ner': 11.171706299339313}\n",
      "Losses {'ner': 11.225600707386834}\n",
      "Losses {'ner': 11.225600707386922}\n",
      "Losses {'ner': 5.849341331579323e-11}\n",
      "Losses {'ner': 1.16029931705414e-09}\n",
      "Losses {'ner': 1.171110353068952e-09}\n",
      "Losses {'ner': 1.2081852758333087e-09}\n",
      "Losses {'ner': 1.7630844679940443e-05}\n",
      "Losses {'ner': 1.7686194637853242e-05}\n",
      "Losses {'ner': 1.768764461207577e-05}\n",
      "Losses {'ner': 1.768764482095687e-05}\n",
      "Losses {'ner': 2.685552240463261e-05}\n",
      "Losses {'ner': 2.6855527488250656e-05}\n",
      "Losses {'ner': 2.6855534786942533e-05}\n",
      "Losses {'ner': 2.6855534888687924e-05}\n",
      "Losses {'ner': 2.6855534902062442e-05}\n",
      "Losses {'ner': 2.6855898372270495e-05}\n",
      "Losses {'ner': 2.6855899368059887e-05}\n",
      "Losses {'ner': 2.6855900412829637e-05}\n",
      "Losses {'ner': 0.18363217563665218}\n",
      "Losses {'ner': 0.18363217563696757}\n",
      "Losses {'ner': 0.18363217563697604}\n",
      "Losses {'ner': 0.18363217938911316}\n",
      "Losses {'ner': 0.18365576514878107}\n",
      "Losses {'ner': 0.18366427011610748}\n",
      "Losses {'ner': 2.1763122386841087}\n",
      "Losses {'ner': 2.176363268082388}\n",
      "Losses {'ner': 2.1763632680823957}\n",
      "Losses {'ner': 2.1763632849257957}\n",
      "Losses {'ner': 2.176363284925799}\n",
      "Losses {'ner': 1.304832755624594e-06}\n",
      "Losses {'ner': 1.3061360162542895e-06}\n",
      "Losses {'ner': 1.3061396462138565e-06}\n",
      "Losses {'ner': 1.3245232999746218e-06}\n",
      "Losses {'ner': 1.3248992687808787e-06}\n",
      "Losses {'ner': 1.4308649437499767}\n",
      "Losses {'ner': 1.430865089512884}\n",
      "Losses {'ner': 1.4308650895128872}\n",
      "Losses {'ner': 1.430865089512892}\n",
      "Losses {'ner': 1.4308650898171515}\n",
      "Losses {'ner': 1.4308650934292668}\n",
      "Losses {'ner': 1.4308650934328524}\n",
      "Losses {'ner': 1.430865093447411}\n",
      "Losses {'ner': 1.4308650934558276}\n",
      "Losses {'ner': 1.4308650934635845}\n",
      "Losses {'ner': 1.430865093463607}\n",
      "Losses {'ner': 1.4308651640128864}\n",
      "Losses {'ner': 1.4308651640128864}\n",
      "Losses {'ner': 1.4308651650415187}\n",
      "Losses {'ner': 1.4308651650633764}\n",
      "Losses {'ner': 1.4308651650633901}\n",
      "Losses {'ner': 1.430865165063455}\n",
      "Losses {'ner': 1.4308652998847138}\n",
      "Losses {'ner': 3.4268968850378587}\n",
      "Losses {'ner': 3.4268973661307083}\n",
      "Losses {'ner': 3.4272666023178764}\n",
      "Losses {'ner': 3.4272666023178764}\n",
      "Losses {'ner': 4.8838146519473076e-17}\n",
      "Losses {'ner': 9.259524260560757e-08}\n",
      "Losses {'ner': 9.259598878862869e-08}\n",
      "Losses {'ner': 9.324641758105134e-08}\n",
      "Losses {'ner': 9.324641758131567e-08}\n",
      "Losses {'ner': 9.324834497486097e-08}\n",
      "Losses {'ner': 9.449228818955205e-08}\n",
      "Losses {'ner': 3.1206741280460237e-06}\n",
      "Losses {'ner': 3.1207719308801937e-06}\n",
      "Losses {'ner': 3.1207721752117076e-06}\n",
      "Losses {'ner': 3.7587517894694865e-06}\n",
      "Losses {'ner': 3.758764212709776e-06}\n",
      "Losses {'ner': 1.9998940867196597}\n",
      "Losses {'ner': 1.9998940867209183}\n",
      "Losses {'ner': 1.9998940867210333}\n",
      "Losses {'ner': 2.031989899830111}\n",
      "Losses {'ner': 2.1906684165756802}\n",
      "Losses {'ner': 2.1906684165757038}\n",
      "Losses {'ner': 2.2077294805955066}\n",
      "Losses {'ner': 2.2078505924605096}\n",
      "Losses {'ner': 2.2078532604219467}\n",
      "Losses {'ner': 2.2078932722954048}\n",
      "Losses {'ner': 2.2082253769196694}\n",
      "Losses {'ner': 2.208225376938502}\n",
      "Losses {'ner': 2.2082253771203892}\n",
      "Losses {'ner': 2.2082366151222197}\n",
      "Losses {'ner': 2.208236615345888}\n",
      "Losses {'ner': 1.8829612731933756}\n",
      "Losses {'ner': 1.882961273203196}\n",
      "Losses {'ner': 1.882961278743132}\n",
      "Losses {'ner': 1.882961278743132}\n",
      "Losses {'ner': 1.8829612787447296}\n",
      "Losses {'ner': 1.8829612788487409}\n",
      "Losses {'ner': 1.8829612789835897}\n",
      "Losses {'ner': 1.8829612789857106}\n",
      "Losses {'ner': 1.8829612790578876}\n",
      "Losses {'ner': 1.8829612790578916}\n",
      "Losses {'ner': 1.882961382247442}\n",
      "Losses {'ner': 1.88296138224764}\n",
      "Losses {'ner': 1.8829613822477653}\n",
      "Losses {'ner': 1.8830039712256106}\n",
      "Losses {'ner': 1.8832073855682323}\n",
      "Losses {'ner': 1.8832073855682347}\n",
      "Losses {'ner': 1.883207385568378}\n",
      "Losses {'ner': 1.883207385570026}\n",
      "Losses {'ner': 3.8819938350024183}\n",
      "Losses {'ner': 3.8819938475155573}\n",
      "Losses {'ner': 3.8819938477473106}\n",
      "Losses {'ner': 3.881993850534554}\n",
      "Losses {'ner': 3.881993850535018}\n",
      "Losses {'ner': 3.881993868897738}\n",
      "Losses {'ner': 3.8819942737416127}\n",
      "Losses {'ner': 3.8819942737416127}\n",
      "Losses {'ner': 3.8819971145305785}\n",
      "Losses {'ner': 1.187921419584273e-11}\n",
      "Losses {'ner': 3.604995838653297e-07}\n",
      "Losses {'ner': 3.6051282739963303e-07}\n",
      "Losses {'ner': 0.540084888772479}\n",
      "Losses {'ner': 0.540084888777536}\n",
      "Losses {'ner': 0.5400848889732704}\n",
      "Losses {'ner': 0.5400849007313572}\n",
      "Losses {'ner': 0.5400849007359199}\n",
      "Losses {'ner': 0.5400849168968219}\n",
      "Losses {'ner': 0.5401141815910244}\n",
      "Losses {'ner': 0.5401141815980565}\n",
      "Losses {'ner': 0.9354734593232769}\n",
      "Losses {'ner': 0.935473463617352}\n",
      "Losses {'ner': 0.935473463617352}\n",
      "Losses {'ner': 0.9354745342746329}\n",
      "Losses {'ner': 0.9354745342746464}\n",
      "Losses {'ner': 0.9354754599431199}\n",
      "Losses {'ner': 0.9354754599431201}\n",
      "Losses {'ner': 0.9354754599431231}\n",
      "Losses {'ner': 0.9354755503296791}\n",
      "Losses {'ner': 0.9354755506701564}\n",
      "Losses {'ner': 0.9354755531389045}\n",
      "Losses {'ner': 0.9354756410730494}\n",
      "Losses {'ner': 0.9354756418314156}\n",
      "Losses {'ner': 0.9354756418314646}\n",
      "Losses {'ner': 0.9354756418314693}\n",
      "Losses {'ner': 0.9354756418314709}\n",
      "Losses {'ner': 8.506712803803801e-14}\n",
      "Losses {'ner': 4.4091507085478606e-11}\n",
      "Losses {'ner': 2.583728232059051e-07}\n",
      "Losses {'ner': 2.583728238356099e-07}\n",
      "Losses {'ner': 2.583743275830577e-07}\n",
      "Losses {'ner': 0.5296807200553212}\n",
      "Losses {'ner': 0.5296811601146616}\n",
      "Losses {'ner': 0.5296811601149581}\n",
      "Losses {'ner': 0.5296811601149609}\n",
      "Losses {'ner': 0.5296811601152984}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 0.5296811601154977}\n",
      "Losses {'ner': 0.5296811601155708}\n",
      "Losses {'ner': 0.5296811601723637}\n",
      "Losses {'ner': 0.5296830081932001}\n",
      "Losses {'ner': 0.5296830081933435}\n",
      "Losses {'ner': 0.5296830081933638}\n",
      "Losses {'ner': 0.5296830152657253}\n",
      "Losses {'ner': 0.5296830152657255}\n",
      "Losses {'ner': 0.5900390212921451}\n",
      "Losses {'ner': 0.5900609083465431}\n",
      "Losses {'ner': 0.5900611379689745}\n",
      "Losses {'ner': 0.5900754626669605}\n",
      "Losses {'ner': 0.5900754626669612}\n",
      "Losses {'ner': 0.5900754626669636}\n",
      "Losses {'ner': 0.5900754626794327}\n",
      "Losses {'ner': 0.5900754626796905}\n",
      "Losses {'ner': 0.5900754627316347}\n",
      "Losses {'ner': 2.1064149404612184e-08}\n",
      "Losses {'ner': 2.106420863963446e-08}\n",
      "Losses {'ner': 2.1466258120788302e-08}\n",
      "Losses {'ner': 0.009682403923218189}\n",
      "Losses {'ner': 0.4465838242780614}\n",
      "Losses {'ner': 0.44658382427833787}\n",
      "Losses {'ner': 0.4465838407225749}\n",
      "Losses {'ner': 0.4465838407225749}\n",
      "Losses {'ner': 0.44658384072257495}\n",
      "Losses {'ner': 0.44658384072259766}\n",
      "Losses {'ner': 0.4465838407315136}\n",
      "Losses {'ner': 0.44658390984891905}\n",
      "Losses {'ner': 0.44658390984896407}\n",
      "Losses {'ner': 0.44700814478167045}\n",
      "Losses {'ner': 0.4470312004695303}\n",
      "Losses {'ner': 0.4470410023096587}\n",
      "Losses {'ner': 0.44705132793852426}\n",
      "Losses {'ner': 0.4470513279385281}\n",
      "Losses {'ner': 0.4470513285762556}\n",
      "Losses {'ner': 0.44705132857784524}\n",
      "Losses {'ner': 0.447051432861105}\n",
      "Losses {'ner': 0.4470514338788475}\n",
      "Losses {'ner': 0.44800795747451233}\n",
      "Losses {'ner': 0.44800798675965503}\n",
      "Losses {'ner': 0.4480080057787985}\n",
      "Losses {'ner': 0.448008466405572}\n",
      "Losses {'ner': 0.448008466405572}\n",
      "Losses {'ner': 3.643256747251082e-07}\n",
      "Losses {'ner': 3.6432805738960564e-07}\n",
      "Losses {'ner': 3.643810316709996e-07}\n",
      "Losses {'ner': 3.643810527195145e-07}\n",
      "Losses {'ner': 1.1515824297620404e-06}\n",
      "Losses {'ner': 1.1521029018665035e-06}\n",
      "Losses {'ner': 1.1521647059944473e-06}\n",
      "Losses {'ner': 0.1063442258760177}\n",
      "Losses {'ner': 0.10634422587604117}\n",
      "Losses {'ner': 0.10634422589878581}\n",
      "Losses {'ner': 0.10634422597537958}\n",
      "Losses {'ner': 0.10634422597538652}\n",
      "Losses {'ner': 0.1063442259762007}\n",
      "Losses {'ner': 0.6061680058996314}\n",
      "Losses {'ner': 1.791211360309279}\n",
      "Losses {'ner': 1.7912118107849784}\n",
      "Losses {'ner': 1.7912123042649042}\n",
      "Losses {'ner': 1.791221252826899}\n",
      "Losses {'ner': 1.791221252826918}\n",
      "Losses {'ner': 1.79122125282692}\n",
      "Losses {'ner': 1.7912212535308794}\n",
      "Losses {'ner': 1.7912230247952394}\n",
      "Losses {'ner': 1.7912230327252636}\n",
      "Losses {'ner': 1.7912230424161069}\n",
      "Losses {'ner': 1.7912230424175448}\n",
      "Losses {'ner': 1.7912250562705831}\n",
      "Losses {'ner': 1.7912250562705925}\n",
      "Losses {'ner': 1.0146684521162451e-14}\n",
      "Losses {'ner': 3.161018305800698e-10}\n",
      "Losses {'ner': 2.9726744858606048e-08}\n",
      "Losses {'ner': 2.9752995484949107e-08}\n",
      "Losses {'ner': 2.988357601097297e-08}\n",
      "Losses {'ner': 7.805621816161105e-08}\n",
      "Losses {'ner': 4.140216987700479e-06}\n",
      "Losses {'ner': 4.3038528325609675e-06}\n",
      "Losses {'ner': 2.9022201199964334e-05}\n",
      "Losses {'ner': 0.09862683743333216}\n",
      "Losses {'ner': 0.09862684424942848}\n",
      "Losses {'ner': 0.0986268770713052}\n",
      "Losses {'ner': 0.09862688270701148}\n",
      "Losses {'ner': 2.0986235448469146}\n",
      "Losses {'ner': 2.0986235448487656}\n",
      "Losses {'ner': 2.0986235448651955}\n",
      "Losses {'ner': 2.098623549856625}\n",
      "Losses {'ner': 2.0986235498569017}\n",
      "Losses {'ner': 2.0986235498569057}\n",
      "Losses {'ner': 2.0986235498569386}\n",
      "Losses {'ner': 2.115037646441522}\n",
      "Losses {'ner': 4.10979735775281}\n",
      "Losses {'ner': 4.1098826257041745}\n",
      "Losses {'ner': 4.1098826259922046}\n",
      "Losses {'ner': 4.1098826261734045}\n",
      "Losses {'ner': 4.109891896752166}\n",
      "Losses {'ner': 4.109891896752202}\n",
      "Losses {'ner': 3.612348127486837e-08}\n",
      "Losses {'ner': 0.8376551100099735}\n",
      "Losses {'ner': 0.8376551101418732}\n",
      "Losses {'ner': 0.8376551102185038}\n",
      "Losses {'ner': 0.8376790161566898}\n",
      "Losses {'ner': 0.8376790161593055}\n",
      "Losses {'ner': 0.8376790161593165}\n",
      "Losses {'ner': 0.8376790161632028}\n",
      "Losses {'ner': 0.840395542552379}\n",
      "Losses {'ner': 0.8403981060855715}\n",
      "Losses {'ner': 0.8403981061003053}\n",
      "Losses {'ner': 0.8403981061003067}\n",
      "Losses {'ner': 0.8783671005356455}\n",
      "Losses {'ner': 0.8783671267380699}\n",
      "Losses {'ner': 0.8783671267380903}\n",
      "Losses {'ner': 0.8783671272106164}\n",
      "Losses {'ner': 2.8768455398387944}\n",
      "Losses {'ner': 2.876845539871845}\n",
      "Losses {'ner': 2.8768456085579466}\n",
      "Losses {'ner': 2.876845609515025}\n",
      "Losses {'ner': 2.8768456095182944}\n",
      "Losses {'ner': 2.876845609518297}\n",
      "Losses {'ner': 2.8768456095183486}\n",
      "Losses {'ner': 2.8768589047181465}\n",
      "Losses {'ner': 2.8768589047195166}\n",
      "Losses {'ner': 2.8768589047202617}\n",
      "Losses {'ner': 2.9554933680181374}\n",
      "Losses {'ner': 1.3706919433254872e-06}\n",
      "Losses {'ner': 1.3768991889694118e-06}\n",
      "Losses {'ner': 1.3769197752366838e-06}\n",
      "Losses {'ner': 1.3769197878374782e-06}\n",
      "Losses {'ner': 2.216279641157856e-05}\n",
      "Losses {'ner': 0.0009944927691323637}\n",
      "Losses {'ner': 0.0009944927756590206}\n",
      "Losses {'ner': 0.0010755072823608334}\n",
      "Losses {'ner': 0.0010755579711371032}\n",
      "Losses {'ner': 0.0010755579776041435}\n",
      "Losses {'ner': 0.0010775799870598227}\n",
      "Losses {'ner': 0.0010775799937571518}\n",
      "Losses {'ner': 0.0010775799986651533}\n",
      "Losses {'ner': 0.0010775800002779752}\n",
      "Losses {'ner': 0.0010775804733589138}\n",
      "Losses {'ner': 0.0010775805067848472}\n",
      "Losses {'ner': 0.0010775805128181593}\n",
      "Losses {'ner': 0.0010775805130136395}\n",
      "Losses {'ner': 0.0010775805143816884}\n",
      "Losses {'ner': 0.0010775805144970128}\n",
      "Losses {'ner': 0.0010775805149660358}\n",
      "Losses {'ner': 0.0010775805149715347}\n",
      "Losses {'ner': 2.0006677739455414}\n",
      "Losses {'ner': 2.0006677739490213}\n",
      "Losses {'ner': 2.0006677754567663}\n",
      "Losses {'ner': 2.000667775456768}\n",
      "Losses {'ner': 2.000667775456768}\n",
      "Losses {'ner': 2.384187732958517e-06}\n",
      "Losses {'ner': 2.384187735710011e-06}\n",
      "Losses {'ner': 2.384216536711442e-06}\n",
      "Losses {'ner': 2.384289791667909e-06}\n",
      "Losses {'ner': 2.3842903847911593e-06}\n",
      "Losses {'ner': 2.39015467615722e-06}\n",
      "Losses {'ner': 2.390154875046177e-06}\n",
      "Losses {'ner': 0.0005996841725651221}\n",
      "Losses {'ner': 0.0005996867378991519}\n",
      "Losses {'ner': 0.0005996884966443407}\n",
      "Losses {'ner': 0.0006009568993816505}\n",
      "Losses {'ner': 0.0006009568993820514}\n",
      "Losses {'ner': 1.3121370305842248}\n",
      "Losses {'ner': 1.3121734004465777}\n",
      "Losses {'ner': 1.3121734009865775}\n",
      "Losses {'ner': 1.3121734098671796}\n",
      "Losses {'ner': 1.313827042249347}\n",
      "Losses {'ner': 1.3142667635545093}\n",
      "Losses {'ner': 1.3142667635546341}\n",
      "Losses {'ner': 1.3142670660854596}\n",
      "Losses {'ner': 1.3142670662918279}\n",
      "Losses {'ner': 1.3142670662920761}\n",
      "Losses {'ner': 1.349859022520107}\n",
      "Losses {'ner': 1.3498590226535676}\n",
      "Losses {'ner': 4.396420193761231}\n",
      "Losses {'ner': 4.396558299149273}\n",
      "Losses {'ner': 4.396558299154345}\n",
      "Losses {'ner': 1.9259519577081707}\n",
      "Losses {'ner': 1.9259519577081707}\n",
      "Losses {'ner': 1.9259522376474816}\n",
      "Losses {'ner': 1.9261299660185696}\n",
      "Losses {'ner': 1.9261299660185878}\n",
      "Losses {'ner': 1.9261299660193616}\n",
      "Losses {'ner': 1.9286659938955661}\n",
      "Losses {'ner': 1.928665993927201}\n",
      "Losses {'ner': 1.9286764458095718}\n",
      "Losses {'ner': 1.9286764458098271}\n",
      "Losses {'ner': 1.9286764458102226}\n",
      "Losses {'ner': 2.3353961029850474}\n",
      "Losses {'ner': 2.3353961080728016}\n",
      "Losses {'ner': 2.3353961080728016}\n",
      "Losses {'ner': 2.3353963500157575}\n",
      "Losses {'ner': 2.335396417317123}\n",
      "Losses {'ner': 2.3353964173194166}\n",
      "Losses {'ner': 2.3353964173220323}\n",
      "Losses {'ner': 2.3441534083881312}\n",
      "Losses {'ner': 2.34415340838822}\n",
      "Losses {'ner': 2.3441534153520482}\n",
      "Losses {'ner': 2.3441534153520514}\n",
      "Losses {'ner': 2.3441534179093666}\n",
      "Losses {'ner': 2.344153417909386}\n",
      "Losses {'ner': 2.3441534470480865}\n",
      "Losses {'ner': 4.301691489271871}\n",
      "Losses {'ner': 4.301691489271871}\n",
      "Losses {'ner': 1.5338772867088812e-09}\n",
      "Losses {'ner': 1.9997823363847471}\n",
      "Losses {'ner': 1.999782336389708}\n",
      "Losses {'ner': 1.999782336389714}\n",
      "Losses {'ner': 1.9997823363897143}\n",
      "Losses {'ner': 1.9997823363897305}\n",
      "Losses {'ner': 1.9997823363897305}\n",
      "Losses {'ner': 1.9997823363897498}\n",
      "Losses {'ner': 2.03865337136957}\n",
      "Losses {'ner': 2.0386533779932554}\n",
      "Losses {'ner': 2.038653377993345}\n",
      "Losses {'ner': 2.0386533779936187}\n",
      "Losses {'ner': 2.0386533779936205}\n",
      "Losses {'ner': 2.0386563141547813}\n",
      "Losses {'ner': 2.03866967234906}\n",
      "Losses {'ner': 2.03866967261894}\n",
      "Losses {'ner': 2.0386754377767073}\n",
      "Losses {'ner': 2.2106529297876882}\n",
      "Losses {'ner': 2.210652932103133}\n",
      "Losses {'ner': 2.211016753689961}\n",
      "Losses {'ner': 2.211016753978754}\n",
      "Losses {'ner': 5.9512231359622625}\n",
      "Losses {'ner': 5.951223136182492}\n",
      "Losses {'ner': 5.95122313883452}\n",
      "Losses {'ner': 5.95122313883452}\n",
      "Losses {'ner': 5.951223138834591}\n",
      "Losses {'ner': 5.951223138834591}\n",
      "Losses {'ner': 1.4089445257379207e-11}\n",
      "Losses {'ner': 1.097865446261113e-10}\n",
      "Losses {'ner': 1.1295682104398241}\n",
      "Losses {'ner': 1.129568210440183}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1.1295682104404328}\n",
      "Losses {'ner': 3.129542302635266}\n",
      "Losses {'ner': 3.129542302635433}\n",
      "Losses {'ner': 3.1295636184692226}\n",
      "Losses {'ner': 3.1295636184692275}\n",
      "Losses {'ner': 3.129563618469471}\n",
      "Losses {'ner': 3.1295636193590135}\n",
      "Losses {'ner': 3.129563619359573}\n",
      "Losses {'ner': 3.1295636193608156}\n",
      "Losses {'ner': 3.1295636595026135}\n",
      "Losses {'ner': 3.129563659503151}\n",
      "Losses {'ner': 3.1360015263576395}\n",
      "Losses {'ner': 3.1360015281398432}\n",
      "Losses {'ner': 3.136001528573785}\n",
      "Losses {'ner': 3.1360015491741944}\n",
      "Losses {'ner': 3.1360015615605303}\n",
      "Losses {'ner': 3.1360789637080013}\n",
      "Losses {'ner': 3.136078963710042}\n",
      "Losses {'ner': 3.1360789639646645}\n",
      "Losses {'ner': 3.13607896396626}\n",
      "Losses {'ner': 3.136078963966472}\n",
      "Losses {'ner': 3.136078965205542}\n",
      "Losses {'ner': 3.1361063644441036}\n",
      "Losses {'ner': 1.2393898374036472e-08}\n",
      "Losses {'ner': 0.20196901011089544}\n",
      "Losses {'ner': 0.20196901735760728}\n",
      "Losses {'ner': 1.0827115135593988}\n",
      "Losses {'ner': 1.0827115174787836}\n",
      "Losses {'ner': 1.0827115174926047}\n",
      "Losses {'ner': 1.0827115175841664}\n",
      "Losses {'ner': 1.0827115182624099}\n",
      "Losses {'ner': 1.083878790363885}\n",
      "Losses {'ner': 1.0838787903844782}\n",
      "Losses {'ner': 1.0838787904656295}\n",
      "Losses {'ner': 1.0838787904658056}\n",
      "Losses {'ner': 1.0838787905081715}\n",
      "Losses {'ner': 1.0838914329292704}\n",
      "Losses {'ner': 1.0860582439194357}\n",
      "Losses {'ner': 1.0860583749592874}\n",
      "Losses {'ner': 1.0860583749617647}\n",
      "Losses {'ner': 1.0860583749659534}\n",
      "Losses {'ner': 1.0860583749761155}\n",
      "Losses {'ner': 1.0860591833010216}\n",
      "Losses {'ner': 1.086059183337052}\n",
      "Losses {'ner': 3.0861493356553753}\n",
      "Losses {'ner': 3.086325936708674}\n",
      "Losses {'ner': 3.086325936708992}\n",
      "Losses {'ner': 3.086325988233141}\n",
      "Losses {'ner': 3.0863259882331424}\n",
      "Losses {'ner': 5.015435446709643}\n",
      "Losses {'ner': 7.812999464399397e-15}\n",
      "Losses {'ner': 7.798915618294861e-10}\n",
      "Losses {'ner': 7.798926498580547e-10}\n",
      "Losses {'ner': 4.287301828635289e-07}\n",
      "Losses {'ner': 4.287959501013643e-07}\n",
      "Losses {'ner': 4.394102400474755e-07}\n",
      "Losses {'ner': 5.70249210150558e-07}\n",
      "Losses {'ner': 5.704908346241722e-07}\n",
      "Losses {'ner': 5.717488762376166e-07}\n",
      "Losses {'ner': 5.718836357531727e-07}\n",
      "Losses {'ner': 5.71886746020165e-07}\n",
      "Losses {'ner': 5.719150256771647e-07}\n",
      "Losses {'ner': 5.719151144798059e-07}\n",
      "Losses {'ner': 2.44881706988691e-06}\n",
      "Losses {'ner': 2.451416124187293e-06}\n",
      "Losses {'ner': 1.159099726200596e-05}\n",
      "Losses {'ner': 1.1590997383803828e-05}\n",
      "Losses {'ner': 1.1591020183772342e-05}\n",
      "Losses {'ner': 1.766256959887739}\n",
      "Losses {'ner': 1.7662570427539475}\n",
      "Losses {'ner': 1.7662570435247635}\n",
      "Losses {'ner': 1.766257043524802}\n",
      "Losses {'ner': 1.766257043550827}\n",
      "Losses {'ner': 1.7662570435562845}\n",
      "Losses {'ner': 1.7662570435580567}\n",
      "Losses {'ner': 1.7662583957503737}\n",
      "Losses {'ner': 1.7662583957503741}\n",
      "Losses {'ner': 2.147780700606444e-09}\n",
      "Losses {'ner': 2.2030964197558685e-09}\n",
      "Losses {'ner': 9.818252464086332e-06}\n",
      "Losses {'ner': 0.009333683884849502}\n",
      "Losses {'ner': 0.03847615221372319}\n",
      "Losses {'ner': 2.037566227709866}\n",
      "Losses {'ner': 2.0375662277110553}\n",
      "Losses {'ner': 2.037566227711119}\n",
      "Losses {'ner': 2.037568137593405}\n",
      "Losses {'ner': 2.037568137593453}\n",
      "Losses {'ner': 2.037568137602084}\n",
      "Losses {'ner': 2.037568137602108}\n",
      "Losses {'ner': 2.03756813760221}\n",
      "Losses {'ner': 2.0375681380047435}\n",
      "Losses {'ner': 2.0375681380050077}\n",
      "Losses {'ner': 2.0375681380050663}\n",
      "Losses {'ner': 2.0375681380050663}\n",
      "Losses {'ner': 2.0375681380132242}\n",
      "Losses {'ner': 2.037568144271963}\n",
      "Losses {'ner': 2.0375682053016657}\n",
      "Losses {'ner': 2.037568205302002}\n",
      "Losses {'ner': 2.037575217578306}\n",
      "Losses {'ner': 2.0375752179527975}\n",
      "Losses {'ner': 2.0375752435165424}\n",
      "Losses {'ner': 2.037575243574146}\n",
      "Losses {'ner': 2.037575243594803}\n",
      "Losses {'ner': 2.037575243712811}\n",
      "Losses {'ner': 3.5508396197081575e-10}\n",
      "Losses {'ner': 6.257469471696699e-10}\n",
      "Losses {'ner': 6.265686791552254e-10}\n",
      "Losses {'ner': 6.265701050684113e-10}\n",
      "Losses {'ner': 6.673228792785205e-10}\n",
      "Losses {'ner': 4.193846625327835e-08}\n",
      "Losses {'ner': 4.489824815707046e-08}\n",
      "Losses {'ner': 5.601964724164359e-07}\n",
      "Losses {'ner': 5.612801716362516e-07}\n",
      "Losses {'ner': 1.5628427946962152e-06}\n",
      "Losses {'ner': 1.5628427990409509e-06}\n",
      "Losses {'ner': 1.5628428074426455e-06}\n",
      "Losses {'ner': 1.035491181344454e-05}\n",
      "Losses {'ner': 0.0004838816511129715}\n",
      "Losses {'ner': 0.0004840707579344672}\n",
      "Losses {'ner': 0.0004840755771966989}\n",
      "Losses {'ner': 0.00048454837843457584}\n",
      "Losses {'ner': 0.00048454839077178506}\n",
      "Losses {'ner': 0.0004858070711284088}\n",
      "Losses {'ner': 0.0004858070711284089}\n",
      "Losses {'ner': 0.00048585032309009197}\n",
      "Losses {'ner': 0.0004858503230924104}\n",
      "Losses {'ner': 0.09920249069485987}\n",
      "Losses {'ner': 0.09920249107679815}\n",
      "Losses {'ner': 0.09920249107778814}\n",
      "Losses {'ner': 0.09920249143550082}\n",
      "Losses {'ner': 0.09920249143898138}\n",
      "Losses {'ner': 0.0003981320188279678}\n",
      "Losses {'ner': 0.0004170231458099454}\n",
      "Losses {'ner': 0.00041702328376658465}\n",
      "Losses {'ner': 0.0004173353242669122}\n",
      "Losses {'ner': 0.0004173353242669202}\n",
      "Losses {'ner': 0.0004173353242682195}\n",
      "Losses {'ner': 0.00041733532445807436}\n",
      "Losses {'ner': 0.00041733533028675564}\n",
      "Losses {'ner': 0.00041734668172997306}\n",
      "Losses {'ner': 0.0004173485498069664}\n",
      "Losses {'ner': 0.0004173485499222061}\n",
      "Losses {'ner': 0.00041734855020646774}\n",
      "Losses {'ner': 0.00041734855020706974}\n",
      "Losses {'ner': 0.00041734855181922134}\n",
      "Losses {'ner': 0.00041734856776726224}\n",
      "Losses {'ner': 0.00041734856784077213}\n",
      "Losses {'ner': 1.9198617663428095}\n",
      "Losses {'ner': 1.9198617663612048}\n",
      "Losses {'ner': 1.9221228188703368}\n",
      "Losses {'ner': 1.922122818870339}\n",
      "Losses {'ner': 1.9221228188704282}\n",
      "Losses {'ner': 1.9221228188759636}\n",
      "Losses {'ner': 1.9221228307165454}\n",
      "Losses {'ner': 1.9221228377624033}\n",
      "Losses {'ner': 1.9221374283073052}\n",
      "Losses {'ner': 1.9221374517405667}\n",
      "Losses {'ner': 1.9221374517502647}\n",
      "Losses {'ner': 3.0763322540993344e-14}\n",
      "Losses {'ner': 2.184403109388812e-10}\n",
      "Losses {'ner': 2.1923536851220634e-10}\n",
      "Losses {'ner': 2.0842963585756786e-07}\n",
      "Losses {'ner': 0.0005921722985341212}\n",
      "Losses {'ner': 0.0005921722987861894}\n",
      "Losses {'ner': 0.0005921778839460337}\n",
      "Losses {'ner': 0.0006362013710875145}\n",
      "Losses {'ner': 0.0006362945383048615}\n",
      "Losses {'ner': 0.0006362945402648042}\n",
      "Losses {'ner': 0.000636294540272921}\n",
      "Losses {'ner': 0.000636294685822498}\n",
      "Losses {'ner': 0.0006362948744112959}\n",
      "Losses {'ner': 0.0006362949470957417}\n",
      "Losses {'ner': 0.0006362949481429895}\n",
      "Losses {'ner': 0.004791271405500463}\n",
      "Losses {'ner': 1.4726549503378141}\n",
      "Losses {'ner': 1.4726549503918889}\n",
      "Losses {'ner': 1.4726549504759432}\n",
      "Losses {'ner': 1.472654951740588}\n",
      "Losses {'ner': 1.4726549517502354}\n",
      "Losses {'ner': 1.4726549517672942}\n",
      "Losses {'ner': 1.4726549582817745}\n",
      "Losses {'ner': 1.4742924319633777}\n",
      "Losses {'ner': 1.4742924333952776}\n",
      "Losses {'ner': 1.4742924333952776}\n",
      "Losses {'ner': 1.4742924333952776}\n",
      "Losses {'ner': 2.4980271514434657e-07}\n",
      "Losses {'ner': 2.4980873559085943e-07}\n",
      "Losses {'ner': 0.0007445622285505398}\n",
      "Losses {'ner': 0.0007445622348373305}\n",
      "Losses {'ner': 0.0007445904322776553}\n",
      "Losses {'ner': 0.0007445904323448007}\n",
      "Losses {'ner': 0.0007445904323465142}\n",
      "Losses {'ner': 0.0007446033586276383}\n",
      "Losses {'ner': 0.000744603358642833}\n",
      "Losses {'ner': 0.0007446033588645129}\n",
      "Losses {'ner': 0.012209890194983385}\n",
      "Losses {'ner': 0.014629085798563337}\n",
      "Losses {'ner': 0.014629085884662834}\n",
      "Losses {'ner': 0.01462908588525662}\n",
      "Losses {'ner': 0.01462908588555247}\n",
      "Losses {'ner': 0.014629085885552815}\n",
      "Losses {'ner': 0.014629089639856109}\n",
      "Losses {'ner': 0.015153797138992361}\n",
      "Losses {'ner': 0.5089140253361686}\n",
      "Losses {'ner': 0.6714986397205014}\n",
      "Losses {'ner': 0.6714986397219479}\n",
      "Losses {'ner': 0.6722372594339593}\n",
      "Losses {'ner': 0.672237259648153}\n",
      "Losses {'ner': 0.6722372596481531}\n",
      "Losses {'ner': 0.6722372596586795}\n",
      "Losses {'ner': 0.6722372600831253}\n",
      "Losses {'ner': 0.6722372600831253}\n",
      "Losses {'ner': 7.953979092896018e-11}\n",
      "Losses {'ner': 5.93723851266099e-05}\n",
      "Losses {'ner': 5.937238514413104e-05}\n",
      "Losses {'ner': 5.93723869173142e-05}\n",
      "Losses {'ner': 5.937238692312774e-05}\n",
      "Losses {'ner': 5.937238692313443e-05}\n",
      "Losses {'ner': 5.94095979525813e-05}\n",
      "Losses {'ner': 5.940959795381586e-05}\n",
      "Losses {'ner': 5.940975741860855e-05}\n",
      "Losses {'ner': 5.940979164586309e-05}\n",
      "Losses {'ner': 5.9409791647628635e-05}\n",
      "Losses {'ner': 5.9409943067965625e-05}\n",
      "Losses {'ner': 5.940994480273496e-05}\n",
      "Losses {'ner': 5.940994480300962e-05}\n",
      "Losses {'ner': 5.940995623223814e-05}\n",
      "Losses {'ner': 5.940995623371152e-05}\n",
      "Losses {'ner': 6.025543575303997e-05}\n",
      "Losses {'ner': 6.0255449505386775e-05}\n",
      "Losses {'ner': 6.025545320288521e-05}\n",
      "Losses {'ner': 6.0255453202894726e-05}\n",
      "Losses {'ner': 6.02554532116738e-05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 0.00011797280988871477}\n",
      "Losses {'ner': 0.00011797287263849264}\n",
      "Losses {'ner': 0.00011797287264345506}\n",
      "Losses {'ner': 0.00011797288761440407}\n",
      "Losses {'ner': 0.000117972887643425}\n",
      "Losses {'ner': 0.000117972887643425}\n",
      "Losses {'ner': 0.002678295516943765}\n",
      "Losses {'ner': 0.0026782955206637667}\n",
      "Losses {'ner': 0.3720523156916641}\n",
      "Losses {'ner': 0.372052315692271}\n",
      "Losses {'ner': 0.3720523165483224}\n",
      "Losses {'ner': 0.37205231683223045}\n",
      "Losses {'ner': 1.3499745179891178}\n",
      "Losses {'ner': 1.349974517989118}\n",
      "Losses {'ner': 1.3499745179891587}\n",
      "Losses {'ner': 1.349974517991757}\n",
      "Losses {'ner': 1.349974517992493}\n",
      "Losses {'ner': 1.3499745184206802}\n",
      "Losses {'ner': 1.3499745184206806}\n",
      "Losses {'ner': 1.4858918244700536}\n",
      "Losses {'ner': 1.4858918245712265}\n",
      "Losses {'ner': 1.4858918245717012}\n",
      "Losses {'ner': 1.4858918249901083}\n",
      "Losses {'ner': 1.4858918250954998}\n",
      "Losses {'ner': 3.485891828135766}\n",
      "Losses {'ner': 3.48589186217362}\n",
      "Losses {'ner': 3.5252131506403463}\n",
      "Losses {'ner': 3.525213150640349}\n",
      "Losses {'ner': 3.525213150640994}\n",
      "Losses {'ner': 3.525213150640994}\n",
      "Losses {'ner': 3.525213150819514}\n",
      "Losses {'ner': 3.5252131508199565}\n",
      "Losses {'ner': 3.5252131508199565}\n",
      "Losses {'ner': 5.142361505339493e-14}\n",
      "Losses {'ner': 1.7098460402471507}\n",
      "Losses {'ner': 1.7098493672130994}\n",
      "Losses {'ner': 1.7098493672131057}\n",
      "Losses {'ner': 1.7098493672732689}\n",
      "Losses {'ner': 1.7098493672809414}\n",
      "Losses {'ner': 1.7098548539532985}\n",
      "Losses {'ner': 1.7098548539623415}\n",
      "Losses {'ner': 1.7098548539623415}\n",
      "Losses {'ner': 1.7098548539639846}\n",
      "Losses {'ner': 1.70985485396531}\n",
      "Losses {'ner': 2.062044902149991}\n",
      "Losses {'ner': 2.06204490215227}\n",
      "Losses {'ner': 2.0620449021524996}\n",
      "Losses {'ner': 3.8053652480347675}\n",
      "Losses {'ner': 3.805365335117206}\n",
      "Losses {'ner': 3.806289553808151}\n",
      "Losses {'ner': 3.8062895538081616}\n",
      "Losses {'ner': 3.8062895538081642}\n",
      "Losses {'ner': 5.805934535508233}\n",
      "Losses {'ner': 5.805934535833226}\n",
      "Losses {'ner': 5.805934536047508}\n",
      "Losses {'ner': 5.805934536047508}\n",
      "Losses {'ner': 5.805934538260633}\n",
      "Losses {'ner': 5.80593453826251}\n",
      "Losses {'ner': 5.80593453826251}\n",
      "Losses {'ner': 5.80593453826251}\n",
      "Losses {'ner': 4.305463519542364e-05}\n",
      "Losses {'ner': 1.939483305031924}\n",
      "Losses {'ner': 1.939483462359743}\n",
      "Losses {'ner': 1.9394834624011816}\n",
      "Losses {'ner': 1.9394834624012007}\n",
      "Losses {'ner': 1.9394834624012007}\n",
      "Losses {'ner': 1.9394844289766053}\n",
      "Losses {'ner': 1.9394844289766056}\n",
      "Losses {'ner': 1.9405234117463983}\n",
      "Losses {'ner': 1.9405234226043049}\n",
      "Losses {'ner': 1.9405234226043067}\n",
      "Losses {'ner': 3.9402460231719028}\n",
      "Losses {'ner': 3.9402460231750682}\n",
      "Losses {'ner': 3.9402460231751295}\n",
      "Losses {'ner': 3.9402460331391502}\n",
      "Losses {'ner': 3.9402473991820024}\n",
      "Losses {'ner': 3.940247399182003}\n",
      "Losses {'ner': 3.940247400031003}\n",
      "Losses {'ner': 3.9402474023690814}\n",
      "Losses {'ner': 3.9402474023690885}\n",
      "Losses {'ner': 4.6595277644983}\n",
      "Losses {'ner': 4.6595277644983}\n",
      "Losses {'ner': 4.6595277644983}\n",
      "Losses {'ner': 4.659667152201122}\n",
      "Losses {'ner': 4.659667152201185}\n",
      "Losses {'ner': 4.6596671523522435}\n",
      "Losses {'ner': 4.6596671523522435}\n",
      "Losses {'ner': 4.2095874994396437e-17}\n",
      "Losses {'ner': 5.585910772934544e-06}\n",
      "Losses {'ner': 5.6634422809165285e-06}\n",
      "Losses {'ner': 5.672403972679789e-06}\n",
      "Losses {'ner': 5.672403972713761e-06}\n",
      "Losses {'ner': 5.672436696182177e-06}\n",
      "Losses {'ner': 5.714582073861496e-06}\n",
      "Losses {'ner': 5.721033027175725e-06}\n",
      "Losses {'ner': 0.6863606689462302}\n",
      "Losses {'ner': 0.6863606694671871}\n",
      "Losses {'ner': 0.6863606696869285}\n",
      "Losses {'ner': 0.6863609803086684}\n",
      "Losses {'ner': 0.6863609803088658}\n",
      "Losses {'ner': 0.6863609803088692}\n",
      "Losses {'ner': 0.6863609803088875}\n",
      "Losses {'ner': 0.6863609804315632}\n",
      "Losses {'ner': 0.7379575176640187}\n",
      "Losses {'ner': 0.7379575176745856}\n",
      "Losses {'ner': 0.7379575176745977}\n",
      "Losses {'ner': 0.7379575176756513}\n",
      "Losses {'ner': 0.7379575269068008}\n",
      "Losses {'ner': 0.7443790897691981}\n",
      "Losses {'ner': 0.7443790897692143}\n",
      "Losses {'ner': 0.7443898113378186}\n",
      "Losses {'ner': 0.7443898113378449}\n",
      "Losses {'ner': 0.7562393473972805}\n",
      "Losses {'ner': 0.7562393473972805}\n",
      "Losses {'ner': 1.5589276382787115e-11}\n",
      "Losses {'ner': 0.0007877169653653963}\n",
      "Losses {'ner': 1.9544535695120637}\n",
      "Losses {'ner': 1.9544535695120675}\n",
      "Losses {'ner': 1.954453569924849}\n",
      "Losses {'ner': 1.954453569924851}\n",
      "Losses {'ner': 1.9641574613599808}\n",
      "Losses {'ner': 3.75232100410757}\n",
      "Losses {'ner': 3.7523211232499083}\n",
      "Losses {'ner': 3.835642247565209}\n",
      "Losses {'ner': 3.835642247565209}\n",
      "Losses {'ner': 3.835642247565457}\n",
      "Losses {'ner': 3.835642247618165}\n",
      "Losses {'ner': 3.8356422476181673}\n",
      "Losses {'ner': 3.8356422476181966}\n",
      "Losses {'ner': 3.8356422476181997}\n",
      "Losses {'ner': 3.8356422476182757}\n",
      "Losses {'ner': 3.835642392355671}\n",
      "Losses {'ner': 3.8356423923562337}\n",
      "Losses {'ner': 3.8356423923562355}\n",
      "Losses {'ner': 3.8356423926005307}\n",
      "Losses {'ner': 3.8356423926022267}\n",
      "Losses {'ner': 3.8356423926022276}\n",
      "Losses {'ner': 3.8356423926022276}\n",
      "Losses {'ner': 3.835642392602228}\n",
      "Losses {'ner': 3.835642392602228}\n",
      "Losses {'ner': 3.835642392603229}\n",
      "Losses {'ner': 1.7939114570622778}\n",
      "Losses {'ner': 1.7939114570622807}\n",
      "Losses {'ner': 1.793911457062287}\n",
      "Losses {'ner': 1.7939114570779582}\n",
      "Losses {'ner': 1.7939114571622943}\n",
      "Losses {'ner': 1.7939114572306265}\n",
      "Losses {'ner': 1.7939114572306274}\n",
      "Losses {'ner': 1.7939114572500985}\n",
      "Losses {'ner': 1.7939115528789504}\n",
      "Losses {'ner': 1.7939115532037753}\n",
      "Losses {'ner': 1.7947074182859817}\n",
      "Losses {'ner': 3.79459202369374}\n",
      "Losses {'ner': 3.794592023693789}\n",
      "Losses {'ner': 3.794592023693789}\n",
      "Losses {'ner': 3.794592030268142}\n",
      "Losses {'ner': 3.794592030268148}\n",
      "Losses {'ner': 3.7945920534020168}\n",
      "Losses {'ner': 3.794592053403666}\n",
      "Losses {'ner': 3.7945920534066637}\n",
      "Losses {'ner': 3.7945920534066637}\n",
      "Losses {'ner': 3.7945920534068143}\n",
      "Losses {'ner': 3.7945920534104562}\n",
      "Losses {'ner': 3.794592053410459}\n",
      "Losses {'ner': 3.7945920534104594}\n",
      "Losses {'ner': 3.7945920534104998}\n",
      "Losses {'ner': 3.7945920612560884}\n",
      "Losses {'ner': 3.7945920612560884}\n",
      "Losses {'ner': 1.9162699581287466e-07}\n",
      "Losses {'ner': 1.9162700124372225e-07}\n",
      "Losses {'ner': 1.9162700124464207e-07}\n",
      "Losses {'ner': 1.9162700125818573e-07}\n",
      "Losses {'ner': 1.9164973498108715e-07}\n",
      "Losses {'ner': 1.916774451310655e-07}\n",
      "Losses {'ner': 0.0011210941589483454}\n",
      "Losses {'ner': 0.0011210941589491893}\n",
      "Losses {'ner': 0.0011210941745710666}\n",
      "Losses {'ner': 0.001121094247240827}\n",
      "Losses {'ner': 0.004473115610863401}\n",
      "Losses {'ner': 0.004473115658402263}\n",
      "Losses {'ner': 0.004473115785317291}\n",
      "Losses {'ner': 0.004473623919325056}\n",
      "Losses {'ner': 0.004473623919330711}\n",
      "Losses {'ner': 0.004473623920502481}\n",
      "Losses {'ner': 0.03176267340004616}\n",
      "Losses {'ner': 0.031762673400256854}\n",
      "Losses {'ner': 0.031762673400256854}\n",
      "Losses {'ner': 0.031763367436599924}\n",
      "Losses {'ner': 0.031763367436602616}\n",
      "Losses {'ner': 0.03176428954299706}\n",
      "Losses {'ner': 0.03176428954299706}\n",
      "Losses {'ner': 0.03176428957241585}\n",
      "Losses {'ner': 0.03176428957241585}\n",
      "Losses {'ner': 0.03177513505216509}\n",
      "Losses {'ner': 0.03177513505216509}\n",
      "Losses {'ner': 2.211302427802435e-17}\n",
      "Losses {'ner': 3.7618229745163515e-16}\n",
      "Losses {'ner': 0.020846053957939526}\n",
      "Losses {'ner': 0.02084605762118655}\n",
      "Losses {'ner': 0.020846057660669864}\n",
      "Losses {'ner': 2.018547225721803}\n",
      "Losses {'ner': 2.0185472257219055}\n",
      "Losses {'ner': 2.0185472257219055}\n",
      "Losses {'ner': 2.018782769383448}\n",
      "Losses {'ner': 2.0268117031618558}\n",
      "Losses {'ner': 2.026811703161935}\n",
      "Losses {'ner': 2.026811703161935}\n",
      "Losses {'ner': 2.0268168452127076}\n",
      "Losses {'ner': 2.0268168452127107}\n",
      "Losses {'ner': 2.0339412132108357}\n",
      "Losses {'ner': 2.033941213212949}\n",
      "Losses {'ner': 2.035019651654277}\n",
      "Losses {'ner': 2.445789418432905}\n",
      "Losses {'ner': 2.8343215503432293}\n",
      "Losses {'ner': 2.837827547578637}\n",
      "Losses {'ner': 2.8378275475833186}\n",
      "Losses {'ner': 2.83782754758338}\n",
      "Losses {'ner': 2.927055194172659}\n",
      "Losses {'ner': 4.92248828629019}\n",
      "Losses {'ner': 4.922488286291314}\n",
      "Losses {'ner': 4.922999321905541}\n",
      "Losses {'ner': 4.922999321905541}\n",
      "Losses {'ner': 2.2394951350152684e-13}\n",
      "Losses {'ner': 1.4774435173215169e-12}\n",
      "Losses {'ner': 6.279222290434962e-09}\n",
      "Losses {'ner': 6.2856338796053904e-09}\n",
      "Losses {'ner': 6.287080518847535e-09}\n",
      "Losses {'ner': 7.125117015583188e-09}\n",
      "Losses {'ner': 1.4296494044481407e-08}\n",
      "Losses {'ner': 2.000000014296524}\n",
      "Losses {'ner': 2.00121670936207}\n",
      "Losses {'ner': 2.001216713796155}\n",
      "Losses {'ner': 2.6976249860962622}\n",
      "Losses {'ner': 2.6976249860962622}\n",
      "Losses {'ner': 2.697624995319415}\n",
      "Losses {'ner': 2.697624995319417}\n",
      "Losses {'ner': 2.706508873350382}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 2.706509437905238}\n",
      "Losses {'ner': 2.7065098298781805}\n",
      "Losses {'ner': 2.706509829878182}\n",
      "Losses {'ner': 2.7070122636806615}\n",
      "Losses {'ner': 2.707039685070976}\n",
      "Losses {'ner': 2.7070396850709817}\n",
      "Losses {'ner': 8.226789125324332}\n",
      "Losses {'ner': 8.22678912532443}\n",
      "Losses {'ner': 8.226789125324458}\n",
      "Losses {'ner': 8.25541221346738}\n",
      "Losses {'ner': 8.25541221346738}\n",
      "Losses {'ner': 8.25541221346738}\n",
      "Losses {'ner': 1.5614167949100252e-11}\n",
      "Losses {'ner': 1.1440995213752032e-09}\n",
      "Losses {'ner': 1.4102420930745453e-08}\n",
      "Losses {'ner': 1.4613222455743633e-08}\n",
      "Losses {'ner': 1.4619146950269967e-08}\n",
      "Losses {'ner': 1.4619229995507392e-08}\n",
      "Losses {'ner': 1.4619231628740861e-08}\n",
      "Losses {'ner': 1.4619231870924308e-08}\n",
      "Losses {'ner': 0.0005464730293909809}\n",
      "Losses {'ner': 0.0005464730989400155}\n",
      "Losses {'ner': 0.0005464731150174597}\n",
      "Losses {'ner': 0.0005464731417800687}\n",
      "Losses {'ner': 0.0005464731418368192}\n",
      "Losses {'ner': 0.0005464731434993105}\n",
      "Losses {'ner': 0.000546473143499958}\n",
      "Losses {'ner': 0.0005464731436394491}\n",
      "Losses {'ner': 1.9994594036323445}\n",
      "Losses {'ner': 1.9994594036448685}\n",
      "Losses {'ner': 1.9994594300576591}\n",
      "Losses {'ner': 1.9994594597864688}\n",
      "Losses {'ner': 1.9994594597865645}\n",
      "Losses {'ner': 1.9994594597865645}\n",
      "Losses {'ner': 1.9994594597865665}\n",
      "Losses {'ner': 1.999459459800398}\n",
      "Losses {'ner': 1.9994594614528935}\n",
      "Losses {'ner': 1.9994627716137732}\n",
      "Losses {'ner': 1.9994627716137732}\n",
      "Losses {'ner': 2.3459787199683696e-15}\n",
      "Losses {'ner': 1.5264516558465131e-13}\n",
      "Losses {'ner': 4.464085085516402e-10}\n",
      "Losses {'ner': 4.756397716117957e-10}\n",
      "Losses {'ner': 3.76514511038512e-09}\n",
      "Losses {'ner': 0.012419430579251465}\n",
      "Losses {'ner': 0.012433451879784159}\n",
      "Losses {'ner': 0.012433451879799813}\n",
      "Losses {'ner': 0.012433451891690407}\n",
      "Losses {'ner': 0.012433451891702127}\n",
      "Losses {'ner': 0.012433451954430305}\n",
      "Losses {'ner': 0.012478248186334664}\n",
      "Losses {'ner': 0.01250833525332361}\n",
      "Losses {'ner': 0.01250834574299067}\n",
      "Losses {'ner': 0.012508352979283094}\n",
      "Losses {'ner': 0.012508352979308426}\n",
      "Losses {'ner': 0.012541585434258531}\n",
      "Losses {'ner': 0.012541585434331688}\n",
      "Losses {'ner': 0.012541585434449505}\n",
      "Losses {'ner': 0.012541585434462585}\n",
      "Losses {'ner': 0.012541586405101328}\n",
      "Losses {'ner': 0.0125415864125578}\n",
      "Losses {'ner': 0.012541586412557856}\n",
      "Losses {'ner': 2.011459540915015}\n",
      "Losses {'ner': 2.0114595409150233}\n",
      "Losses {'ner': 2.0114595409150247}\n",
      "Losses {'ner': 2.0114595409150295}\n",
      "Losses {'ner': 0.001448267255909761}\n",
      "Losses {'ner': 0.7208890627371165}\n",
      "Losses {'ner': 0.7208890627371336}\n",
      "Losses {'ner': 0.7208890627383336}\n",
      "Losses {'ner': 0.7208890676246603}\n",
      "Losses {'ner': 0.7208890676246626}\n",
      "Losses {'ner': 0.7208890676326087}\n",
      "Losses {'ner': 0.7208890676502323}\n",
      "Losses {'ner': 0.7208890676616597}\n",
      "Losses {'ner': 0.7208890676616604}\n",
      "Losses {'ner': 0.7210339610088323}\n",
      "Losses {'ner': 0.7210339610089455}\n",
      "Losses {'ner': 2.7210337225904513}\n",
      "Losses {'ner': 2.7210337225904513}\n",
      "Losses {'ner': 2.7210337238122455}\n",
      "Losses {'ner': 2.721033723813044}\n",
      "Losses {'ner': 2.7210337238130444}\n",
      "Losses {'ner': 2.7210337238130524}\n",
      "Losses {'ner': 2.721033723941112}\n",
      "Losses {'ner': 2.721033723941112}\n",
      "Losses {'ner': 2.721033723941235}\n",
      "Losses {'ner': 2.721033723941461}\n",
      "Losses {'ner': 4.535892637363094}\n",
      "Losses {'ner': 7.863794503884226}\n",
      "Losses {'ner': 7.863794503884226}\n",
      "Losses {'ner': 9.863065300660043}\n",
      "Losses {'ner': 9.863065300660043}\n",
      "Losses {'ner': 1.3490886244336925e-10}\n",
      "Losses {'ner': 9.430745264597318e-05}\n",
      "Losses {'ner': 9.430745876916437e-05}\n",
      "Losses {'ner': 9.430746139197912e-05}\n",
      "Losses {'ner': 0.0002096942613081477}\n",
      "Losses {'ner': 0.00020969427033400484}\n",
      "Losses {'ner': 0.00020969433512215327}\n",
      "Losses {'ner': 0.0002144500791692008}\n",
      "Losses {'ner': 0.00021445574101726852}\n",
      "Losses {'ner': 0.00021445850453033686}\n",
      "Losses {'ner': 0.000214462297220733}\n",
      "Losses {'ner': 0.00021446229813599376}\n",
      "Losses {'ner': 0.00021446235147764257}\n",
      "Losses {'ner': 0.0045600200644568736}\n",
      "Losses {'ner': 0.004560208610453282}\n",
      "Losses {'ner': 0.0045602086106304555}\n",
      "Losses {'ner': 0.004560208767652823}\n",
      "Losses {'ner': 0.00456020876823473}\n",
      "Losses {'ner': 0.0045602100509722935}\n",
      "Losses {'ner': 5.928651987694884}\n",
      "Losses {'ner': 5.928651987700669}\n",
      "Losses {'ner': 5.928651987808662}\n",
      "Losses {'ner': 5.928651987809765}\n",
      "Losses {'ner': 7.86145042539248}\n",
      "Losses {'ner': 7.861450427710759}\n",
      "Losses {'ner': 7.898251165361646}\n",
      "Losses {'ner': 8.144356415373743}\n",
      "Losses {'ner': 3.820743166654921e-11}\n",
      "Losses {'ner': 7.512481966818557e-09}\n",
      "Losses {'ner': 1.0531579343073518e-05}\n",
      "Losses {'ner': 1.0531603158904583e-05}\n",
      "Losses {'ner': 2.104798779781801e-05}\n",
      "Losses {'ner': 3.132227763360014e-05}\n",
      "Losses {'ner': 3.13226494514498e-05}\n",
      "Losses {'ner': 0.9343616670107903}\n",
      "Losses {'ner': 0.9343616729677156}\n",
      "Losses {'ner': 0.9343616731803072}\n",
      "Losses {'ner': 0.9343616737377229}\n",
      "Losses {'ner': 0.9343629240999132}\n",
      "Losses {'ner': 0.9343724224791695}\n",
      "Losses {'ner': 0.9343737500359519}\n",
      "Losses {'ner': 0.9343737500563132}\n",
      "Losses {'ner': 0.9343737500570636}\n",
      "Losses {'ner': 0.9343737500823187}\n",
      "Losses {'ner': 0.9343737500823265}\n",
      "Losses {'ner': 2.8847310680126084}\n",
      "Losses {'ner': 2.884902517569268}\n",
      "Losses {'ner': 2.8849025349424577}\n",
      "Losses {'ner': 2.8849556635840004}\n",
      "Losses {'ner': 2.8849561046375456}\n",
      "Losses {'ner': 2.8849580117739295}\n",
      "Losses {'ner': 2.8849580118602893}\n",
      "Losses {'ner': 2.884958013502454}\n",
      "Losses {'ner': 2.884958013502457}\n",
      "Losses {'ner': 4.9031202019362034e-14}\n",
      "Losses {'ner': 2.4134503774440617e-11}\n",
      "Losses {'ner': 6.750634620126116e-11}\n",
      "Losses {'ner': 6.755626145468175e-11}\n",
      "Losses {'ner': 1.402028083869487}\n",
      "Losses {'ner': 1.4020280838695285}\n",
      "Losses {'ner': 1.4020280838698393}\n",
      "Losses {'ner': 2.184065051959693}\n",
      "Losses {'ner': 2.1840650530907917}\n",
      "Losses {'ner': 2.1840689618748415}\n",
      "Losses {'ner': 4.184115045192151}\n",
      "Losses {'ner': 4.184115047855233}\n",
      "Losses {'ner': 4.184115048180593}\n",
      "Losses {'ner': 6.183600628079034}\n",
      "Losses {'ner': 6.183600631241454}\n",
      "Losses {'ner': 6.183600631292287}\n",
      "Losses {'ner': 6.183600631413627}\n",
      "Losses {'ner': 6.185225259586514}\n",
      "Losses {'ner': 6.185225259586546}\n",
      "Losses {'ner': 6.185225260071939}\n",
      "Losses {'ner': 6.185225260071939}\n",
      "Losses {'ner': 6.18522532922857}\n",
      "Losses {'ner': 6.185225329410247}\n",
      "Losses {'ner': 6.185225329415159}\n",
      "Losses {'ner': 6.185767365963711}\n",
      "Losses {'ner': 6.185767366001931}\n",
      "Losses {'ner': 6.185767366006713}\n",
      "Losses {'ner': 2.5317636328815975e-08}\n",
      "Losses {'ner': 2.5329129926832784e-08}\n",
      "Losses {'ner': 1.9998779550169021}\n",
      "Losses {'ner': 1.9998779550169021}\n",
      "Losses {'ner': 1.9998779550313448}\n",
      "Losses {'ner': 1.9998779578583508}\n",
      "Losses {'ner': 1.9998779585767552}\n",
      "Losses {'ner': 1.9998779586325883}\n",
      "Losses {'ner': 1.9999712660891857}\n",
      "Losses {'ner': 1.9999712660894862}\n",
      "Losses {'ner': 1.9999714941066586}\n",
      "Losses {'ner': 2.004264160050061}\n",
      "Losses {'ner': 2.0042641600531907}\n",
      "Losses {'ner': 2.0042641600531916}\n",
      "Losses {'ner': 2.0042641600561146}\n",
      "Losses {'ner': 2.0042641600561177}\n",
      "Losses {'ner': 2.0190501371603307}\n",
      "Losses {'ner': 2.019174133259967}\n",
      "Losses {'ner': 2.0191741332601008}\n",
      "Losses {'ner': 2.019174133260102}\n",
      "Losses {'ner': 2.019174133260327}\n",
      "Losses {'ner': 3.018042121847222}\n",
      "Losses {'ner': 3.0180421218472384}\n",
      "Losses {'ner': 3.0180421218472393}\n",
      "Losses {'ner': 3.0180421220880045}\n",
      "Losses {'ner': 3.0180421220880556}\n",
      "Losses {'ner': 3.633623949019892}\n",
      "Losses {'ner': 5.140256923975429e-11}\n",
      "Losses {'ner': 2.985436064492295e-10}\n",
      "Losses {'ner': 2.991576371012743e-10}\n",
      "Losses {'ner': 1.607787345725503e-09}\n",
      "Losses {'ner': 1.659676662705351e-09}\n",
      "Losses {'ner': 1.6855254210223081e-09}\n",
      "Losses {'ner': 2.4546517969713056e-08}\n",
      "Losses {'ner': 2.4546527410328736e-08}\n",
      "Losses {'ner': 2.5843442701062513e-08}\n",
      "Losses {'ner': 0.18719347915569776}\n",
      "Losses {'ner': 0.18719347916820173}\n",
      "Losses {'ner': 0.18719348875067032}\n",
      "Losses {'ner': 0.18720009172171756}\n",
      "Losses {'ner': 0.18720009183873204}\n",
      "Losses {'ner': 1.6201547608436857}\n",
      "Losses {'ner': 1.6201547609170557}\n",
      "Losses {'ner': 1.6201547609170825}\n",
      "Losses {'ner': 1.6201548205525382}\n",
      "Losses {'ner': 1.6201548205526737}\n",
      "Losses {'ner': 1.620154820554401}\n",
      "Losses {'ner': 1.6201548205548382}\n",
      "Losses {'ner': 1.6201548205548417}\n",
      "Losses {'ner': 1.620165214288489}\n",
      "Losses {'ner': 1.6201652142884904}\n",
      "Losses {'ner': 1.6201652142897671}\n",
      "Losses {'ner': 1.6252071437175066}\n",
      "Losses {'ner': 1.6252071437175066}\n",
      "Losses {'ner': 4.996077012670649e-12}\n",
      "Losses {'ner': 5.195048560961678e-12}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 5.315113171206873e-09}\n",
      "Losses {'ner': 0.00011738957101294503}\n",
      "Losses {'ner': 0.00011738957111175667}\n",
      "Losses {'ner': 1.6952515535805757}\n",
      "Losses {'ner': 1.6952515535805759}\n",
      "Losses {'ner': 1.6952515535805786}\n",
      "Losses {'ner': 1.6952515535805786}\n",
      "Losses {'ner': 2.0713889085845802}\n",
      "Losses {'ner': 2.071389228290098}\n",
      "Losses {'ner': 2.0713892317692233}\n",
      "Losses {'ner': 2.0713892317731513}\n",
      "Losses {'ner': 2.0713892317731912}\n",
      "Losses {'ner': 2.1439464828895924}\n",
      "Losses {'ner': 2.1439464828895924}\n",
      "Losses {'ner': 2.143946482889595}\n",
      "Losses {'ner': 2.143946482889606}\n",
      "Losses {'ner': 2.1439464828896093}\n",
      "Losses {'ner': 2.143946482909456}\n",
      "Losses {'ner': 2.14394648292005}\n",
      "Losses {'ner': 2.1439464867313722}\n",
      "Losses {'ner': 2.1439464868117337}\n",
      "Losses {'ner': 2.1439464868123985}\n",
      "Losses {'ner': 2.143946486813141}\n",
      "Losses {'ner': 2.1454315217922666}\n",
      "Losses {'ner': 2.1454315217926925}\n",
      "Losses {'ner': 4.699371262805622e-13}\n",
      "Losses {'ner': 2.6942177125866538e-11}\n",
      "Losses {'ner': 4.6101311824107076e-11}\n",
      "Losses {'ner': 4.784525188760554e-11}\n",
      "Losses {'ner': 1.107399994480848e-09}\n",
      "Losses {'ner': 1.9569759379970493}\n",
      "Losses {'ner': 1.9569759380115024}\n",
      "Losses {'ner': 1.9569759380116545}\n",
      "Losses {'ner': 1.9569759391105035}\n",
      "Losses {'ner': 1.9569759391143562}\n",
      "Losses {'ner': 1.9569759391177832}\n",
      "Losses {'ner': 1.9569761224765487}\n",
      "Losses {'ner': 1.9569761494320674}\n",
      "Losses {'ner': 5.956896875254516}\n",
      "Losses {'ner': 7.9566737995571915}\n",
      "Losses {'ner': 7.9566737995571915}\n",
      "Losses {'ner': 7.956673800432631}\n",
      "Losses {'ner': 7.956673800439715}\n",
      "Losses {'ner': 7.956969256240084}\n",
      "Losses {'ner': 7.956969261808326}\n",
      "Losses {'ner': 7.956969261808355}\n",
      "Losses {'ner': 7.956969261821323}\n",
      "Losses {'ner': 7.9569692623149395}\n",
      "Losses {'ner': 7.956969263533596}\n",
      "Losses {'ner': 7.956969263819263}\n",
      "Losses {'ner': 7.956969275720826}\n",
      "Losses {'ner': 7.966072078444761}\n",
      "Losses {'ner': 0.017021179211170773}\n",
      "Losses {'ner': 0.01706161379589244}\n",
      "Losses {'ner': 0.017061613796074203}\n",
      "Losses {'ner': 1.1491391897704937}\n",
      "Losses {'ner': 1.1491391952096985}\n",
      "Losses {'ner': 1.149139195228695}\n",
      "Losses {'ner': 1.1491392017624231}\n",
      "Losses {'ner': 1.149139201768359}\n",
      "Losses {'ner': 1.149139306253343}\n",
      "Losses {'ner': 3.044476291841459}\n",
      "Losses {'ner': 3.044476291841459}\n",
      "Losses {'ner': 3.044476291868325}\n",
      "Losses {'ner': 3.04447629635161}\n",
      "Losses {'ner': 3.044853574195264}\n",
      "Losses {'ner': 3.044853574197304}\n",
      "Losses {'ner': 3.117405851794671}\n",
      "Losses {'ner': 3.117407069921841}\n",
      "Losses {'ner': 3.1174070700953256}\n",
      "Losses {'ner': 3.1174070700953385}\n",
      "Losses {'ner': 3.117407142212423}\n",
      "Losses {'ner': 3.117416937474861}\n",
      "Losses {'ner': 3.1174169374748613}\n",
      "Losses {'ner': 3.1174169374993457}\n",
      "Losses {'ner': 3.117416937499372}\n",
      "Losses {'ner': 3.117416937499372}\n",
      "Losses {'ner': 3.1174169375149683}\n",
      "Losses {'ner': 3.117416937514969}\n",
      "Losses {'ner': 3.2100682655793525e-15}\n",
      "Losses {'ner': 4.572904501547117e-15}\n",
      "Losses {'ner': 3.9518582826884217}\n",
      "Losses {'ner': 3.951858283267594}\n",
      "Losses {'ner': 5.70660364745679}\n",
      "Losses {'ner': 5.706603649858239}\n",
      "Losses {'ner': 5.706603649858243}\n",
      "Losses {'ner': 5.706603649870154}\n",
      "Losses {'ner': 5.710294145026442}\n",
      "Losses {'ner': 5.710294154740172}\n",
      "Losses {'ner': 5.710294154743023}\n",
      "Losses {'ner': 5.710294154746064}\n",
      "Losses {'ner': 5.710294154746064}\n",
      "Losses {'ner': 5.710294154746064}\n",
      "Losses {'ner': 5.710294154868605}\n",
      "Losses {'ner': 5.710294415336462}\n",
      "Losses {'ner': 5.710298349809869}\n",
      "Losses {'ner': 5.710298349832285}\n",
      "Losses {'ner': 5.710298349832285}\n",
      "Losses {'ner': 5.710298349832322}\n",
      "Losses {'ner': 5.710298349832335}\n",
      "Losses {'ner': 5.710298363029817}\n",
      "Losses {'ner': 5.710298363118189}\n",
      "Losses {'ner': 5.7102983631190485}\n",
      "Losses {'ner': 5.710298363119971}\n",
      "Losses {'ner': 5.710298363119971}\n",
      "Losses {'ner': 5.710298363119971}\n",
      "Losses {'ner': 1.9559017480896597}\n",
      "Losses {'ner': 1.9559017765769984}\n",
      "Losses {'ner': 1.9940065156171396}\n",
      "Losses {'ner': 1.9940065156171436}\n",
      "Losses {'ner': 2.112218381042254}\n",
      "Losses {'ner': 4.105185152168048}\n",
      "Losses {'ner': 4.10518515216808}\n",
      "Losses {'ner': 4.10518515216808}\n",
      "Losses {'ner': 4.105185152175028}\n",
      "Losses {'ner': 4.105185152175457}\n",
      "Losses {'ner': 4.105542033048935}\n",
      "Losses {'ner': 4.105542033048935}\n",
      "Losses {'ner': 4.10633101875523}\n",
      "Losses {'ner': 4.3925579639274845}\n",
      "Losses {'ner': 4.392557963927763}\n",
      "Losses {'ner': 4.392557963930629}\n",
      "Losses {'ner': 4.3925717803776045}\n",
      "Losses {'ner': 4.39257178474501}\n",
      "Losses {'ner': 4.3925717967262985}\n",
      "Losses {'ner': 4.392571796726917}\n",
      "Losses {'ner': 4.392571796726917}\n",
      "Losses {'ner': 5.533154596639353}\n",
      "Losses {'ner': 5.533154596645226}\n",
      "Losses {'ner': 5.533154596645226}\n",
      "Losses {'ner': 5.533154596645552}\n",
      "Losses {'ner': 5.533154596645555}\n",
      "Losses {'ner': 5.533154596645555}\n",
      "Losses {'ner': 2.1615594685485894e-16}\n",
      "Losses {'ner': 1.8443607796795615e-07}\n",
      "Losses {'ner': 1.8443608712974302e-07}\n",
      "Losses {'ner': 1.8471869732092439e-07}\n",
      "Losses {'ner': 1.847341269299059e-07}\n",
      "Losses {'ner': 1.849811885831125e-07}\n",
      "Losses {'ner': 1.880072951503023e-07}\n",
      "Losses {'ner': 1.880073045746174e-07}\n",
      "Losses {'ner': 1.8800730463518875e-07}\n",
      "Losses {'ner': 1.2475299825063902e-05}\n",
      "Losses {'ner': 4.889161749097048e-05}\n",
      "Losses {'ner': 4.8892360918154045e-05}\n",
      "Losses {'ner': 4.8892360947206354e-05}\n",
      "Losses {'ner': 0.3494860838467806}\n",
      "Losses {'ner': 0.34948608709544593}\n",
      "Losses {'ner': 0.3494860870960638}\n",
      "Losses {'ner': 0.4786569072260128}\n",
      "Losses {'ner': 0.4786569072260128}\n",
      "Losses {'ner': 0.4786569072283071}\n",
      "Losses {'ner': 0.47865690722830756}\n",
      "Losses {'ner': 0.4786569072283086}\n",
      "Losses {'ner': 0.4786569072283086}\n",
      "Losses {'ner': 0.4786569072541716}\n",
      "Losses {'ner': 0.4786569072845039}\n",
      "Losses {'ner': 0.47865690728766586}\n",
      "Losses {'ner': 0.47865690728766586}\n",
      "Losses {'ner': 0.47865690728766586}\n",
      "Losses {'ner': 6.350267412665847e-11}\n",
      "Losses {'ner': 6.350267471088746e-11}\n",
      "Losses {'ner': 4.620074285507254e-05}\n",
      "Losses {'ner': 4.620076011279473e-05}\n",
      "Losses {'ner': 4.6200783749329343e-05}\n",
      "Losses {'ner': 4.6200784083926324e-05}\n",
      "Losses {'ner': 4.620078408392635e-05}\n",
      "Losses {'ner': 4.620254639078503e-05}\n",
      "Losses {'ner': 4.620995466242138e-05}\n",
      "Losses {'ner': 4.620999050714268e-05}\n",
      "Losses {'ner': 4.621178340350818e-05}\n",
      "Losses {'ner': 4.621178478998006e-05}\n",
      "Losses {'ner': 4.6211784792076474e-05}\n",
      "Losses {'ner': 4.621178479413231e-05}\n",
      "Losses {'ner': 4.621178479418886e-05}\n",
      "Losses {'ner': 2.0000924524235613}\n",
      "Losses {'ner': 2.0000924524235613}\n",
      "Losses {'ner': 2.000092452423562}\n",
      "Losses {'ner': 2.000150782264826}\n",
      "Losses {'ner': 2.0170178611055585}\n",
      "Losses {'ner': 2.0170178611055585}\n",
      "Losses {'ner': 2.0170178611056317}\n",
      "Losses {'ner': 2.017050245809196}\n",
      "Losses {'ner': 2.0170502458137944}\n",
      "Losses {'ner': 2.0170588941861807}\n",
      "Losses {'ner': 2.0170588941861807}\n",
      "Losses {'ner': 2.0170588941861816}\n",
      "Losses {'ner': 5.827930740406637e-15}\n",
      "Losses {'ner': 0.0017241721507220136}\n",
      "Losses {'ner': 0.0017241721597181577}\n",
      "Losses {'ner': 0.001724172159718158}\n",
      "Losses {'ner': 0.0017241721599965706}\n",
      "Losses {'ner': 0.0017241721604837796}\n",
      "Losses {'ner': 0.0017243470304565847}\n",
      "Losses {'ner': 0.001724347030456623}\n",
      "Losses {'ner': 0.001724347030456625}\n",
      "Losses {'ner': 0.0017243470304566305}\n",
      "Losses {'ner': 0.0017243470462975617}\n",
      "Losses {'ner': 0.001724347071535806}\n",
      "Losses {'ner': 0.001724347075312284}\n",
      "Losses {'ner': 0.0017243488470647342}\n",
      "Losses {'ner': 0.0017256720978284033}\n",
      "Losses {'ner': 0.0017256720978464416}\n",
      "Losses {'ner': 0.001725672097847195}\n",
      "Losses {'ner': 0.001725731301626952}\n",
      "Losses {'ner': 0.0017257313016306314}\n",
      "Losses {'ner': 0.0017257313017336612}\n",
      "Losses {'ner': 0.0017257313018016088}\n",
      "Losses {'ner': 0.0017257313018016385}\n",
      "Losses {'ner': 0.0017259096571128022}\n",
      "Losses {'ner': 0.0017259096590710558}\n",
      "Losses {'ner': 0.0017259096590710576}\n",
      "Losses {'ner': 0.001725911846157903}\n",
      "Losses {'ner': 0.0017626661823369042}\n",
      "Losses {'ner': 9.795624722214026e-16}\n",
      "Losses {'ner': 6.872224630377337e-13}\n",
      "Losses {'ner': 6.872257637652125e-13}\n",
      "Losses {'ner': 1.6946565100248779e-12}\n",
      "Losses {'ner': 1.928714340060497e-12}\n",
      "Losses {'ner': 1.9996203635920433e-12}\n",
      "Losses {'ner': 4.0738607246084134e-07}\n",
      "Losses {'ner': 4.0738607246084155e-07}\n",
      "Losses {'ner': 1.9998598646190944}\n",
      "Losses {'ner': 1.9998598753008938}\n",
      "Losses {'ner': 1.9998598762752782}\n",
      "Losses {'ner': 1.999859876831222}\n",
      "Losses {'ner': 1.9998598768317488}\n",
      "Losses {'ner': 1.9998598768328726}\n",
      "Losses {'ner': 1.9998598768329032}\n",
      "Losses {'ner': 2.000743215729148}\n",
      "Losses {'ner': 2.000743215729148}\n",
      "Losses {'ner': 2.0007432212836447}\n",
      "Losses {'ner': 3.837316822059631}\n",
      "Losses {'ner': 3.837316822059631}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3.8379405645273654}\n",
      "Losses {'ner': 3.837940564531717}\n",
      "Losses {'ner': 3.8379450889712277}\n",
      "Losses {'ner': 3.8379450889745277}\n",
      "Losses {'ner': 3.837945088974528}\n",
      "Losses {'ner': 3.8379450889745286}\n",
      "Losses {'ner': 3.8379450889745286}\n",
      "Losses {'ner': 5.278755531936441e-10}\n",
      "Losses {'ner': 5.278760905091796e-10}\n",
      "Losses {'ner': 5.281035177989679e-10}\n",
      "Losses {'ner': 5.2811389303701e-10}\n",
      "Losses {'ner': 7.072813942320679e-10}\n",
      "Losses {'ner': 7.171213416530685e-10}\n",
      "Losses {'ner': 3.8622300493243126e-05}\n",
      "Losses {'ner': 3.8622300511398925e-05}\n",
      "Losses {'ner': 3.862230051139996e-05}\n",
      "Losses {'ner': 4.0573416397545235e-05}\n",
      "Losses {'ner': 4.0573417470118966e-05}\n",
      "Losses {'ner': 4.057341747011898e-05}\n",
      "Losses {'ner': 4.068232955494708e-05}\n",
      "Losses {'ner': 4.068235711067227e-05}\n",
      "Losses {'ner': 4.068247637807655e-05}\n",
      "Losses {'ner': 4.072064037636734e-05}\n",
      "Losses {'ner': 4.072064043082508e-05}\n",
      "Losses {'ner': 4.072064229572868e-05}\n",
      "Losses {'ner': 4.072064249429475e-05}\n",
      "Losses {'ner': 4.072064250009855e-05}\n",
      "Losses {'ner': 4.072068695981437e-05}\n",
      "Losses {'ner': 4.0720686959818474e-05}\n",
      "Losses {'ner': 4.072074564004583e-05}\n",
      "Losses {'ner': 4.0720745899720864e-05}\n",
      "Losses {'ner': 1.9901870108764685}\n",
      "Losses {'ner': 1.9901870108764692}\n",
      "Losses {'ner': 1.9901870108764692}\n",
      "Losses {'ner': 9.16341225062816e-13}\n",
      "Losses {'ner': 9.22591509144152e-07}\n",
      "Losses {'ner': 3.290484157324866e-06}\n",
      "Losses {'ner': 3.2909001281529248e-06}\n",
      "Losses {'ner': 3.2909004261239554e-06}\n",
      "Losses {'ner': 3.290900426125103e-06}\n",
      "Losses {'ner': 3.3241657262152284e-06}\n",
      "Losses {'ner': 3.3241659962850827e-06}\n",
      "Losses {'ner': 3.3241659962911144e-06}\n",
      "Losses {'ner': 3.324196804917498e-06}\n",
      "Losses {'ner': 3.3241968049357115e-06}\n",
      "Losses {'ner': 3.324318026573428e-06}\n",
      "Losses {'ner': 3.324318327992457e-06}\n",
      "Losses {'ner': 3.324318926516861e-06}\n",
      "Losses {'ner': 3.5072624839448096e-06}\n",
      "Losses {'ner': 3.507270236626695e-06}\n",
      "Losses {'ner': 3.5072841010322966e-06}\n",
      "Losses {'ner': 3.507284803799495e-06}\n",
      "Losses {'ner': 3.507284803845999e-06}\n",
      "Losses {'ner': 3.5072848038565027e-06}\n",
      "Losses {'ner': 3.5072848072190558e-06}\n",
      "Losses {'ner': 3.5099136222557236e-06}\n",
      "Losses {'ner': 2.2875225793021374}\n",
      "Losses {'ner': 2.2875225793021374}\n",
      "Losses {'ner': 2.2875225793137814}\n",
      "Losses {'ner': 2.2875225793137814}\n",
      "Losses {'ner': 2.2875225793137814}\n",
      "Losses {'ner': 1.5618275343306005e-17}\n",
      "Losses {'ner': 3.7624104947398135e-17}\n",
      "Losses {'ner': 1.6902093409100877e-10}\n",
      "Losses {'ner': 1.6902140858904282e-10}\n",
      "Losses {'ner': 1.6914141732195693e-10}\n",
      "Losses {'ner': 1.6914141923165208e-10}\n",
      "Losses {'ner': 1.6924332099483635e-10}\n",
      "Losses {'ner': 3.286618054441643e-10}\n",
      "Losses {'ner': 3.2866180559579416e-10}\n",
      "Losses {'ner': 3.2866271438442245e-10}\n",
      "Losses {'ner': 3.2866271832935154e-10}\n",
      "Losses {'ner': 1.9465756419607427}\n",
      "Losses {'ner': 1.9466445767147709}\n",
      "Losses {'ner': 1.9466445767148324}\n",
      "Losses {'ner': 1.946644576714833}\n",
      "Losses {'ner': 1.9466445767148415}\n",
      "Losses {'ner': 1.9466445767148415}\n",
      "Losses {'ner': 1.9466445768061882}\n",
      "Losses {'ner': 1.9466445768061889}\n",
      "Losses {'ner': 1.946644594550169}\n",
      "Losses {'ner': 1.946644594550169}\n",
      "Losses {'ner': 1.9466568691528703}\n",
      "Losses {'ner': 1.9466568691531756}\n",
      "Losses {'ner': 1.9466568691531756}\n",
      "Losses {'ner': 2.124871672491938}\n",
      "Losses {'ner': 2.124935043966171}\n",
      "Losses {'ner': 2.1249350439676085}\n",
      "Losses {'ner': 4.3171366485278035e-10}\n",
      "Losses {'ner': 1.329530239537036}\n",
      "Losses {'ner': 1.3295302395374002}\n",
      "Losses {'ner': 1.3295302395377682}\n",
      "Losses {'ner': 1.6559874420740677}\n",
      "Losses {'ner': 1.6559874481916463}\n",
      "Losses {'ner': 1.6559874481916488}\n",
      "Losses {'ner': 1.6559874917447246}\n",
      "Losses {'ner': 1.6559874917447248}\n",
      "Losses {'ner': 1.6559874917467197}\n",
      "Losses {'ner': 1.6559874917467197}\n",
      "Losses {'ner': 1.6559874951270226}\n",
      "Losses {'ner': 1.9074066221146497}\n",
      "Losses {'ner': 3.9072912275223644}\n",
      "Losses {'ner': 3.9072912275223644}\n",
      "Losses {'ner': 3.9072912275223675}\n",
      "Losses {'ner': 3.9072913003740304}\n",
      "Losses {'ner': 3.9072913003740686}\n",
      "Losses {'ner': 3.9072913003740926}\n",
      "Losses {'ner': 3.9073779621175}\n",
      "Losses {'ner': 3.9073779621175464}\n",
      "Losses {'ner': 3.9073783160529105}\n",
      "Losses {'ner': 3.907378316055824}\n",
      "Losses {'ner': 3.9073783160558246}\n",
      "Losses {'ner': 3.9074356081467716}\n",
      "Losses {'ner': 3.9097674349826432}\n",
      "Losses {'ner': 3.9097674349826432}\n",
      "Losses {'ner': 0.023954235017485732}\n",
      "Losses {'ner': 0.023954235017500065}\n",
      "Losses {'ner': 0.023954235796049897}\n",
      "Losses {'ner': 0.02395423579604992}\n",
      "Losses {'ner': 2.064856493560924}\n",
      "Losses {'ner': 2.0648564939481835}\n",
      "Losses {'ner': 2.064856493948186}\n",
      "Losses {'ner': 2.064856493948186}\n",
      "Losses {'ner': 3.134629214285124}\n",
      "Losses {'ner': 3.1346292144492085}\n",
      "Losses {'ner': 3.134629214449223}\n",
      "Losses {'ner': 4.980337942286315}\n",
      "Losses {'ner': 4.980338612850886}\n",
      "Losses {'ner': 4.980338612850887}\n",
      "Losses {'ner': 4.98033861285157}\n",
      "Losses {'ner': 4.98033861285157}\n",
      "Losses {'ner': 4.980338612864689}\n",
      "Losses {'ner': 4.980338612865137}\n",
      "Losses {'ner': 4.9803386128653}\n",
      "Losses {'ner': 4.9803386128725435}\n",
      "Losses {'ner': 4.980338612872615}\n",
      "Losses {'ner': 4.980760853872841}\n",
      "Losses {'ner': 4.980760853899233}\n",
      "Losses {'ner': 4.980760853903255}\n",
      "Losses {'ner': 5.005842369215451}\n",
      "Losses {'ner': 5.005842391220998}\n",
      "Losses {'ner': 5.005842391221}\n",
      "Losses {'ner': 1.746897703641953e-13}\n",
      "Losses {'ner': 0.0007748378767807722}\n",
      "Losses {'ner': 0.0007748378767811129}\n",
      "Losses {'ner': 0.0007748378768422556}\n",
      "Losses {'ner': 0.0007748380120340395}\n",
      "Losses {'ner': 0.0794278607137935}\n",
      "Losses {'ner': 0.07942786071381538}\n",
      "Losses {'ner': 0.08240383331646844}\n",
      "Losses {'ner': 0.08240383331646847}\n",
      "Losses {'ner': 0.08240383376893137}\n",
      "Losses {'ner': 0.08240383376898754}\n",
      "Losses {'ner': 0.0824038337691067}\n",
      "Losses {'ner': 0.08240383376911928}\n",
      "Losses {'ner': 0.08240383376924713}\n",
      "Losses {'ner': 0.08240383378712124}\n",
      "Losses {'ner': 0.08579877996408067}\n",
      "Losses {'ner': 0.0857987800819436}\n",
      "Losses {'ner': 0.16984199976749864}\n",
      "Losses {'ner': 0.1698419997674987}\n",
      "Losses {'ner': 0.16984200940548988}\n",
      "Losses {'ner': 0.1698420153146665}\n",
      "Losses {'ner': 0.17075765001273263}\n",
      "Losses {'ner': 0.1707576500157576}\n",
      "Losses {'ner': 0.17075765001591148}\n",
      "Losses {'ner': 0.1707576500159119}\n",
      "Losses {'ner': 0.17079599117478192}\n",
      "Losses {'ner': 0.170795991174783}\n",
      "Losses {'ner': 5.879136741768585e-12}\n",
      "Losses {'ner': 5.882644377478858e-12}\n",
      "Losses {'ner': 4.9519610871396345e-11}\n",
      "Losses {'ner': 2.871819755460707e-07}\n",
      "Losses {'ner': 0.010953549634886222}\n",
      "Losses {'ner': 0.010953549636751107}\n",
      "Losses {'ner': 0.010953597706559964}\n",
      "Losses {'ner': 0.619597075714381}\n",
      "Losses {'ner': 0.619597075714381}\n",
      "Losses {'ner': 2.619597102886538}\n",
      "Losses {'ner': 2.619597103094206}\n",
      "Losses {'ner': 4.468557264303312}\n",
      "Losses {'ner': 4.468557264303312}\n",
      "Losses {'ner': 4.468557264303375}\n",
      "Losses {'ner': 4.468557264303503}\n",
      "Losses {'ner': 4.468557679629488}\n",
      "Losses {'ner': 4.468557682722199}\n",
      "Losses {'ner': 4.468557682722199}\n",
      "Losses {'ner': 4.4685576827222775}\n",
      "Losses {'ner': 4.468557682722613}\n",
      "Losses {'ner': 5.291220632292424}\n",
      "Losses {'ner': 5.291220632292424}\n",
      "Losses {'ner': 7.248687238551641}\n",
      "Losses {'ner': 7.248687238551643}\n",
      "Losses {'ner': 7.248687238551645}\n",
      "Losses {'ner': 7.248687238551645}\n",
      "Losses {'ner': 7.248687238658722}\n",
      "('CARDINAL', 'DATASET', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MODEL', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TASK', 'TIME', 'WORK_OF_ART')\n"
     ]
    }
   ],
   "source": [
    "# Importing requirements\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "import random\n",
    "\n",
    "examples = []\n",
    "for sentence, annots in TRAIN_DATA:\n",
    "    examples.append(Example.from_dict(nlp.make_doc(sentence), annots))\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "    # Training for 30 iterations     \n",
    "    for itn in range(200):\n",
    "        # shuffle examples before training\n",
    "        random.shuffle(examples)\n",
    "        # Dictionary to store losses\n",
    "        losses = {}\n",
    "        for batch in minibatch(examples, size=4):#, size=sizes):\n",
    "            # Calling update() over the iteration\n",
    "            nlp.update(batch, losses=losses, drop=0.4)\n",
    "            print(\"Losses\", losses)\n",
    "            \n",
    "print(ner.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">9</br>1</br>0</br>2</br> </br>b</br>e</br>F</br> </br>5</br>2</br> </br> </br>]</br></br>G</br>L</br>.</br>s</br>c</br>[</br> </br> </br>2</br>v</br>6</br>9</br>0</br>1</br>1</br>.</br>9</br>0</br>8</br>1</br>:</br>v</br>i</br>X</br>r</br>a</br></br>Published as a conference paper at ICLR 2019</br></br>LARGE SCALE GAN TRAINING FOR</br>HIGH FIDELITY NATURAL IMAGE SYNTHESIS</br></br>Andrew \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brock∗\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " †</br>Heriot-Watt University</br>ajb5@hw.ac.uk</br></br>Jeff Donahue†</br>DeepMind</br>jeffdonahue@google.com</br></br>Karen Simonyan†</br>DeepMind</br>simonyan@google.com</br></br>ABSTRACT</br></br>Despite recent progress in \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    generative image modeling\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       ", successfully generating</br>high-resolution, diverse samples from complex datasets such as \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " remains</br>an elusive goal. To this end, we train \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Generative Adversarial Networks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " at the</br>largest scale yet attempted, and study the instabilities speciﬁc to such scale. We</br>ﬁnd that applying orthogonal regularization to the generator renders it amenable</br>to a simple “truncation trick,” allowing ﬁne control over the trade-off between</br>sample ﬁdelity and variety by reducing the variance of the Generator’s input. Our</br>modiﬁcations lead to models which set the new state of the art in class-conditional</br>image synthesis. When trained on \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " at 128×128 resolution, our models</br>(BigGANs) achieve an Inception Score (IS) of 166.5 and Fr´echet Inception Dis-</br>tance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65.</br></br>1</br></br>INTRODUCTION</br></br>Figure 1: Class-conditional samples generated by our model.</br></br>The state of \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    generative image modeling\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " has advanced dramatically in recent years, with Generative</br>Adversarial Networks (GANs, Goodfellow et al. (2014)) at the forefront of efforts to generate high-</br>ﬁdelity, diverse images with models learned directly from data. \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " training is dynamic, and</br>sensitive to nearly every aspect of its setup (from optimization parameters to model architecture),</br>but a torrent of research has yielded empirical and theoretical insights enabling stable training in</br>a variety of settings. Despite this progress, the current state of the art in conditional \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       "</br>modeling (Zhang et al., 2018) achieves an Inception Score (Salimans et al., 2016) of 52.5, compared</br>to 233 for real data.</br></br>In this work, we set out to close the gap in ﬁdelity and variety between images generated by GANs</br>and real-world images from the \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " dataset. We make the following three contributions to-</br>wards this goal:</br></br>• We demonstrate that GANs beneﬁt dramatically from scaling, and train models with two</br>to four times as many parameters and eight times the batch size compared to prior art. We</br>introduce two simple, general architectural changes that improve scalability, and modify a</br>regularization scheme to improve conditioning, demonstrably boosting performance.</br></br>∗Work done at DeepMind</br>†Equal contribution</br></br>1</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>• As a side effect of our modiﬁcations, our models become amenable to the “truncation</br>trick,” a simple sampling technique that allows explicit, ﬁne-grained control of the trade-</br>off between sample variety and ﬁdelity.</br></br>• We discover instabilities speciﬁc to large scale GANs, and characterize them empirically.</br>Leveraging insights from this analysis, we demonstrate that a combination of novel and</br>existing techniques can reduce these instabilities, but complete training stability can only</br>be achieved at a dramatic cost to performance.</br></br>Our modiﬁcations substantially improve class-conditional GANs. When trained on \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " at</br>128×128 resolution, our models (\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGANs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ") improve the state-of-the-art Inception Score (IS) and</br>Fr´echet Inception Distance (FID) from 52.52 and 18.65 to 166.5 and 7.4 respectively. We also</br>successfully train BigGANs on \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " at 256×256 and 512×512 resolution, and achieve IS and</br>FID of 232.5 and 8.1 at 256×256 and IS and FID of 241.5 and 11.5 at 512×512. Finally, we train</br>our models on an even larger dataset – JFT-300M – and demonstrate that our design choices transfer</br>well from \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ". Code and weights for our pretrained generators are publicly available 1.</br></br>2 BACKGROUND</br></br>A \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Generative Adversarial Network\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ") involves Generator (G) and Discriminator (D) networks</br>whose purpose, respectively, is to map random noise to samples and discriminate real and generated</br>samples. Formally, the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " objective, in its original form (Goodfellow et al., 2014) involves ﬁnding</br>a Nash equilibrium to the following two player min-max problem:</br></br>min</br>G</br></br>max</br>D</br></br>Ex∼qdata(x)[log D(x)] + Ez∼p(z)[log(1 − D(G(z)))],</br></br>(1)</br></br>where z ∈ Rdz is a latent variable drawn from distribution p(z) such as N (0, I) or U[−1, 1].</br>When applied to images, G and D are usually convolutional \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    neural networks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (Radford et al., 2016).</br>Without auxiliary stabilization techniques, this training procedure is notoriously brittle, \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    requiring\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "ﬁnely-tuned hyperparameters and architectural choices to work at all.</br></br>Much recent research has accordingly focused on modiﬁcations to the vanilla \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " procedure to</br>impart stability, drawing on a growing body of empirical and theoretical insights (Nowozin et al.,</br>2016; Sønderby et al., 2017; Fedus et al., 2018). One line of work is focused on changing the</br>objective function (Arjovsky et al., 2017; Mao et al., 2016; Lim &amp; Ye, 2017; Bellemare et al.,</br>2017; Salimans et al., 2018) to encourage convergence. Another line is focused on constraining</br>D through gradient penalties (Gulrajani et al., 2017; Kodali et al., 2017; Mescheder et al., 2018)</br>or normalization (Miyato et al., 2018), both to counteract the use of unbounded loss functions and</br>ensure D provides gradients everywhere to G.</br></br>Of particular relevance to our work is Spectral Normalization (Miyato et al., 2018), which enforces</br>Lipschitz continuity on D by normalizing its parameters with running estimates of their ﬁrst singular</br>values, inducing backwards dynamics that adaptively regularize the top singular direction. Relatedly</br>Odena et al. (2018) analyze the condition number of the Jacobian of G and ﬁnd that performance is</br>dependent on G’s conditioning. Zhang et al. (2018) ﬁnd that employing Spectral Normalization in</br>G improves stability, allowing for fewer D steps per iteration. We extend on these analyses to gain</br>further insight into the pathology of \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " training.</br></br>Other works focus on the choice of architecture, such as \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SA-GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (Zhang et al., 2018) which adds</br>the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    self-attention\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " block from (Wang et al., 2018) to improve the ability of both G and D to model</br>global structure. ProGAN (Karras et al., 2018) trains high-resolution GANs in the single-class</br>setting by training a single model across a sequence of increasing resolutions.</br></br>In conditional GANs (Mirza &amp; Osindero, 2014) class information can be fed into the model in</br>In (Odena et al., 2017) it is provided to G by concatenating a 1-hot class vector</br>various ways.</br>to the noise vector, and the objective is modiﬁed to encourage conditional samples to maximize</br>the corresponding class probability predicted by an auxiliary classiﬁer. de Vries et al. (2017) and</br></br>1https://tfhub.dev/s?q=biggan</br></br>2</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Batch Ch.</br>64</br>256</br>64</br>512</br>64</br>1024</br>64</br>2048</br>96</br>2048</br>96</br>2048</br>96</br>2048</br>96</br>2048</br>64</br>2048</br></br>Param (M)</br>81.5</br>81.5</br>81.5</br>81.5</br>173.5</br>160.6</br>158.3</br>158.3</br>71.3</br></br>Shared</br></br>Skip-z Ortho.</br></br>SA-GAN Baseline</br>(cid:55)</br>(cid:55)</br>(cid:55)</br>(cid:55)</br>(cid:55)</br>(cid:51)</br>(cid:51)</br>(cid:51)</br></br>(cid:55)</br>(cid:55)</br>(cid:55)</br>(cid:55)</br>(cid:51)</br>(cid:51)</br>(cid:51)</br>(cid:51)</br></br>(cid:55)</br>(cid:55)</br>(cid:55)</br>(cid:55)</br>(cid:55)</br>(cid:55)</br>(cid:51)</br>(cid:51)</br></br>Itr ×103</br>1000</br>1000</br>1000</br>732</br>295(±18)</br>185(±11)</br>152(±7)</br>165(±13)</br>371(±7)</br></br>FID</br>18.65</br>15.30</br>14.88</br>12.39</br>9.54(±0.62)</br>9.18(±0.13)</br>8.73(±0.45)</br>8.51(±0.32)</br>10.48(±0.10)</br></br>IS</br>52.52</br>58.77(±1.18)</br>63.03(±1.42)</br>76.85(±3.83)</br>92.98(±4.27)</br>94.94(±1.32)</br>98.76(±2.84)</br>99.31(±2.10)</br>86.90(±0.61)</br></br>Table 1: Fr´echet Inception Distance (FID, lower is better) and Inception Score (IS, higher is better)</br>for ablations of our proposed modiﬁcations. Batch is batch size, Param is total number of param-</br>eters, Ch. is the channel multiplier representing the number of units in each layer, Shared is using</br>shared embeddings, Skip-z is using skip connections from the latent to multiple layers, Ortho. is</br>Orthogonal Regularization, and Itr indicates if the setting is stable to 106 iterations, or it collapses</br>at the given iteration. Other than rows 1-4, results are computed across 8 random initializations.</br></br>Dumoulin et al. (2017) modify the way class conditioning is passed to G by supplying it with class-</br>conditional gains and biases in \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (Ioffe &amp; Szegedy, 2015) layers. In Miyato &amp; Koyama</br>(2018), D is conditioned by using the cosine similarity between its features and a set of learned</br>class embeddings as additional evidence for distinguishing real and generated samples, effectively</br>encouraging generation of samples whose features match a learned class prototype.</br></br>Objectively evaluating implicit generative models is difﬁcult (Theis et al., 2015). A variety of works</br>have proposed heuristics for measuring the sample quality of models without tractable likelihoods</br>(Salimans et al., 2016; Heusel et al., 2017; Bi´nkowski et al., 2018; Wu et al., 2017). Of these,</br>the Inception Score (IS, Salimans et al. (2016)) and Fr´echet Inception Distance (FID, Heusel et al.</br>(2017)) have become popular despite their notable ﬂaws (Barratt &amp; Sharma, 2018). We employ</br>them as approximate measures of sample quality, and to enable comparison against previous work.</br></br>3 SCALING UP GANS</br></br>In this section, we explore methods for scaling up \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " training to reap the performance beneﬁts of</br>larger models and larger batches. As a baseline, we employ the \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SA-GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " architecture of Zhang et al.</br>(2018), which uses the hinge loss (Lim &amp; Ye, 2017; Tran et al., 2017) \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " objective. We provide</br>class information to G with class-conditional \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (Dumoulin et al., 2017; de Vries et al.,</br>2017) and to D with projection (Miyato &amp; Koyama, 2018). The optimization settings follow Zhang</br>et al. (2018) (notably employing Spectral Norm in G) with the modiﬁcation that we halve the learning</br>rates and take two D steps per G step. For evaluation, we employ moving averages of G’s weights</br>following Karras et al. (2018); Mescheder et al. (2018); Yazc et al. (2018), with a decay of 0.9999.</br>We use Orthogonal Initialization (Saxe et al., 2014), whereas previous works used N (0, 0.02I)</br>(Radford et al., 2016) or Xavier initialization (Glorot &amp; Bengio, 2010). Each model is trained on</br>128 to 512 cores of a Google TPUv3 Pod (Google, 2018), and computes \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " statistics in G</br>across all devices, rather than per-device as is typical. We ﬁnd progressive growing (Karras et al.,</br>2018) unnecessary even for our 512×512 models. Additional details are in Appendix C.</br></br>We begin by increasing the batch size for the baseline model, and immediately ﬁnd tremendous</br>beneﬁts in doing so. Rows 1-4 of Table 1 show that simply increasing the batch size by a factor of</br>8 improves the state-of-the-art IS by 46%. We conjecture that this is a result of each batch covering</br>more modes, providing better gradients for both networks. One notable side effect of this scaling is</br>that our models reach better ﬁnal performance in fewer iterations, but become unstable and undergo</br>complete training collapse. We discuss the causes and ramiﬁcations of this in Section 4. For these</br>experiments, we report scores from checkpoints saved just before collapse.</br></br>We then increase the width (number of channels) in each layer by 50%, approximately doubling the</br>number of parameters in both models. This leads to a further IS improvement of 21%, which we</br>posit is due to the increased capacity of the model relative to the complexity of the dataset. Doubling</br></br>3</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>(a)</br></br>(b)</br></br>Figure 2: (a) The effects of increasing truncation. From left to right, the threshold is set to 2, 1, 0.5,</br>0.04. (b) Saturation artifacts from applying truncation to a poorly conditioned model.</br></br>the depth did not initially lead to improvement – we addressed this later in the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "-deep model,</br>which uses a different residual block structure.</br></br>We note that class embeddings c used for the conditional \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " layers in G contain a large</br>number of weights. Instead of having a separate layer for each embedding (Miyato et al., 2018;</br>Zhang et al., 2018), we opt to use a shared embedding, which is linearly projected to each layer’s</br>gains and biases (Perez et al., 2018). This reduces computation and memory costs, and improves</br>training speed (in number of iterations required to reach a given performance) by 37%. Next, we</br>add direct skip connections (skip-z) from the noise vector z to multiple layers of G rather than just</br>the initial layer. The intuition behind this design is to allow G to use the latent space to directly in-</br>ﬂuence features at different resolutions and levels of hierarchy. In BigGAN, this is accomplished by</br>splitting z into one chunk per resolution, and concatenating each chunk to the conditional vector c</br>which gets projected to the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " gains and biases. In BigGAN-deep, we use an even simpler</br>design, concatenating the entire z with the conditional vector without splitting it into chunks. Pre-</br>vious works (Goodfellow et al., 2014; Denton et al., 2015) have considered variants of this concept;</br>our implementation is a minor modiﬁcation of this design. Skip-z provides a modest performance</br>improvement of around 4%, and improves training speed by a further 18%.</br></br>3.1 TRADING OFF VARIETY AND FIDELITY WITH THE TRUNCATION TRICK</br></br>Unlike models which need to backpropagate through their latents, GANs can employ an arbitrary</br>prior p(z), yet the vast majority of previous works have chosen to draw z from either N (0, I) or</br>U[−1, 1]. We question the optimality of this choice and explore alternatives in Appendix E.</br></br>Remarkably, our best results come from using a different latent distribution for sampling than was</br>used in training. Taking a model trained with z ∼ N (0, I) and sampling z from a truncated nor-</br>mal (where values which fall outside a range are resampled to fall inside that range) immediately</br>provides a boost to IS and FID. We call this the Truncation Trick:</br>truncating a z vector by re-</br>sampling the values with magnitude above a chosen threshold leads to improvement in individual</br>sample quality at the cost of reduction in overall sample variety. Figure 2(a) demonstrates this: as</br>the threshold is reduced, and elements of z are truncated towards zero (the mode of the latent dis-</br>tribution), individual samples approach the mode of G’s output distribution. Related observations</br>about this trade-off were made in (Marchesi, 2016; Pieters &amp; Wiering, 2014).</br></br>This technique allows ﬁne-grained, post-hoc selection of the trade-off between sample quality and</br>variety for a given G. Notably, we can compute FID and IS for a range of thresholds, obtaining the</br>variety-ﬁdelity curve reminiscent of the precision-recall curve (Figure 17). As IS does not penal-</br>ize lack of variety in class-conditional models, reducing the truncation threshold leads to a direct</br>increase in IS (analogous to precision). FID penalizes lack of variety (analogous to recall) but also</br>rewards precision, so we initially see a moderate improvement in FID, but as truncation approaches</br>zero and variety diminishes, the FID sharply drops. The distribution shift caused by sampling with</br>different latents than those seen in training is problematic for many models. Some of our larger</br>models are not amenable to truncation, \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    producing saturation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " artifacts (Figure 2(b)) when fed trun-</br>cated noise. To counteract this, we seek to enforce amenability to truncation by conditioning G to be</br>smooth, so that the full space of z will map to good output samples. For this, we turn to Orthogonal</br>Regularization (Brock et al., 2017), which directly enforces the orthogonality condition:</br></br>4</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Rβ(W ) = β(cid:107)W (cid:62)W − I(cid:107)2</br>F,</br></br>(2)</br></br>where W is a weight matrix and β a hyperparameter. This regularization is known to often be too</br>limiting (Miyato et al., 2018), so we explore several variants designed to relax the constraint while</br>still imparting the desired smoothness to our models. The version we ﬁnd to work best removes the</br>diagonal terms from the regularization, and aims to minimize the pairwise cosine similarity between</br>ﬁlters but does not constrain their norm:</br></br>Rβ(W ) = β(cid:107)W (cid:62)W (cid:12) (1 − I)(cid:107)2</br>F,</br>where 1 denotes a matrix with all elements set to 1. We sweep β values and select 10−4, ﬁnding</br>this small added penalty sufﬁcient to improve the likelihood that our models will be amenable to</br>truncation. Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of</br>models are amenable to truncation, compared to 60% when trained with Orthogonal Regularization.</br></br>(3)</br></br>We ﬁnd that current \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " techniques are sufﬁcient to enable scaling to large models and distributed,</br>large-batch training. We ﬁnd that we can dramatically improve the state of the art and train models</br>up to 512×512 resolution without need for explicit multiscale methods like Karras et al. (2018).</br>Despite these improvements, our models undergo training collapse, necessitating early stopping in</br>practice. In the next two sections we investigate why settings which were stable in previous works</br>become unstable when applied at scale.</br></br>3.2 SUMMARY</br></br>4 ANALYSIS</br></br>(a) G</br></br>(b) D</br></br>Figure 3: A typical plot of the ﬁrst singular value σ0 in the layers of G (a) and D (b) before Spectral</br>Normalization. Most layers in G have well-behaved spectra, but without constraints a small sub-</br>set grow throughout training and explode at collapse. D’s spectra are noisier but otherwise better-</br>behaved. Colors from red to violet indicate increasing depth.</br></br>4.1 CHARACTERIZING INSTABILITY: THE GENERATOR</br></br>Much previous work has investigated \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " stability from a variety of analytical angles and on</br>toy problems, but the instabilities we observe occur for settings which are stable at small scale,</br>necessitating direct analysis at large scale. We monitor a range of weight, gradient, and loss statistics</br>during training, in search of a metric which might presage the onset of training collapse, similar to</br>(Odena et al., 2018). We found the top three singular values σ0, σ1, \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    σ2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " of each weight matrix to be</br>the most informative. They can be efﬁciently computed using the Alrnoldi iteration method (Golub</br>&amp; der Vorst, 2000), which extends the power iteration method, used in Miyato et al. (2018), to</br>estimation of additional singular vectors and values. A clear pattern emerges, as can be seen in</br>Figure 3(a) and Appendix F: most G layers have well-behaved spectral norms, but some layers</br></br>5</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>(typically the ﬁrst layer in G, which is over-complete and not convolutional) are ill-behaved, with</br>spectral norms that grow throughout training and explode at collapse.</br></br>To ascertain if this pathology is a cause of collapse or merely a symptom, we study the effects of</br>imposing additional conditioning on G to explicitly counteract spectral explosion. First, we directly</br>regularize the top singular values σ0 of each weight, either towards a ﬁxed value σreg or towards</br>some ratio r of the second singular value, r · sg(σ1) (with sg the stop-gradient operation to prevent</br>the regularization from increasing σ1). Alternatively, we employ a partial singular value decompo-</br>sition to instead clamp σ0. Given a weight W , its ﬁrst singular vectors u0 and v0, and σclamp the</br>value to which the σ0 will be clamped, our weights become:</br></br>W = W − max(0, σ0 − σclamp)v0u(cid:62)</br>0 ,</br>where σclamp is set to either σreg or r · sg(σ1). We observe that both with and without Spectral</br>Normalization these techniques have the effect of preventing the gradual increase and explosion of</br>either σ0 or σ0</br>, but even though in some cases they mildly improve performance, no combination</br>σ1</br>prevents training collapse. This evidence suggests that while conditioning G might improve stability,</br>it is insufﬁcient to ensure stability. We accordingly turn our \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    attention\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " to D.</br></br>(4)</br></br>4.2 CHARACTERIZING INSTABILITY: THE DISCRIMINATOR</br></br>As with G, we analyze the spectra of D’s weights to gain insight into its behavior, then seek to</br>stabilize training by imposing additional constraints. Figure 3(b) displays a typical plot of σ0 for D</br>(with further plots in Appendix F). Unlike G, we see that the spectra are noisy, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    σ0\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       "</br>is well-behaved,</br>σ1</br>and the singular values grow throughout training but only jump at collapse, instead of exploding.</br></br>The spikes in D’s spectra might suggest that it periodically receives very large gradients, but we</br>observe that the Frobenius norms are smooth (Appendix F), suggesting that this effect is primarily</br>concentrated on the top few singular directions. We posit that this noise is a result of optimization</br>through the adversarial training process, where G periodically produces batches which strongly per-</br>turb D . If this spectral noise is causally related to instability, a natural counter is to employ gradient</br>penalties, which explicitly regularize changes in D’s Jacobian. We explore the R1 zero-centered</br>gradient penalty from Mescheder et al. (2018):</br></br>R1 :=</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    EpD(x\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ")</br></br>(cid:2)(cid:107)∇D(x)(cid:107)2</br></br>(\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cid:3\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ") .</br></br>F</br></br>γ</br>2</br></br>(5)</br></br>With the default suggested γ strength of 10, training becomes stable and improves the smoothness</br>and boundedness of spectra in both G and D, but performance severely degrades, resulting in a 45%</br>reduction in IS. Reducing the penalty partially alleviates this degradation, but results in increasingly</br>ill-behaved spectra; even with the penalty strength reduced to 1 (the lowest strength for which sud-</br>den collapse does not occur) the IS is reduced by 20%. Repeating this experiment with various</br>strengths of Orthogonal Regularization, \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    DropOut\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (Srivastava et al., 2014), and L2 (See Appendix I</br>for details), reveals similar behaviors for these regularization strategies: with high enough penalties</br>on D, training stability can be achieved, but at a substantial cost to performance.</br></br>We also observe that D’s loss approaches zero during training, but undergoes a sharp upward jump at</br>collapse (Appendix F). One possible explanation for this behavior is that D is overﬁtting to the train-</br>ing set, memorizing training examples rather than learning some meaningful boundary between real</br>and generated images. As a simple test for D’s memorization (related to Gulrajani et al. (2017)), we</br>evaluate uncollapsed discriminators on the \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " training and validation sets, and measure what</br>percentage of samples are classiﬁed as real or generated. While the training accuracy is consistently</br>above 98%, the validation accuracy falls in the range of 50-55%, no better than \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    random guessing\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       "</br>(regardless of regularization strategy). This conﬁrms that D is indeed memorizing the training set;</br>we deem this in line with D’s role, which is not explicitly to generalize, but to distill the training</br>data and provide a useful learning signal for G. Additional experiments and discussion are provided</br>in Appendix G.</br></br>4.3 SUMMARY</br></br>We ﬁnd that stability does not come solely from G or D, but from their interaction through the</br>adversarial training process. While the symptoms of their poor conditioning can be used to track and</br></br>6</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Model</br>SN-GAN</br>SA-GAN</br>BigGAN</br>BigGAN</br>BigGAN</br>BigGAN-deep</br>BigGAN-deep</br>BigGAN-deep</br></br>Res.</br>128</br>128</br>128</br>256</br>512</br>128</br>256</br>512</br></br>FID/IS</br>27.62/36.80</br>18.65/52.52</br>8.7 ± .6/98.8 ± 3</br>8.7 ± .1/142.3 ± 2</br>8.1/144.2</br>5.7 ± .3/124.5 ± 2</br>6.9 ± .2/171.4 ± 2</br>7.5/152.8</br></br>(min FID) / IS</br>N/A</br>N/A</br>7.7 ± .2/126.5 ± 0</br>7.7 ± .1/178.0 ± 5</br>7.6/170.3</br>6.3 ± .3/148.1 ± 4</br>7.0 ± .1/202.6 ± 2</br>7.7/181.4</br></br>FID / (valid IS)</br>N/A</br>N/A</br>9.6 ± .4/166.3 ± 1</br>9.3 ± .3/233.1 ± 1</br>11.8/241.4</br>7.4 ± .6/166.5 ± 1</br>8.1 ± .1/232.5 ± 2</br>11.5/241.5</br></br>FID / (max IS)</br>N/A</br>N/A</br>25 ± 2/206 ± 2</br>25 ± 5/291 ± 4</br>27.0/275</br>25 ± 2/253 ± 11</br>27 ± 8/317 ± 6</br>39.7/298</br></br>Table 2: Evaluation of models at different resolutions. We report scores without truncation (Column</br>3), scores at the best FID (Column 4), scores at the IS of validation data (Column 5), and scores at</br>the max IS (Column 6). Standard deviations are computed over at least three random initializations.</br></br>identify instability, ensuring reasonable conditioning proves necessary for training but insufﬁcient to</br>prevent eventual training collapse. It is possible to enforce stability by strongly constraining D, but</br>doing so incurs a dramatic cost in performance. With current techniques, better ﬁnal performance</br>can be achieved by relaxing this conditioning and allowing collapse to occur at the later stages of</br>training, by which time a model is sufﬁciently trained to achieve good results.</br></br>5 EXPERIMENTS</br></br>(a) 128×128</br></br>(b) 256×256</br></br>(c) 512×512</br></br>(d)</br></br>Figure 4: Samples from our \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " model with truncation threshold 0.5 (a-c) and an example of</br>class leakage in a partially trained model (d).</br></br>5.1 EVALUATION ON \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    IMAGENET\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       "</br></br>We evaluate our models on \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " ILSVRC 2012 (Russakovsky et al., 2015) at 128×128,</br>256×256, and 512×512 resolutions, employing the settings from Table 1, row 8. The samples</br>generated by our models are presented in Figure 4, with additional samples in Appendix A, and on-</br>line 2. We report IS and FID in Table 2. As our models are able to trade sample variety for quality, it</br>is unclear how best to compare against prior art; we accordingly report values at three settings, with</br>complete curves in Appendix D. First, we report the FID/IS values at the truncation setting which</br>attains the best FID. Second, we report the FID at the truncation setting for which our model’s IS is</br>the same as that attained by the real validation data, reasoning that this is a passable measure of max-</br>imum sample variety achieved while still achieving a good level of “objectness.” Third, we report</br>FID at the maximum IS achieved by each model, to demonstrate how much variety must be traded</br>off to maximize quality. In all three cases, our models outperform the previous state-of-the-art IS</br>and FID scores achieved by Miyato et al. (2018) and Zhang et al. (2018).</br></br>In addition to the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " model introduced in the ﬁrst version of the paper and used in the majority</br>of experiments (unless otherwise stated), we also present a 4x deeper model (\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "-deep) which</br>uses a different conﬁguration of residual blocks. As can be seen from Table 2, \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "-deep sub-</br>stantially outperforms BigGAN across all resolutions and metrics. This conﬁrms that our ﬁndings</br></br>2https://drive.google.com/drive/folders/1lWC6XEPD0LT5KUnPXeve_kWeY-FxH002</br></br>7</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Skip-z Ortho.</br></br>Ch.</br>64</br>64</br>96</br>128</br></br>Param (M)</br>317.1</br>99.4</br>207.9</br>355.7</br></br>Shared</br>(cid:55)</br>(cid:51)</br>(cid:51)</br>(cid:51)</br></br>(cid:55)</br>(cid:51)</br>(cid:51)</br>(cid:51)</br></br>(cid:55)</br>(cid:51)</br>(cid:51)</br>(cid:51)</br></br>FID</br>48.38</br>23.48</br>18.84</br>13.75</br></br>IS</br>23.27</br>24.78</br>27.86</br>30.61</br></br>(min FID) / IS</br>48.6/23.1</br>22.4/21.0</br>17.1/23.3</br>13.0/28.0</br></br>FID / (max IS)</br>49.1/23.9</br>60.9/35.8</br>51.6/38.1</br>46.2/47.8</br></br>Table 3: BigGAN results on JFT-300M at 256×256 resolution. The FID and IS columns report these</br>scores given by the JFT-300M-trained Inception v2 classiﬁer with noise distributed as z ∼ N (0, I)</br>(non-truncated). The (min FID) / IS and FID / (max IS) columns report scores at the best FID and</br>IS from a sweep across truncated noise distributions ranging from σ = 0 to σ = 2. Images from the</br>JFT-300M validation set have an IS of 50.88 and FID of 1.94.</br></br>extend to other architectures, and that increased depth leads to improvement in sample quality. Both</br>BigGAN and BigGAN-deep architectures are described in Appendix B.</br></br>Our observation that D overﬁts to the training set, coupled with our model’s sample quality, raises</br>the obvious question of whether or not G simply memorizes training points. To test this, we perform</br>class-wise nearest neighbors analysis in pixel space and the feature space of pre-trained classiﬁer</br>networks (Appendix A). In addition, we present both interpolations between samples and class-wise</br>interpolations (where z is held constant) in Figures 8 and 9. Our model convincingly interpolates</br>between disparate samples, and the nearest neighbors for its samples are visually distinct, suggesting</br>that our model does not simply memorize training data.</br></br>We note that some failure modes of our partially-trained models are distinct from those previously</br>observed. Most previous failures involve local artifacts (Odena et al., 2016), images consisting</br>of texture blobs instead of objects (Salimans et al., 2016), or the canonical mode collapse. We</br>observe class leakage, where images from one class contain properties of another, as exempliﬁed</br>by Figure 4(d). We also ﬁnd that many classes on \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " are more difﬁcult than others for our</br>model; our model is more successful at generating dogs (which make up a large portion of the</br>dataset, and are mostly distinguished by their texture) than crowds (which comprise a small portion</br>of the dataset and have more large-scale structure). Further discussion is available in Appendix A.</br></br>5.2 ADDITIONAL EVALUATION ON JFT-300M</br></br>To conﬁrm that our design choices are effective for even larger and more complex and diverse</br>datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017). The</br>full JFT-300M dataset contains 300M real-world images labeled with 18K categories. Since the</br>category distribution is heavily long-tailed, we subsample the dataset to keep only images with the</br>8.5K most common labels. The resulting dataset contains 292M images – two orders of magnitude</br>larger than \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ". For images with multiple labels, we sample a single label randomly and</br>independently whenever an image is sampled. To compute IS and FID for the GANs trained on this</br>dataset, we use an Inception v2 classiﬁer (Szegedy et al., 2016) trained on this dataset. Quantitative</br>results are presented in Table 3. All models are trained with batch size 2048. We compare an ablated</br>version of our model – comparable to \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SA-GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (Zhang et al., 2018) but with the larger batch size</br>– against a “full” \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " model that makes uses of all of the techniques applied to obtain the</br>best results on \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " (shared embedding, skip-z, and orthogonal regularization). Our results</br>show that these techniques substantially improve performance even in the setting of this much larger</br>dataset at the same model capacity (64 base channels). We further show that for a dataset of this</br>scale, we see signiﬁcant additional improvements from expanding the capacity of our models to 128</br>base channels, while for \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " GANs that additional capacity was not beneﬁcial.</br></br>In Figure 19 (Appendix D), we present truncation plots for models trained on this dataset. Unlike</br>for \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", where truncation limits of σ ≈ 0 tend to produce the highest ﬁdelity scores, IS is</br>typically maximized for our JFT-300M models when the truncation value σ ranges from 0.5 to 1.</br>We suspect that this is at least partially due to the intra-class variability of JFT-300M labels, as well</br>as the relative complexity of the image distribution, which includes images with multiple objects at a</br>variety of scales. Interestingly, unlike models trained on \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", where training tends to collapse</br>without heavy regularization (Section 4), the models trained on JFT-300M remain stable over many</br></br>8</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>hundreds of thousands of iterations. This suggests that moving beyond \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " to larger datasets</br>may partially alleviate \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " stability issues.</br></br>The improvement over the baseline \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " model that we achieve on this dataset without changes to</br>the underlying models or training and \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    regularization techniques\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " (beyond expanded capacity) demon-</br>strates that our ﬁndings extend from \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " to datasets with scale and complexity thus far un-</br>precedented for \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    generative\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " models of images.</br></br>We have demonstrated that \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Generative Adversarial Networks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " trained to model natural images of</br>multiple categories highly beneﬁt from scaling up, both in terms of ﬁdelity and variety of the gen-</br>erated samples. As a result, our models set a new level of performance among \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " GAN</br>models, improving on the state of the art by a large margin. We have also presented an analysis</br>of the training behavior of large scale GANs, characterized their stability in terms of the singular</br>values of their weights, and discussed the interplay between stability and performance.</br></br>We would like to thank Kai Arulkumaran, Matthias Bauer, Peter Buchlovsky, Jeffrey Defauw,</br>Sander Dieleman, Ian Goodfellow, Ariel Gordon, Karol Gregor, Dominik Grewe, Chris Jones, Jacob</br>Menick, Augustus Odena, Suman Ravuri, Ali Razavi, Mihaela Rosca, and Jeff Stanway.</br></br>6 \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CONCLUSION\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "</br></br>ACKNOWLEDGMENTS</br></br>REFERENCES</br></br>Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu</br>Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,</br>Rajat Monga, Sherry Moore, Derek Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete</br>Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A system for large-scale</br>machine learning. In OSDI, 2016.</br></br>Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.</br></br>Shane Barratt and Rishi Sharma. A note on the Inception Score. In arXiv preprint arXiv:1801.01973,</br></br>In ICML, 2017.</br></br>2018.</br></br>Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,</br>Stephan Hoyer, and R´emi Munos. The \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cramer\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " distance as a solution to biased Wasserstein gra-</br>dients. In arXiv preprint arXiv:1705.10743, 2017.</br></br>Mikolaj Bi´nkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD</br></br>GANs. In ICLR, 2018.</br></br>Andrew Brock, Theodore Lim, J.M. Ritchie, and Nick Weston. Neural photo editing with introspec-</br></br>tive \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    adversarial\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " networks. In ICLR, 2017.</br></br>Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:</br>Interpretable representation learning by \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    information maximizing\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " generative adversarial nets. In</br>NIPS, 2016.</br></br>Harm de Vries, Florian Strub, J´er´emie Mary, \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hugo Larochelle\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       ", Olivier Pietquin, and Aaron</br></br>Courville. Modulating early visual processing by language. In \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NIPS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 2017.</br></br>Emily Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    generative image\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " models</br></br>using a laplacian pyramid of adversarial networks. In NIPS, 2015.</br></br>Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic</br></br>style. In ICLR, 2017.</br></br>9</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and</br>Ian Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every</br>step. In ICLR, 2018.</br></br>Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural</br></br>networks. In \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AISTATS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 2010.</br></br>Gene Golub and Henk Van der Vorst. Eigenvalue computation in the 20th century. \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Journal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " of</br></br>Computational and Applied Mathematics, 123:35–65, 2000.</br></br>Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,</br></br>and Aaron Courville Yoshua Bengio. \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Generative adversarial nets\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ". In NIPS, 2014.</br></br>Google. Cloud TPUs. https://cloud.google.com/tpu/, 2018.</br></br>Ishaan Gulrajani, Faruk Ahmed, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mart´ın\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " Arjovsky, \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vincent Dumoulin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ", and Aaron C. Courville. Im-</br></br>proved training of Wasserstein GANs. In NIPS, 2017.</br></br>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    residual learning\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " for image recog-</br></br>nition. In \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CVPR\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 2016.</br></br>Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G¨unter Klambauer, and</br>Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilib-</br>rium. In NIPS, 2017.</br></br>Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by</br></br>reducing internal covariate shift. In ICML, 2015.</br></br>Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for</br></br>improved quality, stability, and variation. In ICLR, 2018.</br></br>Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.</br></br>Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of</br></br>GANs. In arXiv preprint arXiv:1705.07215, 2017.</br></br>Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.</br></br>Jae Hyun Lim and Jong Chul Ye. Geometric \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ". In arXiv preprint arXiv:1705.02894, 2017.</br></br>Xudong Mao, \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Qing\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " Li, Haoran Xie, Raymond Y. K. Lau, and Zhen Wang. Least squares generative</br></br>adversarial networks. In arXiv preprint arXiv:1611.04076, 2016.</br></br>Marco Marchesi. Megapixel size image creation using generative adversarial networks. In arXiv</br></br>preprint arXiv:1706.00082, 2016.</br></br>actually converge? In ICML, 2018.</br></br>arXiv:1411.1784, 2014.</br></br>Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do</br></br>Mehdi Mirza and Simon Osindero. Conditional \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    generative adversarial nets\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ".</br></br>In arXiv preprint</br></br>Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In ICLR, 2018.</br></br>Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization</br></br>for \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    generative adversarial networks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ". In ICLR, 2018.</br></br>Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural sam-</br></br>plers using variational divergence minimization. In \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NIPS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 2016.</br></br>Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts.</br></br>Distill, 2016.</br></br>Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-</br></br>iary classiﬁer GANs. In ICML, 2017.</br></br>10</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B. Brown, Christopher Olah, Colin Raf-</br>fel, and Ian Goodfellow. Is generator conditioning causally related to \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " performance?</br>In</br>ICML, 2018.</br></br>Ethan Perez, Florian Strub, Harm de Vries, \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vincent Dumoulin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ", and Aaron Courville. FiLM: Visual</br></br>reasoning with a general conditioning layer. In \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AAAI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 2018.</br></br>Mathijs Pieters and Marco Wiering. Comparing generative adversarial network techniques for image</br></br>creation and modiﬁcatio. In arXiv preprint arXiv:1803.09093, 2014.</br></br>Alec Radford, Luke Metz, and Soumith Chintala. \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Unsupervised representation learning\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " with deep</br></br>convolutional \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    generative adversarial networks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ". In ICLR, 2016.</br></br>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Zhiheng\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " large scale visual</br></br>Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein.</br>recognition challenge. \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    IJCV\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 115:211–252, 2015.</br></br>Tim Salimans and Diederik Kingma. Weight normalization: A simple reparameterization to accel-</br></br>erate training of deep \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    neural networks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ". In \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NIPS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 2016.</br></br>Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.</br></br>Improved techniques for training GANs. In NIPS, 2016.</br></br>Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal</br></br>transport. In ICLR, 2018.</br></br>Andrew Saxe, James McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of</br></br>learning in deep linear \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    neural networks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ". In ICLR, 2014.</br></br>Karen Simonyan and Andrew Zisserman. Very deep \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    convolutional\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " networks for large-scale image</br></br>recognition. In ICLR, 2015.</br></br>Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszr. Amortised</br></br>map inference for \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    image super\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       "-resolution. In ICLR, 2017.</br></br>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</br>Dropout: A simple way to prevent \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    neural networks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " from overﬁtting. JMLR, 15:1929–1958, 2014.</br></br>Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-</br></br>fectiveness of data in deep learning era. In \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ICCV\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 2017.</br></br>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-</br></br>thinking the inception architecture for computer vision. In \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CVPR\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 2016.</br></br>Lucas Theis, A¨aron van den Oord, and Matthias Bethge. A note on the evaluation of generative</br></br>models. In arXiv preprint arXiv:1511.01844, 2015.</br></br>Dustin Tran, Rajesh Ranganath, and David M. Blei. Hierarchical implicit models and likelihood-free</br></br>variational inference. In NIPS, 2017.</br></br>Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    neural networks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ". In</br></br>CVPR, 2018.</br></br>Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger B. Grosse. On the quantitative analysis</br></br>of decoder-based \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    generative\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " models. In ICLR, 2017.</br></br>Yasin Yazc, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Georgios Piliouras\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       ", and Vijay</br>In arXiv preprint</br></br>Chandrasekhar. The unusual effectiveness of averaging in gan training.</br>arXiv:1806.04498, 2018.</br></br>Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative</br></br>adversarial networks. In arXiv preprint arXiv:1805.08318, 2018.</br></br>11</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>APPENDIX A ADDITIONAL SAMPLES, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    INTERPOLATIONS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", AND NEAREST</br></br>NEIGHBORS FROM \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    IMAGENET\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " MODELS</br></br>Figure 5: Samples generated by our BigGAN model at 256×256 resolution.</br></br>Figure 6: Samples generated by our BigGAN model at 512×512 resolution.</br></br>12</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>(a)</br></br>(b)</br></br>Figure 7: Comparing easy classes (a) with difﬁcult classes (b) at 512×512. Classes such as dogs</br>which are largely textural, and common in the dataset, are far easier to model than classes involving</br>unaligned human faces or crowds. Such classes are more dynamic and structured, and often have</br>details to which human observers are more sensitive. The difﬁculty of modeling global structure is</br>further exacerbated when producing high-resolution images, even with non-local blocks.</br></br>Figure 8: Interpolations between z, c pairs.</br></br>13</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Figure 9: Interpolations between c with z held constant. Pose semantics are frequently maintained</br>between endpoints (particularly in the ﬁnal row). Row 2 demonstrates that grayscale is encoded in</br>the joint z, c space, rather than in z.</br></br>Figure 10: Nearest neighbors in VGG-16-fc7 (Simonyan &amp; Zisserman, 2015) feature space. The</br>generated image is in the top left.</br></br>14</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Figure 11: Nearest neighbors in ResNet-50-avgpool (He et al., 2016) feature space. The generated</br>image is in the top left.</br></br>Figure 12: Nearest neighbors in pixel space. The generated image is in the top left.</br></br>15</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Figure 13: Nearest neighbors in VGG-16-fc7 (Simonyan &amp; Zisserman, 2015) feature space. The</br>generated image is in the top left.</br></br>Figure 14: Nearest neighbors in ResNet-50-avgpool (He et al., 2016) feature space. The generated</br>image is in the top left.</br></br>16</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>APPENDIX B ARCHITECTURAL DETAILS</br></br>In the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " model (Figure 15), we use the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (He et al., 2016) \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " architecture of (Zhang</br>et al., 2018), which is identical to that used by (Miyato et al., 2018), but with the channel pattern</br>in D modiﬁed so that the number of ﬁlters in the ﬁrst \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    convolutional\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " layer of each block is equal</br>to the number of output ﬁlters (rather than the number of input ﬁlters, as in Miyato et al. (2018);</br>Gulrajani et al. (2017)). We use a single shared class embedding in G, and skip connections for</br>the latent vector z (skip-z). In particular, we employ hierarchical latent spaces, so that the latent</br>vector z is split along its \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    channel dimension\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " into chunks of equal size (20-D in our case), and each</br>chunk is concatenated to the shared class embedding and passed to a corresponding residual block</br>as a conditioning vector. The conditioning of each block is linearly projected to produce per-sample</br>gains and biases for the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " layers of the block. The bias projections are zero-centered,</br>while the gain projections are centered at 1. Since the number of residual blocks depends on the</br>image resolution, the full dimensionality of z is 120 for 128 × 128, 140 for 256 × 256, and 160 for</br>512 × 512 images.</br></br>The \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN-deep\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " model (Figure 16) differs from BigGAN in several aspects. It uses a simpler vari-</br>ant of skip-z conditioning: instead of ﬁrst splitting z into chunks, we concatenate the entire z with</br>the class embedding, and pass the resulting vector to each residual block through skip connections.</br>BigGAN-deep is based on residual blocks with bottlenecks (He et al., 2016), which incorporate</br>two additional 1 × 1 convolutions: the ﬁrst reduces the number of channels by a factor of 4 before</br>the more expensive 3 × 3 convolutions; the second produces the required number of output chan-</br>nels. While BigGAN relies on 1 × 1 convolutions in the skip connections whenever the number of</br>channels needs to change, in \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN-\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "deep we use a different strategy aimed at preserving identity</br>throughout the skip connections. In G, where the number of channels needs to be reduced, we sim-</br>ply retain the ﬁrst group of channels and drop the rest to produce the required number of channels.</br>In D, where the number of channels should be increased, we pass the input channels unperturbed,</br>and concatenate them with the remaining channels produced by a 1 × 1 convolution. As far as the</br>network conﬁguration is concerned, the discriminator is an exact reﬂection of the generator. There</br>are two blocks at each resolution (\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " uses one), and as a result BigGAN-deep is four times</br>deeper than \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       ". Despite their increased depth, the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "-deep models have signiﬁcantly</br>fewer parameters mainly due to the bottleneck structure of their residual blocks. For example, the</br>128 × 128 BigGAN-deep G and D have 50.4M and 34.6M parameters respectively, while the corre-</br>sponding original BigGAN models have 70.4M and 88.0M parameters. All BigGAN-deep models</br>use \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    attention\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " at 64 × 64 resolution, channel width multiplier ch = 128, and z ∈ R128.</br></br>(a)</br></br>(b)</br></br>(c)</br></br>Figure 15: (a) A typical architectural layout for \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "’s G; details are in the following tables.</br>(b) A Residual Block (\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up) in BigGAN’s G. (c) A Residual Block (\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down) in</br>BigGAN’s D.</br></br>17</br></br>ResBlockResBlockResBlockNon-localImageLinear→ 4x4x16chSplitzClassConcatConcatConcatAdd3x3 ConvBatchNormBatchNormConcatLinearLinearUpsampleUpsample1x1 Conv3x3 ConvReLUReLUAdd3x3 ConvAverage Pooling1x1 ConvReLU3x3 ConvReLUAverage Pooling\f",
       "Published as a conference paper at ICLR 2019</br></br>(a)</br></br>18</br></br>(b)</br></br>(c)</br></br>Figure 16: (a) A typical architectural layout for \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BigGAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "-deep’s G; details are in the following</br>tables. (b) A Residual Block (\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up) in BigGAN-deep’s G. (c) A Residual Block (\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "</br>down) in BigGAN-deep’s D. A \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (without up or down) in BigGAN-deep does not include</br>the Upsample or Average Pooling layers, and has identity skip connections.</br></br>ResBlockResBlockResBlockNon-localImageLinear→ 4x4x16chzClassConcatAdd3x3 ConvBatchNormBatchNormConcatLinearLinearDrop channelsUpsample1x1 \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ConvReLUReLUBatchNormUpsample3x3\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " ConvReLUBatchNormLinear1x1 ConvReLULinearAdd3x3 ConvAveragePooling1x1 ConvReLUReLU3x3 ConvReLU1x1 ConvReLUAverage PoolingConcat1x1 Conv\f",
       "Published as a conference paper at ICLR 2019</br></br>Table 4: BigGAN architecture for 128 × 128 images. ch represents the channel width multiplier in</br>each network from Table 1.</br></br>z ∈ R120 ∼ N (0, I)</br>Embed(y) ∈ R128</br></br>Linear (20 + 128) → 4 × 4 × 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 8ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 4ch → 2ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 2ch → ch</br></br>BN, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 3 × 3 Conv ch → 3</br></br>Tanh</br></br>(a) Generator</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RGB\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " image x ∈ R128×128×3</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down ch → 2ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 2ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 4ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 8ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", Global sum pooling</br></br>Embed(y)·h + (linear → 1)</br></br>(b) Discriminator</br></br>Table 5: BigGAN architecture for 256 × 256 images. Relative to the 128 × 128 architecture, we</br>add an additional \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " in each network at 16×16 resolution, and move the non-local block in</br>G to 128 × 128 resolution. \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Memory constraints\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " prevent us from moving the non-local block in D.</br></br>z ∈ R140 ∼ N (0, I)</br>Embed(y) ∈ R128</br></br>Linear (20 + 128) → 4 × 4 × 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 8ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 4ch → 2ch</br></br>Non-Local Block (128 × 128)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 2ch → ch</br></br>BN, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 3 × 3 Conv ch → 3</br></br>Tanh</br></br>(a) Generator</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RGB\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " image x ∈ R256×256×3</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 2ch → 4ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 4ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 8ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", Global sum pooling</br></br>Embed(y)·h + (linear → 1)</br></br>(b) Discriminator</br></br>19</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Table 6: BigGAN architecture for 512 × 512 images. Relative to the 256 × 256 architecture, we</br>add an additional \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " at the 512 × 512 resolution. \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Memory constraints\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " force us to move the</br>non-local block in both networks back to 64 × 64 resolution as in the 128 × 128 pixel setting.</br></br>z ∈ R160 ∼ N (0, I)</br>Embed(y) ∈ R128</br></br>Linear (20 + 128) → 4 × 4 × 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 8ch → 4ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 4ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 2ch → ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up ch → ch</br></br>BN, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 3 × 3 Conv ch → 3</br></br>Tanh</br></br>(a) Generator</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RGB\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " image x ∈ R512×512×3</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down ch → ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 2ch → 4ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 4ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 8ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", Global sum pooling</br></br>Embed(y)·h + (linear → 1)</br></br>(b) Discriminator</br></br>Table 7: BigGAN-deep architecture for 128 × 128 images.</br></br>z ∈ R128 ∼ N (0, I)</br>Embed(y) ∈ R128</br></br>Linear (128 + 128) → 4 × 4 × 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 8ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 4ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 4ch → 2ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 2ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 2ch → ch</br></br>BN, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 3 × 3 Conv ch → 3</br></br>Tanh</br></br>(a) Generator</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RGB\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " image x ∈ R128×128×3</br></br>3 × 3 Conv 3 → ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 2ch → 2ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 2ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 4ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 4ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 8ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", Global sum pooling</br></br>Embed(y)·h + (linear → 1)</br></br>(b) Discriminator</br></br>20</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Table 8: BigGAN-deep architecture for 256 × 256 images.</br></br>z ∈ R128 ∼ N (0, I)</br>Embed(y) ∈ R128</br></br>Linear (128 + 128) → 4 × 4 × 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 8ch → 4ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 4ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 4ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 2ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 2ch → ch</br></br>BN, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 3 × 3 Conv ch → 3</br></br>Tanh</br></br>(a) Generator</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RGB\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " image x ∈ R256×256×3</br></br>3 × 3 Conv 3 → ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 2ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 2ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 4ch → 4ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 4ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 8ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", Global sum pooling</br></br>Embed(y)·h + (linear → 1)</br></br>(b) Discriminator</br></br>21</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Table 9: BigGAN-deep architecture for 512 × 512 images.</br></br>z ∈ R128 ∼ N (0, I)</br>Embed(y) ∈ R128</br></br>Linear (128 + 128) → 4 × 4 × 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 16ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 8ch → 4ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 4ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 4ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 2ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up 2ch → ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " ch → ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " up ch → ch</br></br>BN, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 3 × 3 Conv ch → 3</br></br>Tanh</br></br>(a) Generator</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RGB\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " image x ∈ R512×512×3</br></br>3 × 3 Conv 3 → ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down ch → ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " ch → ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 2ch → 2ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 2ch → 4ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 4ch → 4ch</br></br>Non-Local Block (64 × 64)</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 4ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 8ch → 8ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 8ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " down 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ResBlock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " 16ch → 16ch</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ReLU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", Global sum pooling</br></br>Embed(y)·h + (linear → 1)</br></br>(b) Discriminator</br></br>22</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>APPENDIX C EXPERIMENTAL DETAILS</br></br>Our basic setup follows \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SA-GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (Zhang et al., 2018), and is implemented in \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    TensorFlow\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (Abadi</br>et al., 2016). We employ the architectures detailed in Appendix B, with non-local blocks inserted at</br>a single stage in each network. Both G and D networks are initialized with Orthogonal Initialization</br>(Saxe et al., 2014). We use Adam optimizer (Kingma &amp; Ba, 2014) with β1 = 0 and β2 = 0.999 and</br>a constant learning rate. For BigGAN models at all resolutions, we use 2 · 10−4 in D and 5 · 10−5</br>in G. For BigGAN-deep, we use the learning rate of 2 · 10−4 in D and 5 · 10−5 in G for 128 × 128</br>models, and 2.5 · 10−5 in both D and G for 256 × 256 and 512 × 512 models. We experimented with</br>the number of D steps per G step (varying it from 1 to 6) and found that two D steps per G step gave</br>the best results.</br></br>We use an exponential moving average of the weights of G at sampling time, with a decay rate set to</br>0.9999. We employ cross-replica \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " (Ioffe &amp; Szegedy, 2015) in G, where batch statistics are</br>aggregated across all devices, rather than a single device as in standard implementations. Spectral</br>Normalization (Miyato et al., 2018) is used in both G and D, following SA-GAN (Zhang et al., 2018).</br>We train on a Google \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    TPU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " v3 Pod, with the number of cores proportional to the resolution: 128 for</br>128×128, 256 for 256×256, and 512 for 512×512. Training takes between 24 and 48 hours for</br>most models. We increase (cid:15) from the default 10−8 to 10−4 in \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " and Spectral Norm to</br>mollify low-precision numerical issues. We preprocess data by cropping along the long edge and</br>rescaling to a given resolution with area resampling.</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    C.1\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " BATCHNORM STATISTICS AND SAMPLING</br></br>The default behavior with batch normalized classiﬁer networks is to use a running average of the</br>activation moments at test time. Previous works (Radford et al., 2016) have instead used batch</br>statistics when sampling images. While this is not technically an invalid way to sample, it means</br>that results are dependent on the test batch size (and how many devices it is split across), and further</br>complicates reproducibility.</br></br>We ﬁnd that this detail is extremely important, with changes in test batch size producing drastic</br>changes in performance. This is further exacerbated when one uses exponential moving averages</br>of G’s weights for sampling, as the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " running averages are computed with non-averaged</br>weights and are poor estimates of the activation statistics for the averaged weights.</br></br>To counteract both these issues, we employ “standing statistics,” where we compute activation statis-</br>tics at sampling time by running the G through multiple forward passes (typically 100) each with</br>different batches of random noise, and storing means and variances aggregated across all forward</br>passes. Analogous to using running statistics, this results in G’s outputs becoming invariant to batch</br>size and the number of devices, even when producing a single sample.</br></br>C.2 \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CIFAR-10\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       "</br></br>We run our networks on \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CIFAR-10\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " (Krizhevsky &amp; Hinton, 2009) using the settings from Table 1,</br>row 8, and achieve an IS of 9.22 and an FID of 14.73 without truncation.</br></br>C.3</br></br>INCEPTION SCORES OF IMAGENET IMAGES</br></br>We compute the IS for both the training and validation sets of \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ImageNet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ". At 128×128 the training</br>data has an IS of 233, and the validation data has an IS of 166. At 256×256 the training data has an</br>IS of 377, and the validation data has an IS of 234. At 512×512 the training data has an IS of 348,</br>and the validation data has an IS of 241. The discrepancy between training and validation scores is</br>due to the Inception classiﬁer having been trained on the training data, resulting in high-conﬁdence</br>outputs that are preferred by the Inception Score.</br></br>23</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>APPENDIX D ADDITIONAL PLOTS</br></br>Figure 17: IS vs. FID at 128×128. Scores are averaged across three random seeds.</br></br>Figure 18: IS vs. FID at 256 and 512 pixels. Scores are averaged across three random seeds for 256.</br></br>24</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>Figure 19: JFT-300M IS vs. FID at 256×256. We show truncation values from σ = 0 to σ = 2</br>(top) and from σ = 0.5 to σ = 1.5 (bottom). Each curve corresponds to a row in Table 3. The</br>curve labeled with baseline corresponds to the ﬁrst row (with orthogonal regularization and other</br>techniques disabled), while the rest correspond to rows 2-4 – the same architecture at different</br>capacities (Ch).</br></br>25</br></br>5101520253035404550JFT-300M Inception Score020406080100120140160180JFT-300M FIDFID vs IS as a function of truncationCh=128Ch=96Ch=64Ch=64 (Baseline)1520253035404550JFT-300M Inception Score1020304050607080JFT-300M FIDFID vs IS as a function of truncationCh=128Ch=96Ch=64Ch=64 (Baseline)\f",
       "Published as a conference paper at ICLR 2019</br></br>APPENDIX E CHOOSING LATENT SPACES</br></br>While most previous work has employed N (0, I) or U[−1, 1] as the prior for z (the noise input to</br>G), we are free to choose any latent distribution from which we can sample. We explore the choice of</br>latents by considering an array of possible designs, described below. For each latent, we provide the</br>intuition behind its design and brieﬂy describe how it performs when used as a drop-in replacement</br>for z ∼ N (0, I) in an SA-GAN baseline. As the \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Truncation Trick\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       " proved more beneﬁcial than</br>switching to any of these latents, we do not perform a full ablation study, and employ z ∼ N (0, I)</br>for our main results to take full advantage of truncation. The two latents which we ﬁnd to work</br>best without truncation are Bernoulli {0, 1} and Censored Normal max (N (0, I), 0), both of which</br>improve speed of training and lightly improve ﬁnal performance, but are less amenable to truncation.</br>We also ablate the choice of latent space dimensonality (which by default is z ∈ R128), ﬁnding that</br>we are able to successfully train with latent dimensions as low as z ∈ R8, and that with z ∈ R32 we</br>see a minimal drop in performance. While this is substantially smaller than many previous works,</br>direct comparison to single-class networks (such as those in Karras et al. (2018), which employ</br>a z ∈ R512 latent space on a highly constrained dataset with 30,000 images) is improper, as our</br>networks have additional class information provided as input.</br></br>\n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    LATENTS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "</br></br>• N (0, I). A standard choice of the latent space which we use in the main experiments.</br>• U[−1, 1]. Another standard choice; we ﬁnd that it performs similarly to N (0, I).</br>• Bernoulli {0, 1}. A discrete latent might reﬂect our prior that underlying factors of \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    variation\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "in natural images are not continuous, but discrete (one feature is present, another is not).</br>This latent outperforms N (0, I) (in terms of IS) by 8% and requires 60% fewer iterations.</br>• max (N (0, I), 0), also called Censored Normal. This latent is designed to introduce spar-</br>sity in the latent space (reﬂecting our prior that certain latent features are sometimes present</br>and sometimes not), but also allow those latents to vary continuously, expressing different</br>degrees of intensity for latents which are active. This latent outperforms N (0, I) (in terms</br>of IS) by 15-20% and tends to require fewer iterations.</br></br>• Bernoulli {−1, 1}. This latent is designed to be discrete, but not sparse (as the network</br>can learn to activate in response to negative inputs). This latent performs near-identically</br>to N (0, I).</br></br>• Independent Categorical in {−1, 0, 1}, with equal probability. This distribution is chosen to</br>be discrete and have sparsity, but also to allow latents to take on both positive and negative</br>values. This latent performs near-identically to N (0, I).</br></br>• N (0, I) multiplied by Bernoulli {0, 1}. This distribution is chosen to have continuous</br>latent factors which are also sparse (with a peak at zero), similar to Censored Normal but</br>not constrained to be positive. This latent performs near-identically to N (0, I).</br></br>• Concatenating N (0, I) and Bernoulli {0, 1}, each taking half of the latent dimensions.</br>This is inspired by Chen et al. (2016), and is chosen to allow some factors of variation to</br>be discrete, while others are continuous. This latent outperforms N (0, I) by around 5%.</br>• Variance annealing: we sample from N (0, σI), where σ is allowed to vary over training.</br>We compared a variety of piecewise schedules and found that starting with σ = 2 and</br>annealing towards σ = 1 over the course of training mildly improved performance. The</br>space of possible variance schedules is large, and we did not explore it in depth – we suspect</br>that a more principled or better-tuned schedule could more strongly impact performance.</br>• Per-sample variable variance: N (0, σiI), where σi ∼ U[σl, σh] independently for each</br>sample i in a batch, and (σl, σh) are hyperparameters. This distribution was chosen to try</br>and improve amenability to the Truncation Trick by feeding the network noise samples with</br>non-constant variance. This did not appear to affect performance, but we did not explore it</br>in depth. One might also consider scheduling (σl, σh), similar to variance annealing.</br></br>26</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>APPENDIX F MONITORED TRAINING STATISTICS</br></br>(a) G σ0</br></br>(b) G σ0</br>σ1</br></br>(c) G σ1</br></br>(d) G σ2</br></br>(e) D σ0</br></br>(f) D σ0</br>σ1</br></br>(g) D σ1</br></br>(h) D σ2</br></br>Figure 20: Training statistics for a typical model without special modiﬁcations. Collapse occurs</br>after 200000 iterations.</br></br>27</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>(a) σ0</br></br>(b) σ0</br>σ1</br></br>(c) σ1</br></br>(d) σ2</br></br>Figure 21: G training statistics with σ0 in G regularized towards 1. Collapse occurs after 125000</br>iterations.</br></br>(a) σ0</br></br>(b) σ0</br>σ1</br></br>(c) σ1</br></br>(d) σ2</br></br>Figure 22: D training statistics with σ0 in G regularized towards 1. Collapse occurs after 125000</br>iterations.</br></br>28</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>(a) σ0</br></br>(b) σ0</br>σ1</br></br>(c) σ1</br></br>(d) σ2</br></br>Figure 23: G training statistics with an R1 Gradient Penalty of strength 10 on D. This model does</br>not collapse, but only reaches a maximum IS of 55.</br></br>(a) σ0</br></br>(b) σ0</br>σ1</br></br>(c) σ1</br></br>(d) σ2</br></br>Figure 24: D training statistics with an R1 Gradient Penalty of strength 10 on D. This model does</br>not collapse, but only reaches a maximum IS of 55.</br></br>29</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>(a) σ0</br></br>(b) σ0</br>σ1</br></br>(c) σ1</br></br>(d) σ2</br></br>Figure 25: G training statistics with Dropout (keep probability 0.8) applied to the last feature layer</br>of D. This model does not collapse, but only reaches a maximum IS of 70.</br></br>(a) σ0</br></br>(b) σ0</br>σ1</br></br>(c) σ1</br></br>(d) σ2</br></br>Figure 26: D training statistics with Dropout (keep probability 0.8) applied to the last feature layer</br>of D. This model does not collapse, but only reaches a maximum IS of 70.</br></br>30</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>(a) G (cid:107)W (cid:107)2</br></br>(b) D (cid:107)W (cid:107)2</br></br>(c) losses</br></br>(d) Variance of all gradient norms in G and D</br></br>Figure 27: Additional training statistics for a typical model without special modiﬁcations. Collapse</br>occurs after 200000 iterations.</br></br>(a) G (cid:107)W (cid:107)2</br></br>(b) D (cid:107)W (cid:107)2</br></br>(c) losses</br></br>(d) Variance of all gradient norms in G and D</br></br>Figure 28: Additional training statistics with an R1 Gradient Penalty of strength 10 on D. This model</br>does not collapse, but only reaches a maximum IS of 55.</br></br>31</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>APPENDIX G ADDITIONAL DISCUSSION: STABILITY AND \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    COLLAPSE\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       "</br></br>In this section, we present and discuss additional investigations into the stability of our models,</br>expanding upon the discussion in Section 4.</br></br>G.1</br></br>INTERVENING BEFORE COLLAPSE</br></br>The symptoms of collapse are sharp and sudden, with sample quality dropping from its peak to</br>its lowest value over the course of a few hundred iterations. We can detect this collapse when the</br>singular values in G explode, but while the (unnormalized) singular values grow throughout training,</br>there is no consistent threshold at which collapse occurs. This raises the question of whether it is</br>possible to prevent or delay collapse by taking a model checkpoint several thousand iterations before</br>collapse, and continuing training with some hyperparameters modiﬁed (e.g., the learning rate).</br></br>We conducted a range of intervention experiments wherein we took checkpoints of a collapsed</br>model ten or twenty thousand iterations before collapse, changed some aspect of the training setup,</br>then observed whether collapse occurred, when it occurred relative to the original collapse, and the</br>ﬁnal performance attained at collapse.</br></br>We found that increasing the learning rates (relative to their initial values) in either G or D, or both G</br>and D, led to immediate collapse. This occurred even when doubling the learning rates from 2 · 10−4</br>in D and 5 · 10−5 in G, to 4 · 10−4 in D and 1 · 10−4 in G, a setting which is not normally unstable</br>when used as the initial learning rates. We also tried changing the momentum terms (Adam’s β1</br>and β2), or resetting the momentum vectors to zero, but this tended to either make no difference or,</br>when increasing the momentum, cause immediate collapse.</br></br>We found that decreasing the learning rate in G, but keeping the learning rate in D unchanged could</br>delay collapse (in some cases by over one hundred thousand iterations), but also crippled training—</br>once the learning rate in G was decayed, performance either stayed constant or slowly decayed.</br>Conversely, reducing the learning rate in D while keeping G’s learning rate led to immediate collapse.</br>We hypothesize that this is because of the need for D to remain optimal throughout training—if its</br>learning rate is reduced, it can no longer “keep up” with G, and training collapses. With this in mind,</br>we also tried increasing the number of D steps per G step, but this either had no effect, or delayed</br>collapse at the cost of crippling training (similar to decaying G’s learning rate).</br></br>To further illuminate these dynamics, we construct two additional intervention experiments, one</br>where we freeze G before collapse (by ceasing all parameter updates) and observe whether D remains</br>stable, and the reverse, where we freeze D before collapse and observe whether G remains stable.</br>We ﬁnd that when G is frozen, D remains stable, and slowly reduces both components of its loss</br>towards zero. However, when D is frozen, G immediately and dramatically collapses, maxing out</br>D’s loss to values upwards of 300, compared to the normal range of 0 to 3.</br></br>This leads to two conclusions: ﬁrst, as has been noted in previous works (Miyato et al., 2018;</br>Gulrajani et al., 2017; Zhang et al., 2018), D must remain optimal with respect to G both for stability</br>and to provide useful gradient information. The consequence of G being allowed to win the game is a</br>complete breakdown of the training process, regardless of G’s conditioning or optimization settings.</br>Second, favoring D over G (either by training it with a larger learning rate, or for more steps) is</br>insufﬁcient to ensure stability even if D is well-conditioned. This suggests either that in practice, an</br>optimal D is necessary but insufﬁcient for training stability, or that some aspect of the system results</br>in D not being trained towards optimality. With the latter possibility in mind, we take a closer look</br>at the noise in D’s spectra in the following section.</br></br>32</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>G.2 SPIKES IN THE DISCRIMINATOR’S SPECTRA</br></br>(a) D σ0</br></br>(b) D σ0</br>σ1</br></br>Figure 29: A closeup of D’s spectra at a noise spike.</br></br>If some element of D’s training process results in undesirable dynamics, it follows that the behavior</br>of D’s spectra may hold clues as to what that element is. The top three singular values of D differ</br>from G’s in that they have a large noise component, tend to grow throughout training but only show</br>a small response to collapse, and the ratio of the ﬁrst two singular values tends to be centered around</br>one, suggesting that the spectra of D have a slow decay. When viewed up close (Figure 29), the</br>noise spikes resemble an impulse response: at each spike, the spectra jump upwards, then slowly</br>decrease, with some oscillation.</br></br>One possible explanation is that this behavior is a consequence of D memorizing the training data,</br>as suggested by experiments in Section 4.2. As it approaches perfect memorization, it receives</br>less and less signal from real data, as both the original \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " loss and the hinge loss provide zero</br>gradients when D outputs a conﬁdent and \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    correct prediction\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       " for a given example. If the gradient</br>signal from real data attenuates to zero, this can result in D eventually becoming biased due to</br>exclusively received gradients that encourage its outputs to be negative. If this bias passes a certain</br>threshold, D will eventually misclassify a large number of real examples and receive a large gradient</br>encouraging positive outputs, resulting in the observed impulse responses.</br></br>This argument suggests several ﬁxes. First, one might consider an unbounded loss (such as the</br>Wasserstein loss (Arjovsky et al., 2017)) which would not suffer this gradient attentuation. We found</br>that even with gradient penalties and brief re-tuning of optimizer hyperparameters, our models did</br>not stably train for more than a few thousand iterations with this loss. We instead explored \n",
       "<mark class=\"entity\" style=\"background: #a29bfe; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    changing\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TASK</span>\n",
       "</mark>\n",
       "</br>the margin of the hinge loss as a partial compromise: for a given model and minibatch of data,</br>increasing the margin will result in more examples falling within the margin, and thus contributing</br>to the loss.3. Training with a smaller margin (by a factor of 2) measurably reduces performance,</br>but training with a larger margin (by up to a factor of 3) does not prevent collapse or reduce the</br>noise in D’s spectra. Increasing the margin beyond 3 results in unstable training similar to using</br>the Wasserstein loss. Finally, the memorization argument might suggest that using a smaller D or</br>using dropout in D would improve training by reducing its capacity to memorize, but in practice this</br>degrades training.</br></br>3Unconstrained models could easily learn a different output scale to account for this margin, but the use of</br></br>Spectral Normalization constrains our models and makes the speciﬁc selection of the margin meaningful.</br></br>33</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>APPENDIX H NEGATIVE RESULTS</br></br>We explored a range of novel and existing techniques which ended up degrading or otherwise not</br>affecting performance in our setting. We report them here; our evaluations for this section are not as</br>thorough as those for the main architectural choices.</br></br>Our intention in reporting these results is to save time for future work, and to give a more complete</br>picture of our attempts to improve performance or stability. We note, however, that these results</br>must be understood to be speciﬁc to the particular setup we used. A pitfall of reporting negative</br>results is that one might report that a particular technique doesn’t work, when the reality is that this</br>technique did not have the desired effect when applied in a particular way to a particular problem.</br>Drawing overly general conclusions might close off potentially fruitful avenues of research.</br></br>• We found that doubling the depth (by inserting an additional Residual block after every up-</br></br>or down-sampling block) hampered performance.</br></br>• We experimented with sharing class embeddings between both G and D (as opposed to just</br>within G). This is accomplished by replacing D’s class embedding with a projection from</br>G’s embeddings, as is done in G’s BatchNorm layers. In our initial experiments this seemed</br>to help and accelerate training, but we found this trick scaled poorly and was sensitive to</br>optimization hyperparameters, particularly the choice of number of D steps per G step.</br>• We tried replacing BatchNorm in G with WeightNorm (Salimans &amp; Kingma, 2016), but</br>this crippled training. We also tried removing \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " and only having Spectral Nor-</br>malization, but this also crippled training.</br></br>• We tried adding \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " to D (both class-conditional and unconditional) in addition to</br></br>Spectral Normalization, but this crippled training.</br></br>• We tried varying the choice of location of the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    attention\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " block in G and D (and inserting</br>multiple \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    attention\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " blocks at different resolutions) but found that at 128×128 there was no</br>noticeable beneﬁt to doing so, and compute and memory costs increased substantially. We</br>found a beneﬁt to moving the \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    attention\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " block up one stage when moving to 256×256,</br>which is in line with our expectations given the increased resolution.</br></br>• We tried using ﬁlter sizes of 5 or 7 instead of 3 in either G or D or both. We found that</br>having a ﬁlter size of 5 in G only provided a small improvement over the baseline but came</br>at an unjustiﬁable compute cost. All other settings degraded performance.</br></br>• We tried varying the dilation for \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    convolutional\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " ﬁlters in both G and D at 128×128, but found</br></br>that even a small amount of dilation in either network degraded performance.</br></br>• We tried bilinear upsampling in G in place of nearest-neighbors upsampling, but this de-</br></br>graded performance.</br></br>• In some of our models, we observed class-conditional mode collapse, where the model</br>would only output one or two samples for a subset of classes but was still able to generate</br>samples for all other classes. We noticed that the collapsed classes had embedings which</br>had become very large relative to the other embeddings, and attempted to ameliorate this</br>issue by applying weight decay to the shared embedding only. We found that small amounts</br>of weight decay (10−6) instead degraded performance, and that only even smaller values</br>(10−8) did not degrade performance, but these values were also too small to prevent the</br>class vectors from exploding. Higher-resolution models appear to be more resilient to this</br>problem, and none of our ﬁnal models appear to suffer from this type of collapse.</br></br>• We experimented with using MLPs instead of linear projections from G’s class embeddings</br>to its \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BatchNorm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " gains and biases, but did not ﬁnd any beneﬁt to doing so. We also exper-</br>imented with Spectrally Normalizing these MLPs, and with providing these (and the linear</br>projections) with a bias at their output, but did not notice any beneﬁt.</br></br>• We tried gradient norm clipping (both the global variant typically used in \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    recurrent net-\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       "</br>works, and a local version where the clipping value is determined on a per-parameter basis)</br>but found this did not alleviate instability.</br></br>34</br></br>\f",
       "Published as a conference paper at ICLR 2019</br></br>APPENDIX I HYPERPARAMETERS</br></br>We performed various hyperparameter sweeps in this work:</br></br>• We swept the Cartesian product of the learning rates for each network through [10−5,</br>5 · 10−5, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    10−4\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 2 · 10−4, 4 · 10−4, 8 · 10−4, 10−3], and initially found that the \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SA-GAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       "</br>settings (G’s learning rate 10−4, D’s learning rate 4 · 10−4) were optimal at lower batch</br>sizes; we did not repeat this sweep at higher batch sizes but did try halving and doubling</br>the learning rate, arriving at the halved settings used for our experiments.</br></br>• We swept the R1 gradient penalty strength through [10−3, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    10−2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 10−1, 0.5, 1, 2, 3, 5, 10].</br>We ﬁnd that the strength of the penalty correlates negatively with performance, but that</br>settings above 0.5 impart training stability.</br></br>• We swept the keep probabilities for \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    DropOut\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " in the ﬁnal layer of D through [0.5, 0.6, 0.7,</br>0.8, 0.9, 0.95]. We ﬁnd that \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    DropOut\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       " has a similar stabilizing effect to R1 but also degrades</br>performance.</br></br>• We swept D’s Adam β1 parameter through [0.1, 0.2, 0.3, 0.4, 0.5] and found it to have</br>a light regularization effect similar to \n",
       "<mark class=\"entity\" style=\"background: #fdcb6e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    DropOut\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MODEL</span>\n",
       "</mark>\n",
       ", but not to signiﬁcantly improve results.</br>Higher β1 terms in either network crippled training.</br></br>• We swept the strength of the modiﬁed Orthogonal Regularization penalty in G through</br></br>[10−5, 5 · 10−5, \n",
       "<mark class=\"entity\" style=\"background: #fab1a0; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    10−4\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATASET</span>\n",
       "</mark>\n",
       ", 5 · 10−4, 10−3, 10−2], and selected 10−4.</br></br>35</br></br>\f",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "import json \n",
    "print(len(doc.ents))\n",
    "\n",
    "doc = nlp(text)\n",
    "options = {\"ents\": ['MODEL', 'DATASET', 'TASK'], \"colors\": { 'MODEL': '#fdcb6e', 'DATASET': '#fab1a0', 'TASK': '#a29bfe'}}\n",
    "#displacy.render(doc.ents, jupyter=True, style='ent', options=options)\n",
    "displacy.render(doc, style=\"ent\", options=options)\n",
    "#print([(e.start, e.text, e.label_) for e in doc.ents])\n",
    "\n",
    "words = [(str(e.text).lower(), e.label_) for e in doc.ents if len(e.text) > 1]\n",
    "most_common_ents = Counter(words).most_common()\n",
    "\n",
    "entities = []\n",
    "for e in most_common_ents:\n",
    "    entities.append({\n",
    "        'value': e[0][0],\n",
    "        'type': e[0][1]\n",
    "    })\n",
    "\n",
    "with open('../../ui/public/attention.json', 'w') as fp:\n",
    "    json.dump(entities, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root 2 text i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "LARGE SCALE GAN TRAINING FOR\n",
      "HIGH FIDELITY NATURAL IMAGE SYNTHESIS\n",
      "\n",
      "Andrew Brock∗ †\n",
      "Heriot-Watt University\n",
      "ajb5@hw.ac.uk\n",
      "\n",
      "\n",
      "root 8 text Jeff Donahue†\n",
      "DeepMind\n",
      "\n",
      "root 2 text jeffdonahue@google.com\n",
      "\n",
      "\n",
      "root 5 text Karen Simonyan†\n",
      "DeepMind\n",
      "simonyan@google.com\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 8 text ABSTRACT\n",
      "\n",
      "\n",
      "root 7 text Despite recent progress in generative image modeling, successfully generating\n",
      "high-resolution, diverse samples from complex datasets such as ImageNet remains\n",
      "an elusive goal.\n",
      "root 5 text To this end, we train Generative Adversarial Networks at the\n",
      "largest scale yet attempted, and study the instabilities speciﬁc to such scale.\n",
      "root 4 text Our\n",
      "modiﬁcations lead to models which set the new state of the art in class-conditional\n",
      "image synthesis.\n",
      "root 7 text When trained on ImageNet at 128×128 resolution, our models\n",
      "(BigGANs) achieve an Inception Score (IS) of 166.5 and Fr´echet Inception Dis-\n",
      "tance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65.\n",
      "root 2 text \n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "\n",
      "root 6 text Figure 1: Class-conditional samples generated by our model.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 8 text The state of generative image modeling has advanced dramatically in recent years, with Generative\n",
      "Adversarial Networks (GANs, Goodfellow et al.\n",
      "root 4 text (2014))\n",
      "root 2 text at the forefront of efforts to generate high-\n",
      "ﬁdelity, diverse images with models learned directly from data.\n",
      "root 2 text GAN training is dynamic, and\n",
      "sensitive to nearly every aspect of its setup (from optimization parameters to model architecture),\n",
      "but a torrent of research has yielded empirical and theoretical insights enabling stable training in\n",
      "a variety of settings.\n",
      "root 8 text Despite this progress, the current state of the art in conditional ImageNet\n",
      "modeling (Zhang et al., 2018) achieves an Inception Score (Salimans et al., 2016) of 52.5, compared\n",
      "to 233 for real data.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 3 text In this work, we set out to close the gap in ﬁdelity and variety between images generated by GANs\n",
      "and real-world images from the ImageNet dataset.\n",
      "root 4 text We make the following three contributions to-\n",
      "root 5 text \n",
      "wards this goal:\n",
      "\n",
      "•\n",
      "root 11 text We demonstrate that GANs beneﬁt dramatically from scaling, and train models with two\n",
      "to four times as many parameters and eight times the batch size compared to prior art.\n",
      "root 9 text We\n",
      "introduce two simple, general architectural changes that improve scalability, and modify a\n",
      "regularization scheme to improve conditioning, demonstrably boosting performance.\n",
      "root 6 text \n",
      "\n",
      "∗Work done at DeepMind\n",
      "†Equal contribution\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "• As a side effect of our modiﬁcations, our models become amenable to the “truncation\n",
      "trick,” a simple sampling technique that allows explicit, ﬁne-grained control of the trade-\n",
      "off between sample variety and ﬁdelity.\n",
      "root 2 text \n",
      "\n",
      "•\n",
      "root 8 text We discover instabilities speciﬁc to large scale GANs, and characterize them empirically.\n",
      "root 11 text \n",
      "Leveraging insights from this analysis, we demonstrate that a combination of novel and\n",
      "existing techniques can reduce these instabilities, but complete training stability can only\n",
      "be achieved at a dramatic cost to performance.\n",
      "root 2 text \n",
      "\n",
      "Our modiﬁcations substantially improve class-conditional GANs.\n",
      "root 7 text When trained on ImageNet at\n",
      "128×128 resolution, our models (BigGANs) improve the state-of-the-art Inception Score (IS) and\n",
      "Fr´echet Inception Distance (FID) from 52.52 and 18.65 to 166.5 and 7.4 respectively.\n",
      "root 5 text We also\n",
      "successfully train BigGANs on ImageNet at 256×256 and 512×512 resolution, and achieve IS and\n",
      "FID of 232.5 and 8.1 at 256×256 and IS and FID of 241.5 and 11.5 at 512×512.\n",
      "root 5 text Finally, we train\n",
      "our models on an even larger dataset – JFT-300M – and demonstrate that our design choices transfer\n",
      "well from ImageNet.\n",
      "root 3 text Code and weights for our pretrained generators are publicly available 1.\n",
      "root 2 text \n",
      "\n",
      "2 BACKGROUND\n",
      "\n",
      "A Generative Adversarial Network (GAN) involves Generator (G) and Discriminator (D) networks\n",
      "whose purpose, respectively, is to map random noise to samples and discriminate real and generated\n",
      "samples.\n",
      "root 2 text Formally, the GAN objective, in its original form (Goodfellow et al., 2014) involves ﬁnding\n",
      "a Nash equilibrium to the following two player min-max problem:\n",
      "\n",
      "min\n",
      "G\n",
      "\n",
      "max\n",
      "D\n",
      "\n",
      "Ex∼qdata(x)[log D(x)]\n",
      "root 2 text + Ez∼p(z)[log(1 − D(G(z)))],\n",
      "\n",
      "(1)\n",
      "\n",
      "\n",
      "root 2 text Rdz is a latent variable drawn from distribution p(z) such as N (0, I) or U[−1, 1].\n",
      "root 3 text \n",
      "When applied to images, G and D are usually convolutional neural networks (Radford et al., 2016).\n",
      "root 2 text Without auxiliary stabilization techniques, this training procedure is notoriously brittle, requiring\n",
      "ﬁnely-tuned hyperparameters and architectural choices to work at all.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 7 text Much recent research has accordingly focused on modiﬁcations to the vanilla GAN procedure to\n",
      "impart stability, drawing on a growing body of empirical and theoretical insights (Nowozin et al.,\n",
      "2016;\n",
      "root 8 text Sønderby et al., 2017; Fedus et al., 2018).\n",
      "root 9 text One line of work is focused on changing the\n",
      "objective function (Arjovsky et al., 2017; Mao et al., 2016; Lim & Ye, 2017; Bellemare et al.,\n",
      "2017; Salimans et al., 2018) to encourage convergence.\n",
      "root 2 text Another line is focused on constraining\n",
      "D through gradient penalties (Gulrajani et al., 2017; Kodali et al., 2017; Mescheder et al., 2018)\n",
      "or normalization (Miyato et al., 2018), both to counteract the use of unbounded loss functions and\n",
      "ensure D provides gradients everywhere to G.\n",
      "\n",
      "Of particular relevance to our work is Spectral Normalization (Miyato et al., 2018), which enforces\n",
      "Lipschitz continuity on D by normalizing its parameters with running estimates of their ﬁrst singular\n",
      "values, inducing backwards dynamics that adaptively regularize the top singular direction.\n",
      "root 2 text Relatedly\n",
      "Odena et al.\n",
      "root 7 text (2018) analyze the condition number of the Jacobian of G and ﬁnd that performance is\n",
      "dependent on G’s conditioning.\n",
      "root 2 text Zhang et al.\n",
      "root 4 text (2018)\n",
      "root 8 text ﬁnd that employing Spectral Normalization in\n",
      "G improves stability, allowing for fewer D steps per iteration.\n",
      "root 6 text We extend on these analyses to gain\n",
      "further insight into the pathology of GAN training.\n",
      "root 5 text \n",
      "\n",
      "Other works focus on the choice of architecture, such as SA-GAN (Zhang et al., 2018) which adds\n",
      "the self-attention block from (Wang et al., 2018) to improve the ability of both G and D to model\n",
      "global structure.\n",
      "root 6 text ProGAN (Karras et al., 2018) trains high-resolution GANs in the single-class\n",
      "setting by training a single model across a sequence of increasing resolutions.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 3 text In conditional GANs (Mirza & Osindero, 2014) class information can be fed into the model in\n",
      "\n",
      "root 8 text In (Odena et al., 2017) it is provided to G by concatenating a 1-hot class vector\n",
      "various ways.\n",
      "root 2 text de Vries et al.\n",
      "root 4 text (2017)\n",
      "root 2 text and\n",
      "\n",
      "1https://tfhub.dev/s?q=biggan\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Batch Ch.\n",
      "root 2 text 64\n",
      "256\n",
      "64\n",
      "512\n",
      "64\n",
      "1024\n",
      "64\n",
      "2048\n",
      "96\n",
      "2048\n",
      "96\n",
      "2048\n",
      "96\n",
      "2048\n",
      "96\n",
      "2048\n",
      "64\n",
      "2048\n",
      "\n",
      "\n",
      "root 5 text Param (M)\n",
      "81.5\n",
      "81.5\n",
      "81.5\n",
      "81.5\n",
      "173.5\n",
      "160.6\n",
      "158.3\n",
      "158.3\n",
      "71.3\n",
      "\n",
      "\n",
      "root 6 text Shared\n",
      "\n",
      "Skip-z Ortho.\n",
      "root 2 text \n",
      "\n",
      "SA-GAN Baseline\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "\n",
      "root 2 text Itr ×103\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "732\n",
      "295(±18)\n",
      "185(±11)\n",
      "152(±7)\n",
      "165(±13)\n",
      "371(±7)\n",
      "\n",
      "FID\n",
      "18.65\n",
      "15.30\n",
      "14.88\n",
      "12.39\n",
      "9.54(±0.62)\n",
      "root 2 text \n",
      "8.51(±0.32)\n",
      "10.48(±0.10)\n",
      "\n",
      "IS\n",
      "52.52\n",
      "58.77(±1.18)\n",
      "63.03(±1.42)\n",
      "76.85(±3.83)\n",
      "92.98(±4.27)\n",
      "94.94(±1.32)\n",
      "98.76(±2.84)\n",
      "99.31(±2.10)\n",
      "86.90(±0.61)\n",
      "\n",
      "\n",
      "root 2 text Table 1: Fr´echet Inception Distance (FID, lower is better) and Inception Score (IS, higher is better)\n",
      "for ablations of our proposed modiﬁcations.\n",
      "root 2 text Batch is batch size, Param is total number of param-\n",
      "eters, Ch.\n",
      "root 5 text is the channel multiplier representing the number of units in each layer, Shared is using\n",
      "shared embeddings, Skip-z is using skip connections from the latent to multiple layers, Ortho.\n",
      "root 2 text is\n",
      "Orthogonal Regularization, and Itr indicates if the setting is stable to 106 iterations, or it collapses\n",
      "at the given iteration.\n",
      "root 8 text Other than rows 1-4, results are computed across 8 random initializations.\n",
      "root 2 text \n",
      "\n",
      "Dumoulin et al.\n",
      "root 6 text (2017) modify the way class conditioning is passed to G by supplying it with class-\n",
      "conditional gains and biases in BatchNorm (Ioffe & Szegedy, 2015) layers.\n",
      "root 11 text In Miyato & Koyama\n",
      "(2018), D is conditioned by using the cosine similarity between its features and a set of learned\n",
      "class embeddings as additional evidence for distinguishing real and generated samples, effectively\n",
      "encouraging generation of samples whose features match a learned class prototype.\n",
      "root 2 text \n",
      "\n",
      "Objectively evaluating implicit generative models is difﬁcult (Theis et al., 2015).\n",
      "root 8 text A variety of works\n",
      "have proposed heuristics for measuring the sample quality of models without tractable likelihoods\n",
      "(Salimans et al., 2016; Heusel et al., 2017; Bi´nkowski et al., 2018; Wu et al., 2017).\n",
      "root 2 text Of these,\n",
      "the Inception Score (IS, Salimans et al.\n",
      "root 4 text (2016))\n",
      "root 8 text and Fr´echet Inception Distance (FID, Heusel et al.\n",
      "root 6 text \n",
      "(2017)) have become popular despite their notable ﬂaws (Barratt & Sharma, 2018).\n",
      "root 6 text We employ\n",
      "them as approximate measures of sample quality, and to enable comparison against previous work.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 7 text 3 SCALING UP GANS\n",
      "\n",
      "\n",
      "root 7 text In this section, we explore methods for scaling up GAN training to reap the performance beneﬁts of\n",
      "larger models and larger batches.\n",
      "root 6 text As a baseline, we employ the SA-GAN architecture of Zhang et al.\n",
      "root 4 text \n",
      "(2018), which uses the hinge loss (Lim & Ye, 2017; Tran et al., 2017) GAN objective.\n",
      "root 7 text We provide\n",
      "class information to G with class-conditional BatchNorm (Dumoulin et al., 2017; de Vries et al.,\n",
      "2017) and to D with projection (Miyato & Koyama, 2018).\n",
      "root 6 text The optimization settings follow Zhang\n",
      "et al.\n",
      "root 9 text (2018) (notably employing Spectral Norm in G) with the modiﬁcation that we halve the learning\n",
      "rates and take two D steps per G step.\n",
      "root 6 text For evaluation, we employ moving averages of G’s weights\n",
      "following Karras et al.\n",
      "root 4 text (2018)\n",
      "root 9 text ; Mescheder et al.\n",
      "root 4 text (2018)\n",
      "root 4 text ; Yazc et al.\n",
      "root 4 text (2018), with a decay of 0.9999.\n",
      "root 3 text We use Orthogonal Initialization (Saxe et al., 2014), whereas previous works used N (0, 0.02I)\n",
      "\n",
      "root 7 text (Radford et al., 2016) or Xavier initialization (Glorot & Bengio, 2010).\n",
      "root 7 text Each model is trained on\n",
      "128 to 512 cores of a Google TPUv3 Pod (Google, 2018), and computes BatchNorm statistics in G\n",
      "across all devices, rather than per-device as is typical.\n",
      "root 7 text We ﬁnd progressive growing (Karras et al.,\n",
      "2018)\n",
      "root 11 text unnecessary even for our 512×512 models.\n",
      "root 5 text Additional details are in Appendix C.\n",
      "\n",
      "We begin by increasing the batch size for the baseline model, and immediately ﬁnd tremendous\n",
      "beneﬁts in doing so.\n",
      "root 2 text Rows 1-4 of Table 1 show that simply increasing the batch size by a factor of\n",
      "8 improves the state-of-the-art IS by 46%.\n",
      "root 10 text We conjecture that this is a result of each batch covering\n",
      "more modes, providing better gradients for both networks.\n",
      "root 2 text One notable side effect of this scaling is\n",
      "that our models reach better ﬁnal performance in fewer iterations, but become unstable and undergo\n",
      "complete training collapse.\n",
      "root 7 text We discuss the causes and ramiﬁcations of this in Section 4.\n",
      "root 6 text For these\n",
      "experiments, we report scores from checkpoints saved just before collapse.\n",
      "root 8 text \n",
      "\n",
      "We then increase the width (number of channels) in each layer by 50%, approximately doubling the\n",
      "number of parameters in both models.\n",
      "root 5 text This leads to a further IS improvement of 21%, which we\n",
      "posit is due to the increased capacity of the model relative to the complexity of the dataset.\n",
      "root 6 text Doubling\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2: (a) The effects of increasing truncation.\n",
      "root 3 text From left to right, the threshold is set to 2, 1, 0.5,\n",
      "0.04.\n",
      "root 9 text (b) Saturation artifacts from applying truncation to a poorly conditioned model.\n",
      "root 9 text \n",
      "\n",
      "the depth did not initially lead to improvement – we addressed this later in the BigGAN-deep model,\n",
      "which uses a different residual block structure.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 4 text We note that class embeddings c used for the conditional BatchNorm layers in G contain a large\n",
      "number of weights.\n",
      "root 3 text Instead of having a separate layer for each embedding (Miyato et al., 2018;\n",
      "Zhang et al., 2018), we opt to use a shared embedding, which is linearly projected to each layer’s\n",
      "gains and biases (Perez et al., 2018).\n",
      "root 7 text This reduces computation and memory costs, and improves\n",
      "training speed (in number of iterations required to reach a given performance) by 37%.\n",
      "root 3 text Next, we\n",
      "add direct skip connections (skip-z) from the noise vector z to multiple layers of G rather than just\n",
      "the initial layer.\n",
      "root 2 text The intuition behind this design is to allow G to use the latent space to directly in-\n",
      "root 8 text \n",
      "ﬂuence features at different resolutions and levels of hierarchy.\n",
      "root 12 text In BigGAN, this is accomplished by\n",
      "splitting z into one chunk per resolution, and concatenating each chunk to the conditional vector c\n",
      "which gets projected to the BatchNorm gains and biases.\n",
      "root 3 text In BigGAN-deep, we use an even simpler\n",
      "design, concatenating the entire z with the conditional vector without splitting it into chunks.\n",
      "root 2 text Pre-\n",
      "vious works (Goodfellow et al., 2014; Denton et al., 2015) have considered variants of this concept;\n",
      "our implementation is a minor modiﬁcation of this design.\n",
      "root 8 text Skip-z provides a modest performance\n",
      "improvement of around 4%, and improves training speed by a further 18%.\n",
      "root 2 text \n",
      "\n",
      "3.1 TRADING OFF VARIETY AND FIDELITY WITH THE TRUNCATION TRICK\n",
      "\n",
      "\n",
      "root 6 text Unlike models which need to backpropagate through their latents, GANs can employ an arbitrary\n",
      "prior p(z), yet the vast majority of previous works have chosen to draw z from either N (0, I) or\n",
      "U[−1, 1].\n",
      "root 8 text We question the optimality of this choice and explore alternatives in Appendix E.\n",
      "\n",
      "\n",
      "root 4 text Remarkably, our best results come from using a different latent distribution for sampling than was\n",
      "used in training.\n",
      "root 8 text Taking a model trained with z ∼ N (0, I) and sampling z from a truncated nor-\n",
      "mal (where values which fall outside a range are resampled to fall inside that range) immediately\n",
      "provides a boost to IS and FID.\n",
      "root 4 text We call this the Truncation Trick:\n",
      "root 5 text \n",
      "truncating a z vector by re-\n",
      "sampling the values with magnitude above a chosen threshold leads to improvement in individual\n",
      "sample quality at the cost of reduction in overall sample variety.\n",
      "root 12 text Figure 2(a) demonstrates this: as\n",
      "the threshold is reduced, and elements of z are truncated towards zero (the mode of the latent dis-\n",
      "tribution), individual samples approach the mode of G’s output distribution.\n",
      "root 4 text Related observations\n",
      "about this trade-off were made in (Marchesi, 2016;\n",
      "root 7 text Pieters & Wiering, 2014).\n",
      "root 6 text \n",
      "\n",
      "This technique allows ﬁne-grained, post-hoc selection of the trade-off between sample quality and\n",
      "variety for a given G.\n",
      "root 7 text Notably, we can compute FID and IS for a range of thresholds, obtaining the\n",
      "variety-ﬁdelity curve reminiscent of the precision-recall curve (Figure 17).\n",
      "root 5 text As IS does not penal-\n",
      "ize lack of variety in class-conditional models, reducing the truncation threshold leads to a direct\n",
      "increase in IS (analogous to precision).\n",
      "root 5 text FID penalizes lack of variety (analogous to recall) but also\n",
      "rewards precision, so we initially see a moderate improvement in FID, but as truncation approaches\n",
      "zero and variety diminishes, the FID sharply drops.\n",
      "root 2 text The distribution shift caused by sampling with\n",
      "different latents than those seen in training is problematic for many models.\n",
      "root 3 text Some of our larger\n",
      "models are not amenable to truncation, producing saturation artifacts (Figure 2(b)) when fed trun-\n",
      "cated noise.\n",
      "root 4 text To counteract this, we seek to enforce amenability to truncation by conditioning G to be\n",
      "smooth, so that the full space of z will map to good output samples.\n",
      "root 4 text For this, we turn to Orthogonal\n",
      "Regularization (Brock et al., 2017), which directly enforces the orthogonality condition:\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Rβ(W )\n",
      "root 2 text (cid:62)W − I(cid:107)2\n",
      "F,\n",
      "\n",
      "(2)\n",
      "\n",
      "where W is a weight matrix and β a hyperparameter.\n",
      "root 7 text This regularization is known to often be too\n",
      "limiting (Miyato et al., 2018), so we explore several variants designed to relax the constraint while\n",
      "still imparting the desired smoothness to our models.\n",
      "root 7 text The version we ﬁnd to work best removes the\n",
      "diagonal terms from the regularization, and aims to minimize the pairwise cosine similarity between\n",
      "ﬁlters but does not constrain their norm:\n",
      "\n",
      "Rβ(W ) =\n",
      "root 11 text β(cid:107)W\n",
      "root 8 text (cid:62)W (cid:12) (1 − I)(cid:107)2\n",
      "F,\n",
      "where 1 denotes a matrix with all elements set to 1.\n",
      "root 5 text We sweep β values and select 10−4, ﬁnding\n",
      "this small added penalty sufﬁcient to improve the likelihood that our models will be amenable to\n",
      "truncation.\n",
      "root 7 text Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of\n",
      "models are amenable to truncation, compared to 60% when trained with Orthogonal Regularization.\n",
      "root 2 text \n",
      "\n",
      "(3)\n",
      "\n",
      "\n",
      "root 3 text We ﬁnd that current GAN techniques are sufﬁcient to enable scaling to large models and distributed,\n",
      "large-batch training.\n",
      "root 4 text (2018).\n",
      "root 7 text Despite these improvements, our models undergo training collapse, necessitating early stopping in\n",
      "practice.\n",
      "root 11 text In the next two sections we investigate why settings which were stable in previous works\n",
      "become unstable when applied at scale.\n",
      "root 2 text \n",
      "\n",
      "3.2 SUMMARY\n",
      "\n",
      "4 ANALYSIS\n",
      "\n",
      "(a) G\n",
      "\n",
      "(b) D\n",
      "\n",
      "Figure 3:\n",
      "root 4 text A typical plot of the ﬁrst singular value σ0 in the layers of G (a) and D (b) before Spectral\n",
      "Normalization.\n",
      "root 4 text Most layers in G have well-behaved spectra, but without constraints a small sub-\n",
      "set grow throughout training and explode at collapse.\n",
      "root 7 text D’s spectra are noisier but otherwise better-\n",
      "behaved.\n",
      "root 8 text Colors from red to violet indicate increasing depth.\n",
      "root 2 text \n",
      "\n",
      "4.1 CHARACTERIZING INSTABILITY: THE GENERATOR\n",
      "\n",
      "Much previous work has investigated GAN stability from a variety of analytical angles and on\n",
      "toy problems, but the instabilities we observe occur for settings which are stable at small scale,\n",
      "necessitating direct analysis at large scale.\n",
      "root 7 text We monitor a range of weight, gradient, and loss statistics\n",
      "during training, in search of a metric which might presage the onset of training collapse, similar to\n",
      "(Odena et al., 2018).\n",
      "root 5 text We found the top three singular values σ0, σ1, σ2 of each weight matrix to be\n",
      "the most informative.\n",
      "root 8 text They can be efﬁciently computed using the Alrnoldi iteration method (Golub\n",
      "& der Vorst, 2000), which extends the power iteration method, used in Miyato et al.\n",
      "root 4 text (2018), to\n",
      "estimation of additional singular vectors and values.\n",
      "root 7 text A clear pattern emerges, as can be seen in\n",
      "Figure 3(a) and\n",
      "root 4 text Appendix F: most G layers have well-behaved spectral norms, but some layers\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "(typically the ﬁrst layer in G, which is over-complete and not convolutional) are ill-behaved, with\n",
      "spectral norms that grow throughout training and explode at collapse.\n",
      "root 5 text \n",
      "\n",
      "To ascertain if this pathology is a cause of collapse or merely a symptom, we study the effects of\n",
      "imposing additional conditioning on G to explicitly counteract spectral explosion.\n",
      "root 10 text First, we directly\n",
      "regularize the top singular values σ0 of each weight, either towards a ﬁxed value σreg or towards\n",
      "some ratio r of the second singular value, r · sg(σ1) (with sg the stop-gradient operation to prevent\n",
      "the regularization from increasing σ1).\n",
      "root 6 text Alternatively, we employ a partial singular value decompo-\n",
      "sition to instead clamp σ0.\n",
      "root 6 text Given a weight W , its ﬁrst singular vectors u0 and v0, and σclamp the\n",
      "value to which the σ0 will be clamped, our weights become:\n",
      "\n",
      "W = W − max(0, σ0 − σclamp)v0u(cid:62)\n",
      "0 ,\n",
      "where σclamp is set to either σreg or r · sg(σ1).\n",
      "root 7 text We observe that both with and without Spectral\n",
      "Normalization these techniques have the effect of preventing the gradual increase and explosion of\n",
      "either σ0 or σ0\n",
      ", but even though in some cases they mildly improve performance, no combination\n",
      "σ1\n",
      "prevents training collapse.\n",
      "root 8 text This evidence suggests that while conditioning G might improve stability,\n",
      "it is insufﬁcient to ensure stability.\n",
      "root 4 text We accordingly turn our attention to D.\n",
      "\n",
      "(4)\n",
      "\n",
      "4.2 CHARACTERIZING INSTABILITY: THE DISCRIMINATOR\n",
      "\n",
      "\n",
      "root 7 text As with G, we analyze the spectra of D’s weights to gain insight into its behavior, then seek to\n",
      "stabilize training by imposing additional constraints.\n",
      "root 8 text Figure 3(b) displays a typical plot of σ0 for D\n",
      "(with further plots in Appendix F).\n",
      "root 2 text Unlike G, we see that the spectra are noisy, σ0\n",
      "is well-behaved,\n",
      "σ1\n",
      "and the singular values grow throughout training but only jump at collapse, instead of exploding.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 7 text The spikes in D’s spectra might suggest that it periodically receives very large gradients, but we\n",
      "observe that the Frobenius norms are smooth (Appendix F), suggesting that this effect is primarily\n",
      "concentrated on the top few singular directions.\n",
      "root 5 text We posit that this noise is a result of optimization\n",
      "through the adversarial training process, where G periodically produces batches which strongly per-\n",
      "turb D .\n",
      "root 2 text If this spectral noise is causally related to instability, a natural counter is to employ gradient\n",
      "penalties, which explicitly regularize changes in D’s Jacobian.\n",
      "root 7 text We explore the R1 zero-centered\n",
      "gradient penalty from Mescheder et al.\n",
      "root 2 text (2018):\n",
      "\n",
      "R1 :=\n",
      "\n",
      "EpD(x)\n",
      "\n",
      "(cid:2)(cid:107)∇D(x)(cid:107)2\n",
      "\n",
      "(cid:3) .\n",
      "root 2 text \n",
      "\n",
      "F\n",
      "\n",
      "γ\n",
      "2\n",
      "\n",
      "(5)\n",
      "\n",
      "\n",
      "root 7 text With the default suggested γ strength of 10, training becomes stable and improves the smoothness\n",
      "and boundedness of spectra in both G and D, but performance severely degrades, resulting in a 45%\n",
      "reduction in IS.\n",
      "root 7 text Reducing the penalty partially alleviates this degradation, but results in increasingly\n",
      "ill-behaved spectra; even with the penalty strength reduced to 1 (the lowest strength for which sud-\n",
      "den collapse does not occur) the IS is reduced by 20%.\n",
      "root 7 text Repeating this experiment with various\n",
      "strengths of Orthogonal Regularization, DropOut (Srivastava et al., 2014), and L2 (See Appendix I\n",
      "for details), reveals similar behaviors for these regularization strategies: with high enough penalties\n",
      "on D, training stability can be achieved, but at a substantial cost to performance.\n",
      "root 7 text \n",
      "\n",
      "We also observe that D’s loss approaches zero during training, but undergoes a sharp upward jump at\n",
      "collapse (Appendix F).\n",
      "root 2 text One possible explanation for this behavior is that D is overﬁtting to the train-\n",
      "ing set, memorizing training examples rather than learning some meaningful boundary between real\n",
      "and generated images.\n",
      "root 2 text As a simple test for D’s memorization (related to Gulrajani et al.\n",
      "root 4 text (2017))\n",
      "root 8 text , we\n",
      "evaluate uncollapsed discriminators on the ImageNet training and validation sets, and measure what\n",
      "percentage of samples are classiﬁed as real or generated.\n",
      "root 5 text While the training accuracy is consistently\n",
      "above 98%, the validation accuracy falls in the range of 50-55%, no better than random guessing\n",
      "(regardless of regularization strategy).\n",
      "root 4 text This conﬁrms that D is indeed memorizing the training set;\n",
      "we deem this in line with D’s role, which is not explicitly to generalize, but to distill the training\n",
      "data and provide a useful learning signal for G. Additional experiments and discussion are provided\n",
      "in Appendix G.\n",
      "\n",
      "4.3 SUMMARY\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 4 text We ﬁnd that stability does not come solely from G or D, but from their interaction through the\n",
      "adversarial training process.\n",
      "root 2 text While the symptoms of their poor conditioning can be used to track and\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Model\n",
      "SN-GAN\n",
      "SA-GAN\n",
      "BigGAN\n",
      "BigGAN\n",
      "BigGAN\n",
      "BigGAN-deep\n",
      "BigGAN-deep\n",
      "BigGAN-deep\n",
      "\n",
      "Res.\n",
      "128\n",
      "128\n",
      "128\n",
      "256\n",
      "512\n",
      "128\n",
      "256\n",
      "512\n",
      "\n",
      "FID/IS\n",
      "27.62/36.80\n",
      "18.65/52.52\n",
      "8.7 ± .6/98.8 ± 3\n",
      "8.7 ± .1/142.3\n",
      "root 2 text (min FID) / IS\n",
      "N/A\n",
      "root 2 text 5\n",
      "7.6/170.3\n",
      "6.3 ± .3/148.1 ± 4\n",
      "7.0 ± .1/202.6 ± 2\n",
      "7.7/181.4\n",
      "\n",
      "FID / (valid IS)\n",
      "N/A\n",
      "N/A\n",
      "9.6 ± .4/166.3 ± 1\n",
      "9.3 ± .3/233.1 ± 1\n",
      "11.8/241.4\n",
      "7.4 ± .6/166.5 ± 1\n",
      "8.1 ± .1/232.5 ±\n",
      "root 2 text 2\n",
      "11.5/241.5\n",
      "\n",
      "FID / (max IS)\n",
      "N/A\n",
      "N/A\n",
      "25 ± 2/206 ± 2\n",
      "25 ± 5/291 ± 4\n",
      "27.0/275\n",
      "25 ± 2/253 ± 11\n",
      "27 ± 8/317 ± 6\n",
      "39.7/298\n",
      "\n",
      "\n",
      "root 5 text Table 2: Evaluation of models at different resolutions.\n",
      "root 2 text We report scores without truncation (Column\n",
      "3), scores at the best FID (Column 4), scores at the IS of validation data (Column 5), and scores at\n",
      "the max IS (Column 6).\n",
      "root 8 text Standard deviations are computed over at least three random initializations.\n",
      "root 8 text \n",
      "\n",
      "identify instability, ensuring reasonable conditioning proves necessary for training but insufﬁcient to\n",
      "prevent eventual training collapse.\n",
      "root 2 text It is possible to enforce stability by strongly constraining D, but\n",
      "doing so incurs a dramatic cost in performance.\n",
      "root 8 text With current techniques, better ﬁnal performance\n",
      "can be achieved by relaxing this conditioning and allowing collapse to occur at the later stages of\n",
      "training, by which time a model is sufﬁciently trained to achieve good results.\n",
      "root 6 text \n",
      "\n",
      "5 EXPERIMENTS\n",
      "\n",
      "(a) 128×128\n",
      "\n",
      "(b) 256×256\n",
      "\n",
      "(c) 512×512\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 4:\n",
      "root 7 text Samples from our BigGAN model with truncation threshold 0.5 (a-c) and an example of\n",
      "class leakage in a partially trained model (d).\n",
      "root 2 text \n",
      "\n",
      "5.1 EVALUATION ON IMAGENET\n",
      "\n",
      "\n",
      "root 8 text We evaluate our models on ImageNet ILSVRC 2012 (Russakovsky et al., 2015) at 128×128,\n",
      "256×256, and 512×512 resolutions, employing the settings from Table 1, row 8.\n",
      "root 9 text The samples\n",
      "generated by our models are presented in Figure 4, with additional samples in Appendix A, and on-\n",
      "line 2.\n",
      "root 6 text We report IS and FID in Table 2.\n",
      "root 6 text As our models are able to trade sample variety for quality, it\n",
      "is unclear how best to compare against prior art; we accordingly report values at three settings, with\n",
      "complete curves in Appendix D. First, we report the FID/IS values at the truncation setting which\n",
      "attains the best FID.\n",
      "root 6 text Second, we report the FID at the truncation setting for which our model’s IS is\n",
      "the same as that attained by the real validation data, reasoning that this is a passable measure of max-\n",
      "imum sample variety achieved while still achieving a good level of “objectness.”\n",
      "root 6 text Third, we report\n",
      "FID at the maximum IS achieved by each model, to demonstrate how much variety must be traded\n",
      "off to maximize quality.\n",
      "root 10 text In all three cases, our models outperform the previous state-of-the-art IS\n",
      "and FID scores achieved by Miyato et al.\n",
      "root 4 text (2018) and Zhang et al.\n",
      "root 4 text (2018).\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 7 text In addition to the BigGAN model introduced in the ﬁrst version of the paper and used in the majority\n",
      "of experiments (unless otherwise stated), we also present a 4x deeper model (BigGAN-deep) which\n",
      "uses a different conﬁguration of residual blocks.\n",
      "root 11 text As can be seen from Table 2, BigGAN-deep sub-\n",
      "stantially outperforms BigGAN across all resolutions and metrics.\n",
      "root 7 text This conﬁrms that our ﬁndings\n",
      "\n",
      "\n",
      "root 2 text 2https://drive.google.com/drive/folders/1lWC6XEPD0LT5KUnPXeve_kWeY-FxH002\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "Skip-z Ortho.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 2 text Ch.\n",
      "root 5 text Param (M)\n",
      "317.1\n",
      "99.4\n",
      "207.9\n",
      "355.7\n",
      "\n",
      "\n",
      "root 2 text Shared\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "(cid:55)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "(cid:51)\n",
      "\n",
      "FID\n",
      "48.38\n",
      "23.48\n",
      "18.84\n",
      "13.75\n",
      "\n",
      "IS\n",
      "23.27\n",
      "24.78\n",
      "27.86\n",
      "30.61\n",
      "\n",
      "(min FID) / IS\n",
      "48.6/23.1\n",
      "22.4/21.0\n",
      "17.1/23.3\n",
      "13.0/28.0\n",
      "\n",
      "\n",
      "root 2 text FID / (max IS)\n",
      "49.1/23.9\n",
      "60.9/35.8\n",
      "51.6/38.1\n",
      "46.2/47.8\n",
      "\n",
      "Table 3: BigGAN results on JFT-300M at 256×256 resolution.\n",
      "root 6 text The FID and IS columns report these\n",
      "scores given by the JFT-300M-trained Inception v2 classiﬁer with noise distributed as z ∼ N (0, I)\n",
      "(non-truncated).\n",
      "root 2 text The (min FID) / IS and FID / (max IS) columns report scores at the best FID and\n",
      "IS from a sweep across truncated noise distributions ranging from σ = 0 to σ = 2.\n",
      "root 4 text Images from the\n",
      "JFT-300M validation set have an IS of 50.88 and FID of 1.94.\n",
      "root 6 text \n",
      "\n",
      "extend to other architectures, and that increased depth leads to improvement in sample quality.\n",
      "root 9 text Both\n",
      "BigGAN and BigGAN-deep architectures are described in Appendix B.\n",
      "\n",
      "Our observation that D overﬁts to the training set, coupled with our model’s sample quality, raises\n",
      "the obvious question of whether or not G simply memorizes training points.\n",
      "root 7 text To test this, we perform\n",
      "class-wise nearest neighbors analysis in pixel space and the feature space of pre-trained classiﬁer\n",
      "networks (Appendix A).\n",
      "root 7 text In addition, we present both interpolations between samples and class-wise\n",
      "interpolations (where z is held constant) in Figures 8 and 9.\n",
      "root 12 text Our model convincingly interpolates\n",
      "between disparate samples, and the nearest neighbors for its samples are visually distinct, suggesting\n",
      "that our model does not simply memorize training data.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 4 text We note that some failure modes of our partially-trained models are distinct from those previously\n",
      "observed.\n",
      "root 7 text Most previous failures involve local artifacts (Odena et al., 2016), images consisting\n",
      "of texture blobs instead of objects (Salimans et al., 2016), or the canonical mode collapse.\n",
      "root 7 text We\n",
      "observe class leakage, where images from one class contain properties of another, as exempliﬁed\n",
      "by Figure 4(d).\n",
      "root 2 text We also ﬁnd that many classes on ImageNet are more difﬁcult than others for our\n",
      "model; our model is more successful at generating dogs (which make up a large portion of the\n",
      "dataset, and are mostly distinguished by their texture) than crowds (which comprise a small portion\n",
      "of the dataset and have more large-scale structure).\n",
      "root 7 text Further discussion is available in Appendix A.\n",
      "\n",
      "5.2 ADDITIONAL EVALUATION ON JFT-300M\n",
      "\n",
      "To conﬁrm that our design choices are effective for even larger and more complex and diverse\n",
      "datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017).\n",
      "root 8 text The\n",
      "full JFT-300M dataset contains 300M real-world images labeled with 18K categories.\n",
      "root 9 text Since the\n",
      "category distribution is heavily long-tailed, we subsample the dataset to keep only images with the\n",
      "8.5K most common labels.\n",
      "root 8 text The resulting dataset contains 292M images – two orders of magnitude\n",
      "larger than ImageNet.\n",
      "root 6 text For images with multiple labels, we sample a single label randomly and\n",
      "independently whenever an image is sampled.\n",
      "root 3 text To compute IS and FID for the GANs trained on this\n",
      "dataset, we use an Inception v2 classiﬁer (Szegedy et al., 2016) trained on this dataset.\n",
      "root 9 text Quantitative\n",
      "results are presented in Table 3.\n",
      "root 7 text All models are trained with batch size 2048.\n",
      "root 7 text We compare an ablated\n",
      "version of our model – comparable to SA-GAN (Zhang et al., 2018) but with the larger batch size\n",
      "– against a “full” BigGAN model that makes uses of all of the techniques applied to obtain the\n",
      "best results on ImageNet (shared embedding, skip-z, and orthogonal regularization).\n",
      "root 4 text Our results\n",
      "show that these techniques substantially improve performance even in the setting of this much larger\n",
      "dataset at the same model capacity (64 base channels).\n",
      "root 4 text We further show that for a dataset of this\n",
      "scale, we see signiﬁcant additional improvements from expanding the capacity of our models to 128\n",
      "base channels, while for ImageNet GANs that additional capacity was not beneﬁcial.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 7 text In Figure 19 (Appendix D), we present truncation plots for models trained on this dataset.\n",
      "root 9 text Unlike\n",
      "for ImageNet, where truncation limits of σ ≈ 0 tend to produce the highest ﬁdelity scores, IS is\n",
      "typically maximized for our JFT-300M models when the truncation value σ ranges from 0.5 to 1.\n",
      "root 7 text We suspect that this is at least partially due to the intra-class variability of JFT-300M labels, as well\n",
      "as the relative complexity of the image distribution, which includes images with multiple objects at a\n",
      "variety of scales.\n",
      "root 6 text Interestingly, unlike models trained on ImageNet, where training tends to collapse\n",
      "without heavy regularization (Section 4), the models trained on JFT-300M remain stable over many\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "hundreds of thousands of iterations.\n",
      "root 8 text This suggests that moving beyond ImageNet to larger datasets\n",
      "may partially alleviate GAN stability issues.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 11 text The improvement over the baseline GAN model that we achieve on this dataset without changes to\n",
      "the underlying models or training and regularization techniques (beyond expanded capacity) demon-\n",
      "root 3 text \n",
      "strates that our ﬁndings extend from ImageNet to datasets with scale and complexity thus far un-\n",
      "precedented for generative models of images.\n",
      "root 12 text \n",
      "\n",
      "We have demonstrated that Generative Adversarial Networks trained to model natural images of\n",
      "multiple categories highly beneﬁt from scaling up, both in terms of ﬁdelity and variety of the gen-\n",
      "erated samples.\n",
      "root 3 text As a result, our models set a new level of performance among ImageNet GAN\n",
      "models, improving on the state of the art by a large margin.\n",
      "root 9 text We have also presented an analysis\n",
      "of the training behavior of large scale GANs, characterized their stability in terms of the singular\n",
      "values of their weights, and discussed the interplay between stability and performance.\n",
      "root 4 text \n",
      "\n",
      "We would like to thank Kai Arulkumaran, Matthias Bauer, Peter Buchlovsky, Jeffrey Defauw,\n",
      "Sander Dieleman, Ian Goodfellow, Ariel Gordon, Karol Gregor, Dominik Grewe, Chris Jones, Jacob\n",
      "Menick, Augustus Odena, Suman Ravuri, Ali Razavi, Mihaela Rosca, and Jeff Stanway.\n",
      "root 2 text \n",
      "\n",
      "6 CONCLUSION\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu\n",
      "Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,\n",
      "Rajat Monga, Sherry Moore, Derek Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete\n",
      "Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.\n",
      "root 10 text TensorFlow:\n",
      "root 6 text A system for large-scale\n",
      "machine learning.\n",
      "root 2 text In OSDI, 2016.\n",
      "root 2 text \n",
      "\n",
      "Martin Arjovsky, Soumith Chintala, and L´eon Bottou.\n",
      "root 10 text Wasserstein generative adversarial networks.\n",
      "root 2 text \n",
      "\n",
      "Shane Barratt and Rishi Sharma.\n",
      "root 4 text A note on the Inception Score.\n",
      "root 2 text In arXiv preprint arXiv:1801.01973,\n",
      "\n",
      "In ICML, 2017.\n",
      "root 2 text \n",
      "\n",
      "2018.\n",
      "root 2 text \n",
      "\n",
      "Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,\n",
      "Stephan Hoyer, and R´emi Munos.\n",
      "root 8 text The Cramer distance as a solution to biased Wasserstein gra-\n",
      "dients.\n",
      "root 2 text In arXiv preprint arXiv:1705.10743, 2017.\n",
      "root 2 text \n",
      "\n",
      "Mikolaj Bi´nkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton.\n",
      "root 2 text Demystifying MMD\n",
      "\n",
      "GANs.\n",
      "root 2 text In ICLR, 2018.\n",
      "root 2 text \n",
      "\n",
      "Andrew Brock, Theodore Lim, J.M. Ritchie, and Nick Weston.\n",
      "root 4 text Neural photo editing with introspec-\n",
      "\n",
      "tive adversarial networks.\n",
      "root 2 text In ICLR, 2017.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 4 text Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel.\n",
      "root 7 text Infogan:\n",
      "Interpretable representation learning by information maximizing generative adversarial nets.\n",
      "root 2 text In\n",
      "NIPS, 2016.\n",
      "root 2 text \n",
      "\n",
      "Harm de Vries, Florian Strub, J´er´emie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron\n",
      "\n",
      "Courville.\n",
      "root 10 text Modulating early visual processing by language.\n",
      "root 2 text In NIPS, 2017.\n",
      "root 6 text \n",
      "\n",
      "Emily Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus.\n",
      "root 6 text Deep generative image models\n",
      "\n",
      "using a laplacian pyramid of adversarial networks.\n",
      "root 2 text In NIPS, 2015.\n",
      "root 2 text \n",
      "\n",
      "Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.\n",
      "root 14 text A learned representation for artistic\n",
      "\n",
      "style.\n",
      "root 2 text In ICLR, 2017.\n",
      "root 2 text \n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "\n",
      "root 5 text William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and\n",
      "Ian Goodfellow.\n",
      "root 4 text Many paths to equilibrium: GANs do not need to decrease a divergence at every\n",
      "step.\n",
      "root 2 text In ICLR, 2018.\n",
      "root 2 text \n",
      "\n",
      "Xavier Glorot and Yoshua Bengio.\n",
      "root 13 text Understanding the difﬁculty of training deep feedforward neural\n",
      "\n",
      "networks.\n",
      "root 2 text In AISTATS, 2010.\n",
      "root 2 text \n",
      "\n",
      "Gene Golub and Henk Van der Vorst.\n",
      "root 11 text Eigenvalue computation in the 20th century.\n",
      "root 13 text Journal of\n",
      "\n",
      "Computational and Applied Mathematics, 123:35–65, 2000.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 10 text Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\n",
      "\n",
      "and Aaron Courville Yoshua Bengio.\n",
      "root 4 text Generative adversarial nets.\n",
      "root 2 text In NIPS, 2014.\n",
      "\n",
      "\n",
      "root 6 text Google.\n",
      "root 4 text Cloud TPUs.\n",
      "root 29 text https://cloud.google.com/tpu/, 2018.\n",
      "root 2 text \n",
      "\n",
      "Ishaan Gulrajani, Faruk Ahmed, Mart´ın Arjovsky, Vincent Dumoulin, and Aaron C. Courville.\n",
      "root 6 text Im-\n",
      "\n",
      "proved training of Wasserstein GANs.\n",
      "root 2 text In NIPS, 2017.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 2 text Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
      "root 8 text Deep residual learning for image recog-\n",
      "root 6 text \n",
      "\n",
      "nition.\n",
      "root 2 text In CVPR, 2016.\n",
      "root 2 text \n",
      "\n",
      "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G¨unter Klambauer, and\n",
      "Sepp Hochreiter.\n",
      "root 8 text GANs trained by a two time-scale update rule converge to a local nash equilib-\n",
      "rium.\n",
      "root 2 text In NIPS, 2017.\n",
      "root 2 text \n",
      "\n",
      "Sergey Ioffe and Christian Szegedy.\n",
      "root 13 text Batch normalization: Accelerating deep network training by\n",
      "\n",
      "reducing internal covariate shift.\n",
      "root 2 text In ICML, 2015.\n",
      "root 2 text \n",
      "\n",
      "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\n",
      "root 8 text Progressive growing of GANs for\n",
      "\n",
      "improved quality, stability, and variation.\n",
      "root 2 text In ICLR, 2018.\n",
      "root 2 text \n",
      "\n",
      "Diederik Kingma and Jimmy Ba.\n",
      "root 4 text Adam: A method for stochastic optimization.\n",
      "root 2 text In ICLR, 2014.\n",
      "root 2 text \n",
      "\n",
      "Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira.\n",
      "root 2 text On convergence and stability of\n",
      "\n",
      "GANs.\n",
      "root 2 text In arXiv preprint arXiv:1705.07215, 2017.\n",
      "root 2 text \n",
      "\n",
      "Alex Krizhevsky and Geoffrey Hinton.\n",
      "root 8 text Learning multiple layers of features from tiny images.\n",
      "root 4 text 2009.\n",
      "root 2 text \n",
      "\n",
      "Jae Hyun Lim and Jong Chul Ye.\n",
      "root 3 text Geometric GAN.\n",
      "root 2 text In arXiv preprint arXiv:1705.02894, 2017.\n",
      "root 2 text \n",
      "\n",
      "Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, and Zhen Wang.\n",
      "root 10 text Least squares generative\n",
      "\n",
      "adversarial networks.\n",
      "root 16 text In arXiv preprint arXiv:1611.04076, 2016.\n",
      "root 2 text \n",
      "\n",
      "Marco Marchesi.\n",
      "root 8 text Megapixel size image creation using generative adversarial networks.\n",
      "root 16 text In arXiv\n",
      "\n",
      "preprint arXiv:1706.00082, 2016.\n",
      "root 8 text \n",
      "\n",
      "actually converge?\n",
      "root 2 text In ICML, 2018.\n",
      "root 15 text \n",
      "\n",
      "arXiv:1411.1784, 2014.\n",
      "root 2 text \n",
      "\n",
      "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.\n",
      "root 2 text Which training methods for GANs do\n",
      "\n",
      "Mehdi Mirza and Simon Osindero.\n",
      "root 4 text Conditional generative adversarial nets.\n",
      "root 2 text \n",
      "\n",
      "In arXiv preprint\n",
      "\n",
      "Takeru Miyato and Masanori Koyama.\n",
      "root 5 text cGANs with projection discriminator.\n",
      "root 2 text In ICLR, 2018.\n",
      "root 2 text \n",
      "\n",
      "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.\n",
      "root 2 text Spectral normalization\n",
      "\n",
      "for generative adversarial networks.\n",
      "root 2 text In ICLR, 2018.\n",
      "root 7 text \n",
      "\n",
      "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.\n",
      "root 8 text f-GAN: Training generative neural sam-\n",
      "\n",
      "plers using variational divergence minimization.\n",
      "root 2 text In NIPS, 2016.\n",
      "root 2 text \n",
      "\n",
      "Augustus Odena, Vincent Dumoulin, and Chris Olah.\n",
      "root 9 text Deconvolution and checkerboard artifacts.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 7 text Distill, 2016.\n",
      "root 2 text \n",
      "\n",
      "Augustus Odena, Christopher Olah, and Jonathon Shlens.\n",
      "root 2 text Conditional image synthesis with auxil-\n",
      "\n",
      "iary classiﬁer GANs.\n",
      "root 2 text In ICML, 2017.\n",
      "root 2 text \n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "\n",
      "root 5 text Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B. Brown, Christopher Olah, Colin Raf-\n",
      "fel, and Ian Goodfellow.\n",
      "root 2 text Is generator conditioning causally related to GAN performance?\n",
      "root 2 text In\n",
      "ICML, 2018.\n",
      "root 2 text \n",
      "\n",
      "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville.\n",
      "root 4 text FiLM:\n",
      "root 9 text Visual\n",
      "\n",
      "reasoning with a general conditioning layer.\n",
      "root 2 text In AAAI, 2018.\n",
      "root 2 text \n",
      "\n",
      "Mathijs Pieters and Marco Wiering.\n",
      "root 9 text Comparing generative adversarial network techniques for image\n",
      "\n",
      "creation and modiﬁcatio.\n",
      "root 2 text In arXiv preprint arXiv:1803.09093, 2014.\n",
      "root 2 text \n",
      "\n",
      "Alec Radford, Luke Metz, and Soumith Chintala.\n",
      "root 14 text Unsupervised representation learning with deep\n",
      "\n",
      "convolutional generative adversarial networks.\n",
      "root 2 text In ICLR, 2016.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 11 text Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\n",
      "\n",
      "root 6 text ImageNet large scale visual\n",
      "\n",
      "Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein.\n",
      "root 9 text \n",
      "recognition challenge.\n",
      "root 4 text IJCV, 115:211–252, 2015.\n",
      "root 2 text \n",
      "\n",
      "Tim Salimans and Diederik Kingma.\n",
      "root 5 text Weight normalization: A simple reparameterization to accel-\n",
      "\n",
      "erate training of deep neural networks.\n",
      "root 2 text In NIPS, 2016.\n",
      "root 2 text \n",
      "\n",
      "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\n",
      "root 2 text \n",
      "\n",
      "Improved techniques for training GANs.\n",
      "root 2 text In NIPS, 2016.\n",
      "root 2 text \n",
      "\n",
      "Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas.\n",
      "root 9 text Improving GANs using optimal\n",
      "\n",
      "transport.\n",
      "root 2 text In ICLR, 2018.\n",
      "root 2 text \n",
      "\n",
      "Andrew Saxe, James McClelland, and Surya Ganguli.\n",
      "root 9 text Exact solutions to the nonlinear dynamics of\n",
      "\n",
      "learning in deep linear neural networks.\n",
      "root 2 text In ICLR, 2014.\n",
      "root 2 text \n",
      "\n",
      "Karen Simonyan and Andrew Zisserman.\n",
      "root 2 text Very deep convolutional networks for large-scale image\n",
      "\n",
      "recognition.\n",
      "root 2 text In ICLR, 2015.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 8 text Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszr.\n",
      "root 3 text Amortised\n",
      "\n",
      "map inference for image super-resolution.\n",
      "root 2 text In ICLR, 2017.\n",
      "root 2 text \n",
      "\n",
      "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n",
      "root 7 text Dropout: A simple way to prevent neural networks from overﬁtting.\n",
      "root 4 text JMLR, 15:1929–1958, 2014.\n",
      "root 2 text \n",
      "\n",
      "Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.\n",
      "root 10 text Revisiting unreasonable ef-\n",
      "root 2 text \n",
      "\n",
      "fectiveness of data in deep learning era.\n",
      "root 2 text In ICCV, 2017.\n",
      "root 2 text \n",
      "\n",
      "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "root 3 text Re-\n",
      "root 8 text \n",
      "\n",
      "thinking the inception architecture for computer vision.\n",
      "root 2 text In CVPR, 2016.\n",
      "root 2 text \n",
      "\n",
      "Lucas Theis, A¨aron\n",
      "root 4 text van den Oord, and Matthias Bethge.\n",
      "root 4 text A note on the evaluation of generative\n",
      "\n",
      "models.\n",
      "root 2 text In arXiv preprint arXiv:1511.01844, 2015.\n",
      "root 2 text \n",
      "\n",
      "Dustin Tran, Rajesh Ranganath, and David M. Blei.\n",
      "root 6 text Hierarchical implicit models and likelihood-free\n",
      "\n",
      "variational inference.\n",
      "root 2 text In NIPS, 2017.\n",
      "root 2 text \n",
      "\n",
      "Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He.\n",
      "root 8 text Non-local neural networks.\n",
      "root 2 text In\n",
      "\n",
      "CVPR, 2018.\n",
      "root 2 text \n",
      "\n",
      "Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger B. Grosse.\n",
      "root 2 text On the quantitative analysis\n",
      "\n",
      "of decoder-based generative models.\n",
      "root 2 text In ICLR, 2017.\n",
      "root 2 text \n",
      "\n",
      "Yasin Yazc, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay\n",
      "\n",
      "root 2 text In arXiv preprint\n",
      "\n",
      "Chandrasekhar.\n",
      "root 13 text The unusual effectiveness of averaging in gan training.\n",
      "root 16 text \n",
      "arXiv:1806.04498, 2018.\n",
      "root 2 text \n",
      "\n",
      "Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena.\n",
      "root 2 text Self-attention generative\n",
      "\n",
      "adversarial networks.\n",
      "root 2 text In arXiv preprint arXiv:1805.08318, 2018.\n",
      "root 2 text \n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "\n",
      "root 2 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX A ADDITIONAL SAMPLES, INTERPOLATIONS, AND NEAREST\n",
      "\n",
      "NEIGHBORS FROM IMAGENET MODELS\n",
      "\n",
      "\n",
      "root 6 text Figure 5: Samples generated by our BigGAN model at 256×256 resolution.\n",
      "root 6 text \n",
      "\n",
      "Figure 6: Samples generated by our BigGAN model at 512×512 resolution.\n",
      "root 2 text \n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "\n",
      "root 6 text (a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 7:\n",
      "root 9 text Comparing easy classes (a) with difﬁcult classes (b) at 512×512.\n",
      "root 3 text Classes such as dogs\n",
      "which are largely textural, and common in the dataset, are far easier to model than classes involving\n",
      "unaligned human faces or crowds.\n",
      "root 3 text Such classes are more dynamic and structured, and often have\n",
      "details to which human observers are more sensitive.\n",
      "root 2 text The difﬁculty of modeling global structure is\n",
      "further exacerbated when producing high-resolution images, even with non-local blocks.\n",
      "root 6 text \n",
      "\n",
      "Figure 8: Interpolations between z, c pairs.\n",
      "root 14 text \n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Figure 9: Interpolations between c with z held constant.\n",
      "root 10 text Pose semantics are frequently maintained\n",
      "between endpoints (particularly in the ﬁnal row).\n",
      "root 12 text Row 2 demonstrates that grayscale is encoded in\n",
      "the joint z, c space, rather than in z.\n",
      "\n",
      "\n",
      "root 6 text Figure 10: Nearest neighbors in VGG-16-fc7 (Simonyan & Zisserman, 2015) feature space.\n",
      "root 2 text The\n",
      "generated image is in the top left.\n",
      "root 2 text \n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Figure 11: Nearest neighbors in ResNet-50-avgpool (He et al., 2016) feature space.\n",
      "root 2 text The generated\n",
      "image is in the top left.\n",
      "root 6 text \n",
      "\n",
      "Figure 12: Nearest neighbors in pixel space.\n",
      "root 2 text The generated image is in the top left.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 9 text 15\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Figure 13: Nearest neighbors in VGG-16-fc7 (Simonyan & Zisserman, 2015) feature space.\n",
      "root 2 text The\n",
      "generated image is in the top left.\n",
      "root 6 text \n",
      "\n",
      "Figure 14: Nearest neighbors in ResNet-50-avgpool (He et al., 2016) feature space.\n",
      "root 2 text The generated\n",
      "image is in the top left.\n",
      "root 2 text \n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX B ARCHITECTURAL DETAILS\n",
      "\n",
      "\n",
      "root 3 text In the BigGAN model (Figure 15), we use the ResNet (He et al., 2016)\n",
      "root 12 text GAN architecture of (Zhang\n",
      "et al., 2018), which is identical to that used by (Miyato et al., 2018)\n",
      "root 2 text , but with the channel pattern\n",
      "in D modiﬁed so that the number of ﬁlters in the ﬁrst convolutional layer of each block is equal\n",
      "to the number of output ﬁlters (rather than the number of input ﬁlters, as in Miyato et al.\n",
      "root 4 text (2018)\n",
      "root 9 text ;\n",
      "Gulrajani et al.\n",
      "root 4 text (2017)).\n",
      "root 3 text We use a single shared class embedding in G, and skip connections for\n",
      "the latent vector z (skip-z).\n",
      "root 6 text In particular, we employ hierarchical latent spaces, so that the latent\n",
      "vector z is split along its channel dimension into chunks of equal size (20-D in our case), and each\n",
      "chunk is concatenated to the shared class embedding and passed to a corresponding residual block\n",
      "as a conditioning vector.\n",
      "root 9 text The conditioning of each block is linearly projected to produce per-sample\n",
      "gains and biases for the BatchNorm layers of the block.\n",
      "root 3 text The bias projections are zero-centered,\n",
      "while the gain projections are centered at 1.\n",
      "root 2 text Since the number of residual blocks depends on the\n",
      "image resolution, the full dimensionality of z is 120 for 128 × 128, 140 for 256 × 256, and 160 for\n",
      "512\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 7 text The BigGAN-deep model (Figure 16) differs from BigGAN in several aspects.\n",
      "root 4 text It uses a simpler vari-\n",
      "ant of skip-z conditioning: instead of ﬁrst splitting z into chunks, we concatenate the entire z with\n",
      "the class embedding, and pass the resulting vector to each residual block through skip connections.\n",
      "root 8 text \n",
      "BigGAN-deep is based on residual blocks with bottlenecks (He et al., 2016), which incorporate\n",
      "two additional 1 × 1 convolutions: the ﬁrst reduces the number of channels by a factor of 4 before\n",
      "the more expensive 3 × 3 convolutions; the second produces the required number of output chan-\n",
      "nels.\n",
      "root 3 text While BigGAN relies on 1 × 1 convolutions in the skip connections whenever the number of\n",
      "channels needs to change, in BigGAN-deep we use a different strategy aimed at preserving identity\n",
      "throughout the skip connections.\n",
      "root 4 text In G, where the number of channels needs to be reduced, we sim-\n",
      "\n",
      "root 6 text ply retain the ﬁrst group of channels and drop the rest to produce the required number of channels.\n",
      "root 4 text In D, where the number of channels should be increased, we pass the input channels unperturbed,\n",
      "and concatenate them with the remaining channels produced by a 1 × 1 convolution.\n",
      "root 2 text As far as the\n",
      "network conﬁguration is concerned, the discriminator is an exact reﬂection of the generator.\n",
      "root 3 text There\n",
      "are two blocks at each resolution (BigGAN uses one), and as a result BigGAN-deep is four times\n",
      "deeper than BigGAN.\n",
      "root 4 text Despite their increased depth, the BigGAN-deep models have signiﬁcantly\n",
      "fewer parameters mainly due to the bottleneck structure of their residual blocks.\n",
      "root 4 text For example, the\n",
      "128 × 128 BigGAN-deep G and D have 50.4M and 34.6M parameters respectively, while the corre-\n",
      "sponding original BigGAN models have 70.4M and 88.0M parameters.\n",
      "root 3 text All BigGAN-deep models\n",
      "use attention at 64 × 64 resolution, channel width multiplier ch = 128, and z ∈ R128.\n",
      "root 2 text \n",
      "\n",
      "(a)\n",
      "\n",
      "\n",
      "root 3 text (b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 15: (a) A typical architectural layout for BigGAN’s G; details are in the following tables.\n",
      "root 2 text \n",
      "(b) A Residual Block (ResBlock up) in BigGAN’s G. (c) A Residual Block (ResBlock down) in\n",
      "BigGAN’s D.\n",
      "\n",
      "17\n",
      "\n",
      "ResBlockResBlockResBlockNon-localImageLinear→\n",
      "root 43 text 4x4x16chSplitzClassConcatConcatConcatAdd3x3\n",
      "root 7 text ConvBatchNormBatchNormConcatLinearLinearUpsampleUpsample1x1 Conv3x3 ConvReLUReLUAdd3x3 ConvAverage Pooling1x1\n",
      "root 11 text ConvReLU3x3 ConvReLUAverage\n",
      "root 7 text Pooling\f",
      "\n",
      "root 2 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a)\n",
      "\n",
      "18\n",
      "\n",
      "(b)\n",
      "\n",
      "\n",
      "root 3 text (c)\n",
      "\n",
      "Figure 16: (a) A typical architectural layout for BigGAN-deep’s G; details are in the following\n",
      "tables.\n",
      "root 5 text (b) A Residual Block (ResBlock up) in BigGAN-deep’s G. (c)\n",
      "root 7 text A Residual Block (ResBlock\n",
      "down) in BigGAN-deep’s D. A ResBlock (without up or down) in BigGAN-deep does not include\n",
      "the Upsample or Average Pooling layers, and has identity skip connections.\n",
      "root 17 text \n",
      "\n",
      "ResBlockResBlockResBlockNon-localImageLinear→ 4x4x16chzClassConcatAdd3x3\n",
      "root 26 text ConvBatchNormBatchNormConcatLinearLinearDrop channelsUpsample1x1 ConvReLUReLUBatchNormUpsample3x3 ConvReLUBatchNormLinear1x1 ConvReLULinearAdd3x3 ConvAveragePooling1x1 ConvReLUReLU3x3 ConvReLU1x1 ConvReLUAverage\n",
      "root 9 text PoolingConcat1x1 Conv\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "Table 4: BigGAN architecture for 128 × 128 images.\n",
      "root 10 text ch represents the channel width multiplier in\n",
      "each network from Table 1.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 7 text z ∈ R120 ∼ N (0, I)\n",
      "Embed(y)\n",
      "root 4 text R128\n",
      "\n",
      "\n",
      "root 6 text Linear (20 + 128) → 4 × 4 × 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 8ch\n",
      "root 2 text → 4ch\n",
      "\n",
      "ResBlock up 4ch → 2ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 2 text Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R128×128×3\n",
      "\n",
      "\n",
      "root 8 text ResBlock down ch\n",
      "root 2 text → 2ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 2ch → 4ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 4ch → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 8ch → 16ch\n",
      "\n",
      "ResBlock down 16ch → 16ch\n",
      "\n",
      "\n",
      "root 7 text ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h\n",
      "root 5 text + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "Table 5: BigGAN architecture for 256 × 256 images.\n",
      "root 3 text Relative to the 128 × 128 architecture, we\n",
      "add an additional ResBlock in each network at 16×16 resolution, and move the non-local block in\n",
      "G to 128 × 128 resolution.\n",
      "root 7 text Memory constraints prevent us from moving the non-local block in D.\n",
      "\n",
      "\n",
      "root 7 text z ∈ R140 ∼ N (0, I)\n",
      "Embed(y)\n",
      "root 4 text R128\n",
      "\n",
      "\n",
      "root 6 text Linear (20 + 128) → 4 × 4 × 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 8ch\n",
      "root 2 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 8ch\n",
      "root 2 text → 4ch\n",
      "\n",
      "ResBlock up 4ch → 2ch\n",
      "\n",
      "Non-Local Block (128 × 128)\n",
      "\n",
      "\n",
      "root 2 text Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R256×256×3\n",
      "\n",
      "\n",
      "root 8 text ResBlock down ch\n",
      "root 2 text → 2ch\n",
      "\n",
      "ResBlock down 2ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 4ch → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 8ch → 16ch\n",
      "\n",
      "ResBlock down 16ch → 16ch\n",
      "\n",
      "\n",
      "root 7 text ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h\n",
      "root 2 text + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "19\n",
      "\n",
      "\f",
      "\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "Table 6: BigGAN architecture for 512 × 512 images.\n",
      "root 3 text Relative to the 256 × 256 architecture, we\n",
      "add an additional ResBlock at the 512 × 512 resolution.\n",
      "root 5 text Memory constraints force us to move the\n",
      "non-local block in both networks back to 64 × 64 resolution as in the 128 × 128 pixel setting.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 7 text z ∈ R160 ∼ N (0, I)\n",
      "Embed(y)\n",
      "root 4 text R128\n",
      "\n",
      "\n",
      "root 6 text Linear (20 + 128) → 4 × 4 × 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 8ch\n",
      "root 2 text → 8ch\n",
      "\n",
      "\n",
      "root 2 text ResBlock up 8ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 4ch → 2ch\n",
      "\n",
      "\n",
      "root 2 text ResBlock up 2ch → ch\n",
      "\n",
      "ResBlock up ch\n",
      "root 4 text → ch\n",
      "\n",
      "BN, ReLU, 3 × 3 Conv ch\n",
      "root 2 text → 3\n",
      "\n",
      "Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R512×512×3\n",
      "\n",
      "\n",
      "root 8 text ResBlock down ch\n",
      "root 2 text → ch\n",
      "\n",
      "ResBlock down ch\n",
      "root 2 text → 2ch\n",
      "\n",
      "ResBlock down 2ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 4ch → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 8ch → 16ch\n",
      "\n",
      "ResBlock down 16ch → 16ch\n",
      "\n",
      "\n",
      "root 7 text ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h\n",
      "root 2 text + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "\n",
      "root 5 text Table 7: BigGAN-deep architecture for 128 × 128 images.\n",
      "root 2 text \n",
      "\n",
      "z ∈ R128 ∼ N (0, I)\n",
      "Embed(y)\n",
      "root 4 text R128\n",
      "\n",
      "\n",
      "root 6 text Linear (128 + 128)\n",
      "root 4 text ResBlock 16ch\n",
      "root 4 text → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 16ch\n",
      "\n",
      "\n",
      "root 4 text ResBlock 16ch → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 8ch\n",
      "\n",
      "\n",
      "root 3 text ResBlock 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 8ch\n",
      "root 2 text → 4ch\n",
      "\n",
      "ResBlock 4ch\n",
      "root 3 text → 4ch\n",
      "\n",
      "\n",
      "root 2 text ResBlock up 4ch → 2ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 2 text ResBlock 2ch → 2ch\n",
      "\n",
      "ResBlock up 2ch → ch\n",
      "\n",
      "BN, ReLU, 3 × 3 Conv ch → 3\n",
      "\n",
      "\n",
      "root 2 text Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R128×128×3\n",
      "\n",
      "\n",
      "root 2 text 3 × 3 Conv 3 → ch\n",
      "\n",
      "ResBlock down ch\n",
      "root 2 text → 2ch\n",
      "\n",
      "ResBlock 2ch → 2ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 2ch → 4ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock 4ch → 4ch\n",
      "\n",
      "ResBlock down 4ch → 8ch\n",
      "\n",
      "\n",
      "root 3 text ResBlock 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 8ch → 16ch\n",
      "\n",
      "\n",
      "root 4 text ResBlock 16ch → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 16ch → 16ch\n",
      "\n",
      "\n",
      "root 7 text ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h\n",
      "root 2 text + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "20\n",
      "\n",
      "\f",
      "\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "Table 8: BigGAN-deep architecture for 256 × 256 images.\n",
      "root 2 text \n",
      "\n",
      "z ∈ R128 ∼ N (0, I)\n",
      "Embed(y)\n",
      "root 4 text R128\n",
      "\n",
      "\n",
      "root 6 text Linear (128 + 128)\n",
      "root 4 text ResBlock 16ch\n",
      "root 4 text → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 16ch\n",
      "\n",
      "\n",
      "root 4 text ResBlock 16ch → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 8ch\n",
      "\n",
      "\n",
      "root 3 text ResBlock 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 8ch\n",
      "root 2 text → 8ch\n",
      "\n",
      "ResBlock 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 2 text ResBlock up 8ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 8 text ResBlock 4ch → 4ch\n",
      "\n",
      "ResBlock up 4ch → 2ch\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root 2 text ResBlock 2ch → 2ch\n",
      "\n",
      "ResBlock up 2ch → ch\n",
      "\n",
      "BN, ReLU, 3 × 3 Conv ch → 3\n",
      "\n",
      "\n",
      "root 2 text Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R256×256×3\n",
      "\n",
      "3 × 3 Conv 3 → ch\n",
      "\n",
      "ResBlock down ch\n",
      "root 2 text → 2ch\n",
      "\n",
      "ResBlock 2ch → 2ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 2ch → 4ch\n",
      "\n",
      "\n",
      "root 2 text ResBlock 4ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 4ch → 8ch\n",
      "\n",
      "\n",
      "root 3 text ResBlock 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 3 text ResBlock 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 8ch → 16ch\n",
      "\n",
      "\n",
      "root 4 text ResBlock 16ch → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 16ch → 16ch\n",
      "\n",
      "\n",
      "root 7 text ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h\n",
      "root 2 text + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "21\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "\n",
      "root 5 text Table 9: BigGAN-deep architecture for 512 × 512 images.\n",
      "root 2 text \n",
      "\n",
      "z ∈ R128 ∼ N (0, I)\n",
      "Embed(y)\n",
      "root 4 text R128\n",
      "\n",
      "\n",
      "root 6 text Linear (128 + 128)\n",
      "root 4 text ResBlock 16ch\n",
      "root 4 text → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 16ch\n",
      "\n",
      "\n",
      "root 4 text ResBlock 16ch → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 16ch → 8ch\n",
      "\n",
      "\n",
      "root 3 text ResBlock 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock up 8ch\n",
      "root 2 text → 8ch\n",
      "\n",
      "ResBlock 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 2 text ResBlock up 8ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 8 text ResBlock 4ch → 4ch\n",
      "\n",
      "ResBlock up 4ch → 2ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock 2ch → 2ch\n",
      "\n",
      "ResBlock up 2ch → ch\n",
      "\n",
      "\n",
      "root 2 text ResBlock ch → ch\n",
      "\n",
      "\n",
      "root 2 text Tanh\n",
      "\n",
      "(a) Generator\n",
      "\n",
      "RGB image x ∈ R512×512×3\n",
      "\n",
      "\n",
      "root 2 text 3 × 3 Conv 3 → ch\n",
      "\n",
      "ResBlock down ch\n",
      "root 2 text → ch\n",
      "\n",
      "ResBlock ch\n",
      "root 2 text → ch\n",
      "\n",
      "ResBlock down ch\n",
      "root 2 text → 2ch\n",
      "\n",
      "ResBlock 2ch → 2ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 2ch → 4ch\n",
      "\n",
      "\n",
      "root 2 text ResBlock 4ch → 4ch\n",
      "\n",
      "Non-Local Block (64 × 64)\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 4ch → 8ch\n",
      "\n",
      "\n",
      "root 3 text ResBlock 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 3 text ResBlock 8ch\n",
      "root 3 text → 8ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 8ch → 16ch\n",
      "\n",
      "\n",
      "root 4 text ResBlock 16ch → 16ch\n",
      "\n",
      "\n",
      "root 8 text ResBlock down 16ch → 16ch\n",
      "\n",
      "\n",
      "root 7 text ResBlock 16ch → 16ch\n",
      "\n",
      "ReLU, Global sum pooling\n",
      "\n",
      "Embed(y)·h\n",
      "root 2 text + (linear → 1)\n",
      "\n",
      "(b) Discriminator\n",
      "\n",
      "22\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX C EXPERIMENTAL DETAILS\n",
      "\n",
      "\n",
      "root 7 text Our basic setup follows SA-GAN (Zhang et al., 2018), and is implemented in TensorFlow (Abadi\n",
      "et al., 2016).\n",
      "root 6 text We employ the architectures detailed in Appendix B, with non-local blocks inserted at\n",
      "a single stage in each network.\n",
      "root 11 text Both G and D networks are initialized with Orthogonal Initialization\n",
      "(Saxe et al., 2014).\n",
      "root 3 text We use Adam optimizer (Kingma & Ba, 2014) with β1 = 0 and β2 = 0.999 and\n",
      "a constant learning rate.\n",
      "root 3 text For BigGAN models at all resolutions, we use 2 · 10−4 in D and 5 · 10−5\n",
      "in G. For BigGAN-deep, we use the learning rate of 2 · 10−4 in D and 5 · 10−5 in G for 128 × 128\n",
      "models, and 2.5 · 10−5 in both D and G for 256 × 256 and 512 × 512 models.\n",
      "root 12 text We experimented with\n",
      "the number of D steps per G step (varying it from 1 to 6) and found that two D steps per G step gave\n",
      "the best results.\n",
      "root 3 text \n",
      "\n",
      "We use an exponential moving average of the weights of G at sampling time, with a decay rate set to\n",
      "0.9999.\n",
      "root 6 text We employ cross-replica BatchNorm (Ioffe & Szegedy, 2015) in G, where batch statistics are\n",
      "aggregated across all devices, rather than a single device as in standard implementations.\n",
      "root 4 text Spectral\n",
      "Normalization (Miyato et al., 2018) is used in both G and D, following SA-GAN (Zhang et al., 2018).\n",
      "root 5 text We train on a Google TPU v3 Pod, with the number of cores proportional to the resolution: 128 for\n",
      "128×128, 256 for 256×256, and 512 for 512×512.\n",
      "root 5 text Training takes between 24 and 48 hours for\n",
      "most models.\n",
      "root 8 text We increase (cid:15) from the default 10−8 to 10−4 in BatchNorm and Spectral Norm to\n",
      "mollify low-precision numerical issues.\n",
      "root 10 text We preprocess data by cropping along the long edge and\n",
      "rescaling to a given resolution with area resampling.\n",
      "root 3 text \n",
      "\n",
      "C.1 BATCHNORM STATISTICS AND SAMPLING\n",
      "\n",
      "\n",
      "root 10 text The default behavior with batch normalized classiﬁer networks is to use a running average of the\n",
      "activation moments at test time.\n",
      "root 4 text Previous works (Radford et al., 2016) have instead used batch\n",
      "statistics when sampling images.\n",
      "root 5 text While this is not technically an invalid way to sample, it means\n",
      "that results are dependent on the test batch size (and how many devices it is split across), and further\n",
      "complicates reproducibility.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 2 text We ﬁnd that this detail is extremely important, with changes in test batch size producing drastic\n",
      "changes in performance.\n",
      "root 11 text This is further exacerbated when one uses exponential moving averages\n",
      "of G’s weights for sampling, as the BatchNorm running averages are computed with non-averaged\n",
      "weights and are poor estimates of the activation statistics for the averaged weights.\n",
      "root 6 text \n",
      "\n",
      "To counteract both these issues, we employ “standing statistics,” where we compute activation\n",
      "root 7 text statis-\n",
      "tics at sampling time by running the G through multiple forward passes (typically 100) each with\n",
      "different batches of random noise, and storing means and variances aggregated across all forward\n",
      "passes.\n",
      "root 7 text Analogous to using running statistics, this results in G’s outputs becoming invariant to batch\n",
      "size and the number of devices, even when producing a single sample.\n",
      "root 8 text \n",
      "\n",
      "C.2 CIFAR-10\n",
      "\n",
      "\n",
      "root 3 text We run our networks on CIFAR-10 (Krizhevsky & Hinton, 2009) using the settings from Table 1,\n",
      "row 8, and achieve an IS of 9.22 and an FID of 14.73 without truncation.\n",
      "root 2 text \n",
      "\n",
      "C.3\n",
      "root 9 text \n",
      "\n",
      "INCEPTION SCORES OF IMAGENET IMAGES\n",
      "\n",
      "\n",
      "root 7 text We compute the IS for both the training and validation sets of ImageNet.\n",
      "root 3 text At 128×128 the training\n",
      "data has an IS of 233, and the validation data has an IS of 166.\n",
      "root 3 text At 256×256 the training data has an\n",
      "IS of 377, and the validation data has an IS of 234.\n",
      "root 3 text At 512×512 the training data has an IS of 348,\n",
      "and the validation data has an IS of 241.\n",
      "root 2 text The discrepancy between training and validation scores is\n",
      "due to the Inception classiﬁer having been trained on the training data, resulting in high-conﬁdence\n",
      "outputs that are preferred by the Inception Score.\n",
      "root 2 text \n",
      "\n",
      "23\n",
      "\n",
      "\f",
      "\n",
      "root 5 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX D ADDITIONAL PLOTS\n",
      "\n",
      "\n",
      "root 2 text Figure 17: IS vs. FID at 128×128.\n",
      "root 8 text Scores are averaged across three random seeds.\n",
      "root 2 text \n",
      "\n",
      "Figure 18: IS vs. FID at 256 and 512 pixels.\n",
      "root 8 text Scores are averaged across three random seeds for 256.\n",
      "root 2 text \n",
      "\n",
      "24\n",
      "\n",
      "\f",
      "\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "Figure 19:\n",
      "root 2 text JFT-300M IS vs. FID at 256×256.\n",
      "root 4 text We show truncation values from σ = 0 to σ = 2\n",
      "(top) and from σ = 0.5 to σ = 1.5 (bottom).\n",
      "root 11 text Each curve corresponds to a row in Table 3.\n",
      "root 8 text The\n",
      "curve labeled with baseline corresponds to the ﬁrst row (with orthogonal regularization and other\n",
      "techniques disabled), while the rest correspond to rows 2-4 – the same architecture at different\n",
      "capacities (Ch).\n",
      "root 2 text \n",
      "\n",
      "25\n",
      "\n",
      "5101520253035404550JFT-300M Inception\n",
      "root 2 text Score020406080100120140160180JFT-300M FIDFID vs IS as a function of truncationCh=128Ch=96Ch=64Ch=64 (Baseline)1520253035404550JFT-300M Inception Score1020304050607080JFT-300M FIDFID vs IS as a function of truncationCh=128Ch=96Ch=64Ch=64 (Baseline)\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX E CHOOSING LATENT SPACES\n",
      "\n",
      "\n",
      "root 3 text While most previous work has employed N (0, I) or U[−1, 1] as the prior for z (the noise input to\n",
      "G), we are free to choose any latent distribution from which we can sample.\n",
      "root 7 text We explore the choice of\n",
      "latents by considering an array of possible designs, described below.\n",
      "root 7 text For each latent, we provide the\n",
      "intuition behind its design and brieﬂy describe how it performs when used as a drop-in replacement\n",
      "for z ∼ N (0, I) in an SA-GAN baseline.\n",
      "root 7 text As the Truncation Trick proved more beneﬁcial than\n",
      "switching to any of these latents, we do not perform a full ablation study, and employ z ∼ N (0, I)\n",
      "for our main results to take full advantage of truncation.\n",
      "root 3 text The two latents which we ﬁnd to work\n",
      "best without truncation are Bernoulli {0, 1} and Censored Normal max (N (0, I), 0), both of which\n",
      "improve speed of training and lightly improve ﬁnal performance, but are less amenable to truncation.\n",
      "root 6 text We also ablate the choice of latent space dimensonality (which by default is z ∈ R128), ﬁnding that\n",
      "we are able to successfully train with latent dimensions as low as z ∈ R8, and that with z ∈ R32 we\n",
      "see a minimal drop in performance.\n",
      "root 2 text While this is substantially smaller than many previous works,\n",
      "direct comparison to single-class networks (such as those in Karras et al.\n",
      "root 2 text (2018), which employ\n",
      "a z ∈ R512 latent space on a highly constrained dataset with 30,000 images) is improper, as our\n",
      "networks have additional class information provided as input.\n",
      "root 7 text \n",
      "\n",
      "LATENTS\n",
      "\n",
      "• N (0, I).\n",
      "root 6 text A standard choice of the latent space which we use in the main experiments.\n",
      "root 4 text \n",
      "• U[−1, 1].\n",
      "root 6 text Another standard choice; we ﬁnd that it performs similarly to N (0, I).\n",
      "root 9 text \n",
      "• Bernoulli {0, 1}.\n",
      "root 2 text A discrete latent might reﬂect our prior that underlying factors of variation\n",
      "in natural images are not continuous, but discrete (one feature is present, another is not).\n",
      "root 11 text This latent outperforms N (0, I) (in terms of IS) by 8% and requires 60% fewer iterations.\n",
      "root 6 text \n",
      "• max (N (0, I), 0), also called Censored Normal.\n",
      "root 8 text This latent is designed to introduce spar-\n",
      "sity in the latent space (reﬂecting our prior that certain latent features are sometimes present\n",
      "and sometimes not), but also allow those latents to vary continuously, expressing different\n",
      "degrees of intensity for latents which are active.\n",
      "root 11 text This latent outperforms N (0, I) (in terms\n",
      "of IS) by 15-20% and tends to require fewer iterations.\n",
      "root 8 text This latent is designed to be discrete, but not sparse (as the network\n",
      "can learn to activate in response to negative inputs).\n",
      "root 8 text This latent performs near-identically\n",
      "to N (0, I).\n",
      "root 6 text This distribution is chosen to\n",
      "be discrete and have sparsity, but also to allow latents to take on both positive and negative\n",
      "values.\n",
      "root 8 text This latent performs near-identically to N (0, I).\n",
      "root 2 text \n",
      "\n",
      "• N (0, I) multiplied by Bernoulli {0, 1}.\n",
      "root 6 text This distribution is chosen to have continuous\n",
      "latent factors which are also sparse (with a peak at zero), similar to Censored Normal but\n",
      "not constrained to be positive.\n",
      "root 8 text This latent performs near-identically to N (0, I).\n",
      "root 8 text This is inspired by Chen et al.\n",
      "root 4 text (2016),\n",
      "root 6 text and is chosen to allow some factors of variation to\n",
      "be discrete, while others are continuous.\n",
      "root 11 text This latent outperforms N (0, I) by around 5%.\n",
      "root 6 text \n",
      "• Variance annealing: we sample from N (0, σI), where σ is allowed to vary over training.\n",
      "root 8 text We compared a variety of piecewise schedules and found that starting with σ = 2 and\n",
      "annealing towards σ = 1 over the course of training mildly improved performance.\n",
      "root 7 text The\n",
      "space of possible variance schedules is large, and we did not explore it in depth – we suspect\n",
      "that a more principled or better-tuned schedule could more strongly impact performance.\n",
      "root 8 text \n",
      "• Per-sample variable variance: N (0, σiI), where σi ∼ U[σl, σh] independently for each\n",
      "sample i in a batch, and (σl, σh) are hyperparameters.\n",
      "root 6 text This distribution was chosen to try\n",
      "and improve amenability to the Truncation Trick by feeding the network noise samples with\n",
      "non-constant variance.\n",
      "root 6 text This did not appear to affect performance, but we did not explore it\n",
      "in depth.\n",
      "root 8 text One might also consider scheduling (σl, σh), similar to variance annealing.\n",
      "root 2 text \n",
      "\n",
      "26\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX F MONITORED TRAINING STATISTICS\n",
      "\n",
      "(a) G σ0\n",
      "\n",
      "\n",
      "root 6 text (b) G σ0\n",
      "σ1\n",
      "\n",
      "(c) G σ1\n",
      "\n",
      "(d) G σ2\n",
      "\n",
      "(e) D σ0\n",
      "\n",
      "(f) D σ0\n",
      "σ1\n",
      "\n",
      "(g) D σ1\n",
      "\n",
      "(h) D σ2\n",
      "\n",
      "Figure 20: Training statistics for a typical model without special modiﬁcations.\n",
      "root 6 text Collapse occurs\n",
      "after 200000 iterations.\n",
      "root 9 text \n",
      "\n",
      "27\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "\n",
      "root 11 text Figure 21: G training statistics with σ0 in G regularized towards 1.\n",
      "root 6 text Collapse occurs after 125000\n",
      "iterations.\n",
      "root 11 text \n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "Figure 22: D training statistics with σ0 in G regularized towards 1.\n",
      "root 6 text Collapse occurs after 125000\n",
      "iterations.\n",
      "root 6 text \n",
      "\n",
      "28\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "Figure 23: G training statistics with an R1 Gradient Penalty of strength 10 on D. This model does\n",
      "not collapse, but only reaches a maximum IS of 55.\n",
      "root 2 text \n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "\n",
      "root 2 text (c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "\n",
      "root 8 text Figure 24: D training statistics with an R1 Gradient Penalty of strength 10 on D. This model does\n",
      "not collapse, but only reaches a maximum IS of 55.\n",
      "root 2 text \n",
      "\n",
      "29\n",
      "\n",
      "\f",
      "\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "\n",
      "root 7 text Figure 25: G training statistics with Dropout (keep probability 0.8) applied to the last feature layer\n",
      "of D. This model does not collapse, but only reaches a maximum IS of 70.\n",
      "root 2 text \n",
      "\n",
      "(a) σ0\n",
      "\n",
      "(b) σ0\n",
      "σ1\n",
      "\n",
      "(c) σ1\n",
      "\n",
      "(d) σ2\n",
      "\n",
      "\n",
      "root 6 text Figure 26: D training statistics with Dropout (keep probability 0.8)\n",
      "root 7 text applied to the last feature layer\n",
      "of D. This model does not collapse, but only reaches a maximum IS of 70.\n",
      "root 2 text \n",
      "\n",
      "30\n",
      "\n",
      "\f",
      "\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "(a) G (cid:107)W (cid:107)2\n",
      "\n",
      "(b) D (cid:107)W (cid:107)2\n",
      "\n",
      "(c) losses\n",
      "\n",
      "(d) Variance of all gradient norms in G and D\n",
      "\n",
      "Figure 27: Additional training statistics for a typical model without special modiﬁcations.\n",
      "root 6 text Collapse\n",
      "occurs after 200000 iterations.\n",
      "root 6 text \n",
      "\n",
      "(a) G (cid:107)W (cid:107)2\n",
      "\n",
      "(b) D (cid:107)W (cid:107)2\n",
      "\n",
      "(c) losses\n",
      "\n",
      "(d) Variance of all gradient norms in G and D\n",
      "\n",
      "Figure 28: Additional training statistics with an R1 Gradient Penalty of strength 10 on D. This model\n",
      "does not collapse, but only reaches a maximum IS of 55.\n",
      "root 2 text \n",
      "\n",
      "31\n",
      "\n",
      "\f",
      "\n",
      "root 9 text Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX G ADDITIONAL DISCUSSION:\n",
      "root 9 text STABILITY AND COLLAPSE\n",
      "\n",
      "\n",
      "root 7 text In this section, we present and discuss additional investigations into the stability of our models,\n",
      "expanding upon the discussion in Section 4.\n",
      "root 2 text \n",
      "\n",
      "G.1\n",
      "\n",
      "INTERVENING BEFORE COLLAPSE\n",
      "\n",
      "\n",
      "root 3 text The symptoms of collapse are sharp and sudden, with sample quality dropping from its peak to\n",
      "its lowest value over the course of a few hundred iterations.\n",
      "root 6 text We can detect this collapse when the\n",
      "singular values in G explode, but while the (unnormalized) singular values grow throughout training,\n",
      "there is no consistent threshold at which collapse occurs.\n",
      "root 6 text This raises the question of whether it is\n",
      "possible to prevent or delay collapse by taking a model checkpoint several thousand iterations before\n",
      "collapse, and continuing training with some hyperparameters modiﬁed (e.g., the learning rate).\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 8 text We conducted a range of intervention experiments wherein we took checkpoints of a collapsed\n",
      "model ten or twenty thousand iterations before collapse, changed some aspect of the training setup,\n",
      "then observed whether collapse occurred, when it occurred relative to the original collapse, and the\n",
      "ﬁnal performance attained at collapse.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 5 text We found that increasing the learning rates (relative to their initial values) in either G or D, or both G\n",
      "and D, led to immediate collapse.\n",
      "root 8 text This occurred even when doubling the learning rates from 2 · 10−4\n",
      "in D and 5 · 10−5 in G, to 4 · 10−4 in D and 1 · 10−4 in G, a setting which is not normally unstable\n",
      "when used as the initial learning rates.\n",
      "root 5 text We also tried changing the momentum terms (Adam’s β1\n",
      "and β2), or resetting the momentum vectors to zero, but this tended to either make no difference or,\n",
      "when increasing the momentum, cause immediate collapse.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 5 text We found that decreasing the learning rate in G, but keeping the learning rate in D unchanged could\n",
      "delay collapse (in some cases by over one hundred thousand iterations), but also crippled training—\n",
      "once the learning rate in G was decayed, performance either stayed constant or slowly decayed.\n",
      "root 3 text Conversely, reducing the learning rate in D while keeping G’s learning rate led to immediate collapse.\n",
      "root 11 text We hypothesize that this is because of the need for D to remain optimal throughout training—if its\n",
      "learning rate is reduced, it can no longer “keep up” with G, and training collapses.\n",
      "root 5 text With this in mind,\n",
      "we also tried increasing the number of D steps per G step, but this either had no effect, or delayed\n",
      "collapse at the cost of crippling training (similar to decaying G’s learning rate).\n",
      "root 9 text \n",
      "\n",
      "To further illuminate these dynamics, we construct two additional intervention experiments, one\n",
      "where we freeze G before collapse (by ceasing all parameter updates) and observe whether D remains\n",
      "stable, and the reverse, where we freeze D before collapse and observe whether G remains stable.\n",
      "root 7 text We ﬁnd that when G is frozen, D remains stable, and slowly reduces both components of its loss\n",
      "towards zero.\n",
      "root 7 text However, when D is frozen, G immediately and dramatically collapses, maxing out\n",
      "D’s loss to values upwards of 300, compared to the normal range of 0 to 3.\n",
      "root 6 text \n",
      "\n",
      "This leads to two conclusions: ﬁrst, as has been noted in previous works (Miyato et al., 2018;\n",
      "Gulrajani et al., 2017; Zhang et al., 2018), D must remain optimal with respect to G both for stability\n",
      "and to provide useful gradient information.\n",
      "root 2 text The consequence of G being allowed to win the game is a\n",
      "complete breakdown of the training process, regardless of G’s conditioning or optimization settings.\n",
      "root 2 text Second, favoring D over G (either by training it with a larger learning rate, or for more steps) is\n",
      "insufﬁcient to ensure stability even if D is well-conditioned.\n",
      "root 8 text This suggests either that in practice, an\n",
      "optimal D is necessary but insufﬁcient for training stability, or that some aspect of the system results\n",
      "in D not being trained towards optimality.\n",
      "root 4 text With the latter possibility in mind, we take a closer look\n",
      "at the noise in D’s spectra in the following section.\n",
      "root 2 text \n",
      "\n",
      "32\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "\n",
      "root 6 text G.2 SPIKES IN THE DISCRIMINATOR’S SPECTRA\n",
      "\n",
      "(a) D σ0\n",
      "\n",
      "(b) D σ0\n",
      "σ1\n",
      "\n",
      "Figure 29: A closeup of D’s spectra at a noise spike.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 7 text If some element of D’s training process results in undesirable dynamics, it follows that the behavior\n",
      "of D’s spectra may hold clues as to what that element is.\n",
      "root 6 text The top three singular values of D differ\n",
      "from G’s in that they have a large noise component, tend to grow throughout training but only show\n",
      "a small response to collapse, and the ratio of the ﬁrst two singular values tends to be centered around\n",
      "one, suggesting that the spectra of D have a slow decay.\n",
      "root 8 text When viewed up close (Figure 29), the\n",
      "noise spikes resemble an impulse response: at each spike, the spectra jump upwards, then slowly\n",
      "decrease, with some oscillation.\n",
      "root 2 text \n",
      "\n",
      "One possible explanation is that this behavior is a consequence of D memorizing the training data,\n",
      "as suggested by experiments in Section 4.2.\n",
      "root 8 text As it approaches perfect memorization, it receives\n",
      "less and less signal from real data, as both the original GAN loss and the hinge loss provide zero\n",
      "gradients when D outputs a conﬁdent and correct prediction for a given example.\n",
      "root 6 text If the gradient\n",
      "signal from real data attenuates to zero, this can result in D eventually becoming biased due to\n",
      "exclusively received gradients that encourage its outputs to be negative.\n",
      "root 11 text If this bias passes a certain\n",
      "threshold, D will eventually misclassify a large number of real examples and receive a large gradient\n",
      "encouraging positive outputs, resulting in the observed impulse responses.\n",
      "root 2 text \n",
      "\n",
      "\n",
      "root 8 text This argument suggests several ﬁxes.\n",
      "root 8 text First, one might consider an unbounded loss (such as the\n",
      "Wasserstein loss (Arjovsky et al., 2017)) which would not suffer this gradient attentuation.\n",
      "root 5 text We found\n",
      "that even with gradient penalties and brief re-tuning of optimizer hyperparameters, our models did\n",
      "not stably train for more than a few thousand iterations with this loss.\n",
      "root 8 text We instead explored changing\n",
      "the margin of the hinge loss as a partial compromise: for a given model and minibatch of data,\n",
      "increasing the margin will result in more examples falling within the margin, and thus contributing\n",
      "to the loss.3.\n",
      "root 7 text Training with a smaller margin (by a factor of 2) measurably reduces performance,\n",
      "but training with a larger margin (by up to a factor of 3) does not prevent collapse or reduce the\n",
      "noise in D’s spectra.\n",
      "root 10 text Increasing the margin beyond 3 results in unstable training similar to using\n",
      "the Wasserstein loss.\n",
      "root 7 text Finally, the memorization argument might suggest that using a smaller D or\n",
      "using dropout in D would improve training by reducing its capacity to memorize, but in practice this\n",
      "degrades training.\n",
      "root 5 text \n",
      "\n",
      "3Unconstrained models could easily learn a different output scale to account for this margin, but the use of\n",
      "\n",
      "Spectral Normalization constrains our models and makes the speciﬁc selection of the margin meaningful.\n",
      "root 2 text \n",
      "\n",
      "33\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX H NEGATIVE RESULTS\n",
      "\n",
      "\n",
      "root 8 text We explored a range of novel and existing techniques which ended up degrading or otherwise not\n",
      "affecting performance in our setting.\n",
      "root 3 text We report them here; our evaluations for this section are not as\n",
      "thorough as those for the main architectural choices.\n",
      "root 2 text \n",
      "\n",
      "Our intention in reporting these results is to save time for future work, and to give a more complete\n",
      "picture of our attempts to improve performance or stability.\n",
      "root 4 text We note, however, that these results\n",
      "must be understood to be speciﬁc to the particular setup we used.\n",
      "root 2 text A pitfall of reporting negative\n",
      "results is that one might report that a particular technique doesn’t work, when the reality is that this\n",
      "technique did not have the desired effect when applied in a particular way to a particular problem.\n",
      "root 5 text Drawing overly general conclusions might close off potentially fruitful avenues of research.\n",
      "root 5 text \n",
      "\n",
      "• We found that doubling the depth (by inserting an additional Residual block after every up-\n",
      "\n",
      "or down-sampling block) hampered performance.\n",
      "root 12 text We experimented with sharing class embeddings between both G and D (as opposed to just\n",
      "within G).\n",
      "root 12 text This is accomplished by replacing D’s class embedding with a projection from\n",
      "G’s embeddings, as is done in G’s BatchNorm layers.\n",
      "root 6 text In our initial experiments this seemed\n",
      "to help and accelerate training, but we found this trick scaled poorly and was sensitive to\n",
      "optimization hyperparameters, particularly the choice of number of D steps per G step.\n",
      "root 5 text \n",
      "• We tried replacing BatchNorm in G with WeightNorm (Salimans & Kingma, 2016), but\n",
      "this crippled training.\n",
      "root 5 text We also tried removing BatchNorm and only having Spectral Nor-\n",
      "malization, but this also crippled training.\n",
      "root 5 text \n",
      "\n",
      "• We tried adding BatchNorm to D (both class-conditional and unconditional) in addition to\n",
      "\n",
      "Spectral Normalization, but this crippled training.\n",
      "root 5 text \n",
      "\n",
      "• We tried varying the choice of location of the attention block in G and D (and inserting\n",
      "multiple attention blocks at different resolutions) but found that at 128×128 there was no\n",
      "noticeable beneﬁt to doing so, and compute and memory costs increased substantially.\n",
      "root 5 text We\n",
      "found a beneﬁt to moving the attention block up one stage when moving to 256×256,\n",
      "which is in line with our expectations given the increased resolution.\n",
      "root 5 text \n",
      "\n",
      "• We tried using ﬁlter sizes of 5 or 7 instead of 3 in either G or D or both.\n",
      "root 5 text We found that\n",
      "having a ﬁlter size of 5 in G only provided a small improvement over the baseline but came\n",
      "at an unjustiﬁable compute cost.\n",
      "root 8 text All other settings degraded performance.\n",
      "root 5 text \n",
      "\n",
      "• We tried varying the dilation for convolutional ﬁlters in both G and D at 128×128, but found\n",
      "\n",
      "that even a small amount of dilation in either network degraded performance.\n",
      "root 5 text We tried bilinear upsampling in G in place of nearest-neighbors upsampling, but this de-\n",
      "\n",
      "graded performance.\n",
      "root 8 text \n",
      "\n",
      "• In some of our models, we observed class-conditional mode collapse, where the model\n",
      "would only output one or two samples for a subset of classes but was still able to generate\n",
      "samples for all other classes.\n",
      "root 7 text We noticed that the collapsed classes had embedings which\n",
      "had become very large relative to the other embeddings, and attempted to ameliorate this\n",
      "issue by applying weight decay to the shared embedding only.\n",
      "root 5 text We found that small amounts\n",
      "of weight decay (10−6) instead degraded performance, and that only even smaller values\n",
      "(10−8) did not degrade performance, but these values were also too small to prevent the\n",
      "class vectors from exploding.\n",
      "root 6 text Higher-resolution models appear to be more resilient to this\n",
      "problem, and none of our ﬁnal models appear to suffer from this type of collapse.\n",
      "root 7 text We also exper-\n",
      "imented with Spectrally Normalizing these MLPs, and with providing these (and the linear\n",
      "projections) with a bias at their output, but did not notice any beneﬁt.\n",
      "root 5 text \n",
      "\n",
      "• We tried gradient norm clipping (both the global variant typically used in recurrent net-\n",
      "works, and a local version where the clipping value is determined on a per-parameter basis)\n",
      "but found this did not alleviate instability.\n",
      "root 15 text \n",
      "\n",
      "34\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2019\n",
      "\n",
      "APPENDIX I HYPERPARAMETERS\n",
      "\n",
      "\n",
      "root 6 text We performed various hyperparameter sweeps in this work:\n",
      "\n",
      "• We swept the Cartesian product of the learning rates for each network through [10−5,\n",
      "5 · 10−5, 10−4, 2 · 10−4, 4 · 10−4, 8 · 10−4, 10−3], and initially found that the SA-GAN\n",
      "settings (G’s learning rate 10−4, D’s learning rate 4 · 10−4) were optimal at lower batch\n",
      "sizes; we did not repeat this sweep at higher batch sizes but did try halving and doubling\n",
      "the learning rate, arriving at the halved settings used for our experiments.\n",
      "root 5 text We swept the R1 gradient penalty strength through [10−3, 10−2, 10−1, 0.5, 1, 2, 3, 5, 10].\n",
      "root 10 text We ﬁnd that the strength of the penalty correlates negatively with performance, but that\n",
      "settings above 0.5 impart training stability.\n",
      "root 5 text We swept the keep probabilities for DropOut in the ﬁnal layer of D through [0.5, 0.6, 0.7,\n",
      "0.8, 0.9, 0.95].\n",
      "root 2 text We ﬁnd that DropOut has a similar stabilizing effect to R1 but also degrades\n",
      "performance.\n",
      "root 5 text We swept D’s Adam β1 parameter through [0.1, 0.2, 0.3, 0.4, 0.5] and found it to have\n",
      "a light regularization effect similar to DropOut, but not to signiﬁcantly improve results.\n",
      "root 5 text \n",
      "Higher β1 terms in either network crippled training.\n",
      "root 5 text We swept the strength of the modiﬁed Orthogonal Regularization penalty in G through\n",
      "\n",
      "[10−5, 5 · 10−5, 10−4, 5 · 10−4, 10−3, 10−2], and selected 10−4.\n",
      "root 2 text \n",
      "\n",
      "35\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    if len(sent.root.text) > 1:\n",
    "        print('root', len(sent.root.text), 'text', sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from parsr_client import ParsrClient\n",
    "parsr = ParsrClient('localhost:3001')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': './1706.03762.pdf',\n",
       " 'config': './config.json',\n",
       " 'status_code': 202,\n",
       " 'server_response': '32ea087a6c86ccfad89d6efe96fd4e'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsr.send_document(file_path='./1706.03762.pdf', config_path='./config.json', save_request_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
