[{"paperId": "e8d330f11df9c69f38b78a7cc4b1333ebecf7c55", "url": "https://www.semanticscholar.org/paper/e8d330f11df9c69f38b78a7cc4b1333ebecf7c55", "title": "Ethical Machine Learning in Health Care", "abstract": "The use of machine learning (ML) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of healthcare. Specifically, we frame ethics of ML in healthcare through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to postdeployment considerations. We close by summarizing recommendations to address these challenges.", "year": 2021, "authors": [{"authorId": "34574044", "name": "I. Chen"}, {"authorId": "145192191", "name": "E. Pierson"}, {"authorId": "48345067", "name": "Sherri Rose"}, {"authorId": "34287745", "name": "Shalmali Joshi"}, {"authorId": "6745873", "name": "Kadija Ferryman"}, {"authorId": "2804918", "name": "M. Ghassemi"}], "cluster": 2, "position": {"x": -165.43104553222656, "y": -30.7840576171875}}, {"paperId": "e7bf950be4cf8309d0df68d18fa09e77f5b2511a", "url": "https://www.semanticscholar.org/paper/e7bf950be4cf8309d0df68d18fa09e77f5b2511a", "title": "De-identification of patient notes with recurrent neural networks", "abstract": "Objective\nPatient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of electronic health record databases, the limited number of researchers with access to non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.\n\n\nMaterials and Methods\nWe introduce the first de-identification system based on artificial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the MIMIC de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.\n\n\nResults\nOur ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall of 97.38 and a precision of 98.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with a recall of 99.25 and a precision of 99.21.\n\n\nConclusion\nOur findings support the use of ANNs for de-identification of patient notes, as they show better performance than previously published systems while requiring no manual feature engineering.", "year": 2017, "authors": [{"authorId": "2462276", "name": "Franck Dernoncourt"}, {"authorId": "2108574824", "name": "J. Y. Lee"}, {"authorId": "1723337", "name": "\u00d6zlem Uzuner"}, {"authorId": "1679873", "name": "Peter Szolovits"}], "cluster": 8, "position": {"x": -53.93682098388672, "y": 12.194001197814941}}, {"paperId": "4fd75d18b231f73e9c878cb6029900e4b0e327bb", "url": "https://www.semanticscholar.org/paper/4fd75d18b231f73e9c878cb6029900e4b0e327bb", "title": "Intimate Partner Violence and Injury Prediction From Radiology Reports", "abstract": "Intimate partner violence (IPV) is an urgent, prevalent, and under-detected public health issue. We present machine learning models to assess patients for IPV and injury. We train the predictive algorithms on radiology reports with 1) IPV labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship-trained physicians. Our dataset includes 34,642 radiology reports and 1479 patients of IPV victims and control patients. Our best model predicts IPV a median of 3.08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95%. We conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model.", "year": 2021, "authors": [{"authorId": "2056276811", "name": "Irene Chen"}, {"authorId": "50229020", "name": "Emily Alsentzer"}, {"authorId": "46904371", "name": "Hyesun Park"}, {"authorId": "2110842212", "name": "Richard Thomas"}, {"authorId": "4000824", "name": "Babina Gosangi"}, {"authorId": "6604731", "name": "Rahul Gujrathi"}, {"authorId": "5559176", "name": "B. Khurana"}], "cluster": 2, "position": {"x": -150.31600952148438, "y": -30.20763397216797}}, {"paperId": "e21ab639b0308a71aeac643a12f59a32603d091d", "url": "https://www.semanticscholar.org/paper/e21ab639b0308a71aeac643a12f59a32603d091d", "title": "UPSTAGE: Unsupervised Context Augmentation for Utterance Classification in Patient-Provider Communication", "abstract": "Conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving. When analyzing utterances in such conversations, it is not sufficient to consider each sentence in isolation, since its context may play a role in determining its semantic meaning. Recently, contextual information in natural language documents has been modeled using various techniques, such as recurrent neural networks with latent variables, or neural networks with attention mechanisms. In this paper, we present UnsuPerviSed conText AuGmEntation (Upstage), a classification framework that relies on both local and global contextual information from different sources. Upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient-provider conversations. In addition, Upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context, which leads to improved classification performance.", "year": 2020, "authors": [{"authorId": "2028953652", "name": "D. Min"}, {"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}, {"authorId": "11976579", "name": "S. Kuo"}, {"authorId": "5492986", "name": "W. Herman"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}], "cluster": 3, "position": {"x": -167.05882263183594, "y": 4.264462947845459}}, {"paperId": "defeaddec92b53fc2e9787cc3652e3d8be812193", "url": "https://www.semanticscholar.org/paper/defeaddec92b53fc2e9787cc3652e3d8be812193", "title": "Segment convolutional neural networks (Seg-CNNs) for classifying relations in clinical notes", "abstract": "We propose Segment Convolutional Neural Networks (Seg-CNNs) for classifying relations from clinical notes. Seg-CNNs use only word-embedding features without manual feature engineering. Unlike typical CNN models, relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence: preceding, concept1, middle, concept2, and succeeding. We evaluate Seg-CNN on the i2b2/VA relation classification challenge dataset. We show that Seg-CNN achieves a state-of-the-art micro-average F-measure of 0.742 for overall evaluation, 0.686 for classifying medical problem-treatment relations, 0.820 for medical problem-test relations, and 0.702 for medical problem-medical problem relations. We demonstrate the benefits of learning segment-level representations. We show that medical domain word embeddings help improve relation classification. Seg-CNNs can be trained quickly for the i2b2/VA dataset on a graphics processing unit (GPU) platform. These results support the use of CNNs computed over segments of text for classifying medical relations, as they show state-of-the-art performance while requiring no manual feature engineering.", "year": 2018, "authors": [{"authorId": "1683396", "name": "Yuan Luo"}, {"authorId": "145215470", "name": "Yu Cheng"}, {"authorId": "1723337", "name": "\u00d6zlem Uzuner"}, {"authorId": "1679873", "name": "Peter Szolovits"}, {"authorId": "1690334", "name": "J. Starren"}], "cluster": 3, "position": {"x": -161.20277404785156, "y": 2.918689489364624}}, {"paperId": "4554bc45be85cca73e3f94220c8cb056cf6ebf74", "url": "https://www.semanticscholar.org/paper/4554bc45be85cca73e3f94220c8cb056cf6ebf74", "title": "Fast, Structured Clinical Documentation via Contextual Autocomplete", "abstract": "We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.", "year": 2020, "authors": [{"authorId": "39921534", "name": "Divya Gopinath"}, {"authorId": "2056898702", "name": "Monica Agrawal"}, {"authorId": "2056702606", "name": "Luke S. Murray"}, {"authorId": "2986398", "name": "S. Horng"}, {"authorId": "1743286", "name": "D. Karger"}, {"authorId": "1746662", "name": "D. Sontag"}], "cluster": 3, "position": {"x": -156.4526824951172, "y": -1.3295739889144897}}, {"paperId": "d77719ba4be4196c9a918ce8dff0edf5aa3c04e7", "url": "https://www.semanticscholar.org/paper/d77719ba4be4196c9a918ce8dff0edf5aa3c04e7", "title": "A Review of Challenges and Opportunities in Machine Learning for Health.", "abstract": "Modern electronic health records (EHRs) provide data to answer clinically meaningful questions. The growing data in EHRs makes healthcare ripe for the use of machine learning. However, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. For example, diseases in EHRs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.", "year": 2020, "authors": [{"authorId": "2804918", "name": "M. Ghassemi"}, {"authorId": "2113888405", "name": "Tristan Naumann"}, {"authorId": "145610328", "name": "Peter F. Schulam"}, {"authorId": "1507094362", "name": "A. Beam"}, {"authorId": "34574044", "name": "I. Chen"}, {"authorId": "2615814", "name": "R. Ranganath"}], "cluster": 2, "position": {"x": -158.25836181640625, "y": -26.366497039794922}}, {"paperId": "4a10dffca6dcce9c570cb75aa4d76522c34a2fd4", "url": "https://www.semanticscholar.org/paper/4a10dffca6dcce9c570cb75aa4d76522c34a2fd4", "title": "CORD-19: The COVID-19 Open Research Dataset", "abstract": "The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.", "year": 2020, "authors": [{"authorId": "31860505", "name": "Lucy Lu Wang"}, {"authorId": "46258841", "name": "Kyle Lo"}, {"authorId": "1648642525", "name": "Yoganand Chandrasekhar"}, {"authorId": "65983884", "name": "Russell Reas"}, {"authorId": "82148460", "name": "Jiangjiang Yang"}, {"authorId": "40329918", "name": "Darrin Eide"}, {"authorId": "37996742", "name": "Kathryn Funk"}, {"authorId": "143967880", "name": "Rodney Michael Kinney"}, {"authorId": "51427852", "name": "Ziyang Liu"}, {"authorId": "153908924", "name": "William Merrill"}, {"authorId": "115392299", "name": "P. Mooney"}, {"authorId": "69437054", "name": "D. Murdick"}, {"authorId": "1453742562", "name": "Devvret Rishi"}, {"authorId": "2055678827", "name": "J. Sheehan"}, {"authorId": "3303634", "name": "Zhihong Shen"}, {"authorId": "1405473759", "name": "Brandon Stilson"}, {"authorId": "1860983", "name": "Alex D Wade"}, {"authorId": "1748169", "name": "Kuansan Wang"}, {"authorId": "46212260", "name": "Christopher Wilhelm"}, {"authorId": "2064542611", "name": "Boya Xie"}, {"authorId": "21811471", "name": "Douglas A. Raymond"}, {"authorId": "1780531", "name": "Daniel S. Weld"}, {"authorId": "1741101", "name": "Oren Etzioni"}, {"authorId": "41018147", "name": "Sebastian Kohlmeier"}], "cluster": 7, "position": {"x": 16.438703536987305, "y": 68.95325469970703}}, {"paperId": "3f8d4444cd124e21d4fa47b514c8267a3d5d9649", "url": "https://www.semanticscholar.org/paper/3f8d4444cd124e21d4fa47b514c8267a3d5d9649", "title": "Can AI Help Reduce Disparities in General Medical and Mental Health Care?", "abstract": "Background\nAs machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems' data, algorithms, and recommendations. Simply put, as health care improves for some, it might not improve for all.\n\n\nMethods\nTwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status.\n\n\nResults\nClinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30-day readmission.\n\n\nConclusions\nThis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.", "year": 2019, "authors": [{"authorId": "34574044", "name": "I. Chen"}, {"authorId": "1679873", "name": "Peter Szolovits"}, {"authorId": "2804918", "name": "M. Ghassemi"}], "cluster": 2, "position": {"x": -159.62269592285156, "y": -32.48564910888672}}, {"paperId": "2d40f76055a7c28dd30c8b8f60c31724f8991cd8", "url": "https://www.semanticscholar.org/paper/2d40f76055a7c28dd30c8b8f60c31724f8991cd8", "title": "The Ivory Tower Lost: How College Students Respond Differently than the General Public to the COVID-19 Pandemic", "abstract": "In the United States, the country with the highest confirmed COVID-19 infection cases, a nationwide social distancing protocol has been implemented by the President. Following the closure of the University of Washington on March 7th, more than 1000 colleges and universities in the United States have cancelled in-person classes and campus activities, impacting millions of students. This paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people's opinions on social media. We discover several topics embedded in a large number of COVID-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. Moreover, we find significant differences between these two groups of Twitter users with respect to the sentiments they expressed towards the COVID-19 issues. To our best knowledge, this is the first social media-based study which focuses on the college student community's demographics and responses to prevalent social issues during a major crisis.", "year": 2020, "authors": [{"authorId": "1840946", "name": "Viet-An Duong"}, {"authorId": "144270183", "name": "Phu Pham"}, {"authorId": "9572971", "name": "Tongyu Yang"}, {"authorId": null, "name": "Yu Wang"}, {"authorId": "33642939", "name": "Jiebo Luo"}], "cluster": 10, "position": {"x": -6.309895992279053, "y": -34.68769073486328}}, {"paperId": "462cc2046ef4d48d844813b66d8a1ed6dfda3bc0", "url": "https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0", "title": "HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks", "abstract": "Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.", "year": 2021, "authors": [{"authorId": "37784060", "name": "Firoj Alam"}, {"authorId": "1738611962", "name": "Umair Qazi"}, {"authorId": "151491159", "name": "Muhammad Imran"}, {"authorId": "48046557", "name": "Ferda Ofli"}], "cluster": 9, "position": {"x": -55.92226028442383, "y": -19.97994613647461}}, {"paperId": "2a29f7cc10fc18ce6e031e0cf6fc307a6b85d8e9", "url": "https://www.semanticscholar.org/paper/2a29f7cc10fc18ce6e031e0cf6fc307a6b85d8e9", "title": "CrisisMMD: Multimodal Twitter Datasets from Natural Disasters", "abstract": "During natural and man-made disasters, people use social media platforms such as Twitter to post textual and multime- dia content to report updates about injured or dead people, infrastructure damage, and missing or found people among other information types. Studies have revealed that this on- line information, if processed timely and effectively, is ex- tremely useful for humanitarian organizations to gain situational awareness and plan relief operations. In addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. Despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. One of the reasons is the lack of labeled imagery data in this domain. Therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset collected from Twitter during different natural disasters. We provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.", "year": 2018, "authors": [{"authorId": "37784060", "name": "Firoj Alam"}, {"authorId": "1727159", "name": "Ferda Ofli"}, {"authorId": "32584303", "name": "Muhammad Imran"}], "cluster": 9, "position": {"x": -50.22290802001953, "y": -20.40726089477539}}, {"paperId": "561ede166947a8bedb8be9acff182913156e06c6", "url": "https://www.semanticscholar.org/paper/561ede166947a8bedb8be9acff182913156e06c6", "title": "Domain Adaptation with Adversarial Training and Graph Embeddings", "abstract": "The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.", "year": 2018, "authors": [{"authorId": "37784060", "name": "Firoj Alam"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}, {"authorId": "32584303", "name": "Muhammad Imran"}], "cluster": 9, "position": {"x": -62.72995376586914, "y": -19.77484893798828}}, {"paperId": "579914dac924d6b4a17889ea7b0dd744a62bbd74", "url": "https://www.semanticscholar.org/paper/579914dac924d6b4a17889ea7b0dd744a62bbd74", "title": "IBC-C : A Dataset for Armed Conflict Event Analysis", "abstract": "We describe the Iraq Body Count Corpus (IBC-C) dataset, the first substantial armed conflict-related dataset which can be used for conflict analysis. IBC-C provides a ground-truth dataset for conflict specific named entity recognition, slot filling, and event de-duplication. IBC-C is constructed using data collected by the Iraq Body Count project which has been recording casualties resulting from the ongoing war in Iraq since 2003. We describe the dataset\u2019s creation, how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using Hidden Markov Models, Conditional Random Fields, and Recursive Neural Networks.", "year": 2016, "authors": [{"authorId": "1400416026", "name": "Andrej \u017dukov-Gregori\u010d"}, {"authorId": "41209732", "name": "Bartal Veyhe"}, {"authorId": "2114937923", "name": "Zhiyuan Luo"}], "cluster": 0, "position": {"x": -51.54615020751953, "y": -140.7416229248047}}, {"paperId": "42c63d952f0cce7f89738e818fc9ab4e723bf7b6", "url": "https://www.semanticscholar.org/paper/42c63d952f0cce7f89738e818fc9ab4e723bf7b6", "title": "One-to-X analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts", "abstract": "We extend the well-known word analogy task to a one-to-X formulation, including one-to-none cases, when no correct answer exists. The task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type `location:armed-group' based on data about past events. As the source of semantic information, we use diachronic word embedding models trained on English news texts. A simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. Finally, we publish a ready-to-use test set for one-to-X analogy evaluation on historical armed conflicts data.", "year": 2019, "authors": [{"authorId": "2689095", "name": "Andrey Kutuzov"}, {"authorId": "2027091", "name": "Erik Velldal"}, {"authorId": "2732223", "name": "Lilja \u00d8vrelid"}], "cluster": 0, "position": {"x": -40.90468978881836, "y": -140.90940856933594}}, {"paperId": "3004f78d84752f3ffe8e54fa265b2ee85dac9728", "url": "https://www.semanticscholar.org/paper/3004f78d84752f3ffe8e54fa265b2ee85dac9728", "title": "Text as Data for Conflict Research: A Literature Survey", "abstract": "Computer-aided text analysis (CATA) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. The chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. This includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). Particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. Both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. Finally, cross-validation is highlighted as a crucial step in CATA, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.", "year": 2019, "authors": [{"authorId": "9040191", "name": "Seraphine F. Maerz"}, {"authorId": "70224648", "name": "C. Puschmann"}], "cluster": 12, "position": {"x": 77.01834869384766, "y": -36.389915466308594}}, {"paperId": "0eb5872733e643f43a0c1a7ff78953dfea74dfea", "url": "https://www.semanticscholar.org/paper/0eb5872733e643f43a0c1a7ff78953dfea74dfea", "title": "Automated Scoring: Beyond Natural Language Processing", "abstract": "In this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. Automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. Such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. Our position is that it is essential for us as NLP researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.", "year": 2018, "authors": [{"authorId": "1723404", "name": "Nitin Madnani"}, {"authorId": "145557710", "name": "A. Cahill"}], "cluster": 5, "position": {"x": -79.36921691894531, "y": 59.03699493408203}}, {"paperId": "aea14f23a951975f605a981d003386e46bf8acfe", "url": "https://www.semanticscholar.org/paper/aea14f23a951975f605a981d003386e46bf8acfe", "title": "A Neural Approach to Automated Essay Scoring", "abstract": "Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering.", "year": 2016, "authors": [{"authorId": "2916420", "name": "K. Taghipour"}, {"authorId": "34789794", "name": "H. Ng"}], "cluster": 5, "position": {"x": -74.42517852783203, "y": 56.28812789916992}}, {"paperId": "636d4c0b0fe6919abe6eb546907d28ed39bf56e6", "url": "https://www.semanticscholar.org/paper/636d4c0b0fe6919abe6eb546907d28ed39bf56e6", "title": "Using Natural Language Processing for Automatic Detection of Plagiarism", "abstract": "Current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level. In this study the aim is to improve the accuracy of plagiarism detection by incorporating Natural Language Processing (NLP) techniques into existing approaches. We propose a framework for external plagiarism detection in which a number of NLP techniques are applied to process a set of suspicious and original documents, not only to analyse strings but also the structure of the text, using resources to account for text relations. Initial results obtained with a corpus of plagiarised short paragraphs have showed that NLP techniques improve the accuracy of existing approaches.", "year": 2010, "authors": [{"authorId": "2004654", "name": "Miranda Chong"}, {"authorId": "1702974", "name": "Lucia Specia"}, {"authorId": "1746371", "name": "R. Mitkov"}], "cluster": 5, "position": {"x": -87.3418197631836, "y": 70.40060424804688}}, {"paperId": "2ca0403eabc3893fed255fc119a927d83a1af739", "url": "https://www.semanticscholar.org/paper/2ca0403eabc3893fed255fc119a927d83a1af739", "title": "Tracing armed conflicts with diachronic word embedding models", "abstract": "Recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. In this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data. The results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models. At the same time, we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set.", "year": 2017, "authors": [{"authorId": "2689095", "name": "Andrey Kutuzov"}, {"authorId": "2027091", "name": "Erik Velldal"}, {"authorId": "2732223", "name": "Lilja \u00d8vrelid"}], "cluster": 0, "position": {"x": -44.963623046875, "y": -137.51626586914062}}, {"paperId": "0cdeb238355617a640471987af36d8e09cd905c0", "url": "https://www.semanticscholar.org/paper/0cdeb238355617a640471987af36d8e09cd905c0", "title": "Event Data on Armed Conflict and Security: New Perspectives, Old Challenges, and Some Solutions", "abstract": "This article presents the Event Data on Conflict and Security (EDACS) dataset, discusses the inherent problems of georeferenced conflict data, and shows how these challenges are met within EDACS. Based on an event data approach, EDACS contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation. However, the unreflected use of any of these datasets will give researchers unjustified confidence in their findings, as the pitfalls are many and propagating errors can result in misleading conclusions. To identify and handle the different challenges to overall event data quality, we argue in favor of transparency in the data collection and coding process, to empower analysts to challenge the data and avoid cascading errors. In particular, we investigate how the choice of news sources, the handling of geographic precision, and the use of auxiliary data can bias event data. We demonstrate how the EDACS dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources, possible sources of bias, and detailed information on geographic precision. This allows for a flexible use of the data based on individual analytical requirements.", "year": 2012, "authors": [{"authorId": "52098235", "name": "S. Chojnacki"}, {"authorId": "69960834", "name": "Christian Ickler"}, {"authorId": "144839872", "name": "Michael Spies"}, {"authorId": "119191332", "name": "J. Wiesel"}], "cluster": 0, "position": {"x": -48.41514587402344, "y": -130.3275146484375}}, {"paperId": "4ed75f35ae7343cd906f311390f572a58e36805c", "url": "https://www.semanticscholar.org/paper/4ed75f35ae7343cd906f311390f572a58e36805c", "title": "Enriching textbooks through data mining", "abstract": "Textbooks play an important role in any educational system. Unfortunately, many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts. We propose a technological solution to address this problem based on enriching textbooks with authoritative web content. We augment textbooks at the section level for key concepts discussed in the section. We use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation. Our evaluation, employing textbooks from India, shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques.", "year": 2010, "authors": [{"authorId": "144947410", "name": "R. Agrawal"}, {"authorId": "144979147", "name": "Sreenivas Gollapudi"}, {"authorId": "1769861", "name": "K. Kenthapadi"}, {"authorId": "2897313", "name": "Nitish Srivastava"}, {"authorId": "13027331", "name": "R. Velu"}], "cluster": 5, "position": {"x": -112.33484649658203, "y": 42.89316940307617}}, {"paperId": "bc6a4304afdf59783638a6a0d9fc2c2acb6e5b67", "url": "https://www.semanticscholar.org/paper/bc6a4304afdf59783638a6a0d9fc2c2acb6e5b67", "title": "Educational Question Answering Motivated by Question-Specific Concept Maps", "abstract": "Question answering (QA) is the automated process of answering general questions submitted by humans in natural language. QA has previously been explored within the educational context to facilitate learning, however the majority of works have focused on text-based answering. As an alternative, this paper proposes an approach to return answers as a concept map, which further encourages meaningful learning and knowledge organisation. Additionally, this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit. A randomised experiment was conducted with a sample of 59 Computer Science undergraduates, obtaining statistically significant results on learning gain when students are provided with the question-specific concept maps. Further, time spent on studying the concept maps were positively correlated with the learning gain.", "year": 2015, "authors": [{"authorId": "2469250", "name": "Thushari Atapattu"}, {"authorId": "1679867", "name": "K. Falkner"}, {"authorId": "1807135", "name": "Nickolas J. G. Falkner"}], "cluster": 5, "position": {"x": -102.44075012207031, "y": 53.484764099121094}}, {"paperId": "e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a", "url": "https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a", "title": "Question answering system on education acts using NLP techniques", "abstract": "Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.", "year": 2016, "authors": [{"authorId": "40649724", "name": "Sweta P. Lende"}, {"authorId": "1714912", "name": "M. Raghuwanshi"}], "cluster": 5, "position": {"x": -97.29777526855469, "y": 56.35870361328125}}, {"paperId": "d114af5d3dcb3792bd9faec1476feba5aaf3617f", "url": "https://www.semanticscholar.org/paper/d114af5d3dcb3792bd9faec1476feba5aaf3617f", "title": "A Multimodal Human-Computer Interaction System and Its Application in Smart Learning Environments", "abstract": "A multimodal human-computer interaction system is composed of the comprehensive usage of various input and output channels. For the information input, apart from the traditional keyboard typing, mouse clicking, screen touching, the latest speech and face recognition technology can be used. For the output, the traditional screen display, the latest speech and facial expression synthesis and gesture generation can be used. After literature review of related works, this paper at first presents such a system, MMISE (Multimodal Interaction System for Education), about its architecture and working mechanism, POOOIIM (Pedagogical Objective Oriented Output, Input and Implementation Mechanism) illustrated with practical examples. Then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020.", "year": 2020, "authors": [{"authorId": "40111240", "name": "Jiyou Jia"}, {"authorId": "1820790357", "name": "Yunfan He"}, {"authorId": "11019966", "name": "Huixiao Le"}], "cluster": 5, "position": {"x": -119.3096923828125, "y": 55.51091384887695}}, {"paperId": "311381feeb6346bfcb2ba622bd8f713261a4075d", "url": "https://www.semanticscholar.org/paper/311381feeb6346bfcb2ba622bd8f713261a4075d", "title": "Modeling the Relationship between User Comments and Edits in Document Revision", "abstract": "Management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document\u2019s evolution. Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. A number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. Thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: Comment Ranking and Edit Anchoring. We begin by collecting a dataset with more than half a million comment-edit pairs based on Wikipedia revision histories. We then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. In a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. We are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for Comment Ranking, while we achieve 74.4% accuracy on Edit Anchoring.", "year": 2019, "authors": [{"authorId": "2048981220", "name": "Xuchao Zhang"}, {"authorId": "1801149", "name": "Dheeraj Rajagopal"}, {"authorId": "2417334", "name": "Michael Gamon"}, {"authorId": "3001990", "name": "Sujay Kumar Jauhar"}, {"authorId": "1752590", "name": "Chang-Tien Lu"}], "cluster": -1, "position": {"x": -121.93009185791016, "y": -44.64213180541992}}, {"paperId": "911e61212eb26325d3fe58f454f36f4e70c54c7a", "url": "https://www.semanticscholar.org/paper/911e61212eb26325d3fe58f454f36f4e70c54c7a", "title": "Characterizing Stage-aware Writing Assistance for Collaborative Document Authoring", "abstract": "Writing is a complex non-linear process that begins with a mental model of intent, and progresses through an outline of ideas, to words on paper (and their subsequent refinement). Despite past research in understanding writing, Web-scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution, providing opportune assistance based on authors' situated actions and context. In this paper, we present three studies that explore temporal stages of document authoring. We first survey information workers at a large technology company about their writing habits and preferences, concluding that writers do in fact conceptually progress through several distinct phases while authoring documents. We also explore, qualitatively, how writing stages are linked to document lifespan. We supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents. Finally, as a first step towards facilitating an intelligent digital writing assistant, we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document. Our results support the benefit of tools tailored to writing stages, identify primary tasks associated with these stages, and show that it is possible to predict stages from anonymous interaction logs. Together, these results argue for the benefit and feasibility of more tailored digital writing assistance.", "year": 2020, "authors": [{"authorId": "2264984", "name": "Bahareh Sarrafzadeh"}, {"authorId": "3001990", "name": "Sujay Kumar Jauhar"}, {"authorId": "2417334", "name": "Michael Gamon"}, {"authorId": "144502831", "name": "E. Lank"}, {"authorId": "34286525", "name": "Ryen W. White"}], "cluster": -1, "position": {"x": -119.17134094238281, "y": -49.10036087036133}}, {"paperId": "065332576fd9b264a5a338549239bb39e6733819", "url": "https://www.semanticscholar.org/paper/065332576fd9b264a5a338549239bb39e6733819", "title": "Inferring Social Media Users\u2019 Mental Health Status from Multimodal Information", "abstract": "Worldwide, an increasing number of people are suffering from mental health disorders such as depression and anxiety. In the United States alone, one in every four adults suffers from a mental health condition, which makes mental health a pressing concern. In this paper, we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status. Specifically, we focus on identifying social media activity that either indicates a mental health condition or its onset. We collect posts from Flickr and apply a multimodal approach that consists of jointly analyzing language, visual, and metadata cues and their relation to mental health. We conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness. Our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time, and can provide important cues into a user\u2019s mental status.", "year": 2020, "authors": [{"authorId": "2898992", "name": "Z. Xu"}, {"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}], "cluster": 9, "position": {"x": -41.990203857421875, "y": -21.62924575805664}}, {"paperId": "f8868be52c8c0d64920e5f67870c01ba48601608", "url": "https://www.semanticscholar.org/paper/f8868be52c8c0d64920e5f67870c01ba48601608", "title": "Natural Language Processing and Language Learning", "abstract": "As a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago, Natural Language Processing (NLP) is concerned with the automated processing of human language. It addresses the analysis and generation of written and spoken language, though speech processing is often regarded as a separate subfield. NLP emphasizes processing and applications and as such can be seen as the applied side of Computational Linguistics, the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of Linguistics, Computer Science, and Psychology. In terms of the language aspects dealt with in NLP, traditionally lexical, morphological and syntactic aspects of language were at the center of attention, but aspects of meaning, discourse, and the relation to the extra-linguistic context have become increasingly prominent in the last decade. A good introduction and overview of the field is provided in Jurafsky & Martin (2009).", "year": 2012, "authors": [{"authorId": "46191146", "name": "C. Chapelle"}, {"authorId": "2080321132", "name": "Blackwell"}], "cluster": 5, "position": {"x": -86.17549133300781, "y": 62.71332550048828}}, {"paperId": "81d13af7eb78725492854bb9860b57af2b2a1778", "url": "https://www.semanticscholar.org/paper/81d13af7eb78725492854bb9860b57af2b2a1778", "title": "Data Mining and Student e-Learning Profiles", "abstract": "Data mining techniques have been applied to educational research in various ways. In this paper, I presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web-based learning environment (gStudy). The data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation. The use of this method is illustrated through a sequential pattern analysis of gStudy log files generated by university students.", "year": 2010, "authors": [{"authorId": "50504088", "name": "Mingming Zhou"}], "cluster": 5, "position": {"x": -114.18639373779297, "y": 48.04365158081055}}, {"paperId": "9b0c9d241269b98c80f65a14d5d65263d0688d70", "url": "https://www.semanticscholar.org/paper/9b0c9d241269b98c80f65a14d5d65263d0688d70", "title": "What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations", "abstract": "The quality of a counseling intervention relies highly on the active collaboration between clients and counselors. In this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.", "year": 2019, "authors": [{"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}, {"authorId": "50171962", "name": "Xinyi Wu"}, {"authorId": "34613400", "name": "K. Resnicow"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}], "cluster": 6, "position": {"x": -63.10304641723633, "y": -59.65132141113281}}, {"paperId": "c278eb333c78313a3762b5b2f07ace7443582b94", "url": "https://www.semanticscholar.org/paper/c278eb333c78313a3762b5b2f07ace7443582b94", "title": "Expressive Interviewing: A Conversational System for Coping with COVID-19", "abstract": "The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce \\textit{Expressive Interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system's design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues.", "year": 2020, "authors": [{"authorId": "1484893124", "name": "Charles Welch"}, {"authorId": "1801600316", "name": "Allison Lahnala"}, {"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}, {"authorId": "2072820796", "name": "Siqi Shen"}, {"authorId": "94361572", "name": "S. Seraj"}, {"authorId": "2113905187", "name": "Lawrence An"}, {"authorId": "34613400", "name": "K. Resnicow"}, {"authorId": "1854783", "name": "J. Pennebaker"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}], "cluster": 10, "position": {"x": -23.235889434814453, "y": -44.40036392211914}}, {"paperId": "aa2bbf5a1485cc90401e4a35c04463fcb4e632f0", "url": "https://www.semanticscholar.org/paper/aa2bbf5a1485cc90401e4a35c04463fcb4e632f0", "title": "Quantifying the Effects of COVID-19 on Mental Health Support Forums", "abstract": "The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to better understand the effects of COVID-19 on mental health by examining discussions within mental health support communities on Reddit. First, we quantify the rate at which COVID-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. Next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. Finally, we analyze how COVID-19 has influenced language use and topics of discussion within each subreddit.", "year": 2020, "authors": [{"authorId": "1395927418", "name": "Laura Biester"}, {"authorId": "92093876", "name": "K. Matton"}, {"authorId": "10197529", "name": "Janarthanan Rajendran"}, {"authorId": "2523983", "name": "E. Provost"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}], "cluster": 10, "position": {"x": -12.431851387023926, "y": -36.604862213134766}}, {"paperId": "35112c6bfa715b2bc86fc5b6c331f8a1a0b07add", "url": "https://www.semanticscholar.org/paper/35112c6bfa715b2bc86fc5b6c331f8a1a0b07add", "title": "Understanding and Predicting Empathic Behavior in Counseling Therapy", "abstract": "Counselor empathy is associated with better outcomes in psychology and behavioral counseling. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. Particularly, we analyze aspects such as participants\u2019 engagement, participants\u2019 verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.", "year": 2017, "authors": [{"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}, {"authorId": "34613400", "name": "K. Resnicow"}, {"authorId": "2108384855", "name": "Satinder Singh"}, {"authorId": "2113905187", "name": "Lawrence An"}], "cluster": 6, "position": {"x": -70.67552947998047, "y": -61.72417449951172}}, {"paperId": "b688b67aa1980225e72e81f3d971d0e97a0d5484", "url": "https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484", "title": "Building a Motivational Interviewing Dataset", "abstract": "This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with", "year": 2016, "authors": [{"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}, {"authorId": "34613400", "name": "K. Resnicow"}, {"authorId": "2108384855", "name": "Satinder Singh"}, {"authorId": "2113905187", "name": "Lawrence An"}], "cluster": 6, "position": {"x": -67.59474182128906, "y": -71.30198669433594}}, {"paperId": "55925bc2522a1d4cb007ac78273f7c2efafa3916", "url": "https://www.semanticscholar.org/paper/55925bc2522a1d4cb007ac78273f7c2efafa3916", "title": "Predicting Counselor Behaviors in Motivational Interviewing Encounters", "abstract": "As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.", "year": 2017, "authors": [{"authorId": "2108384855", "name": "Satinder Singh"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}, {"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}, {"authorId": "34613400", "name": "K. Resnicow"}, {"authorId": "2113905187", "name": "Lawrence An"}, {"authorId": "5400787", "name": "K. Goggin"}, {"authorId": "2307781", "name": "D. Catley"}], "cluster": 6, "position": {"x": -63.53562545776367, "y": -66.40493774414062}}, {"paperId": "c239aaa2a87ea63fae666b2051f1923afaa45fc2", "url": "https://www.semanticscholar.org/paper/c239aaa2a87ea63fae666b2051f1923afaa45fc2", "title": "HappyDB: A Corpus of 100, 000 Crowdsourced Happy Moments", "abstract": "The science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion. Recently, there has been interest in developing technologies that help incorporate the findings of the science of happiness into users' daily lives by steering them towards behaviors that increase happiness. With the goal of building technology that can understand how people express their happy moments in text, we crowd-sourced HappyDB, a corpus of 100,000 happy moments that we make publicly available. This paper describes HappyDB and its properties, and outlines several important NLP problems that can be studied with the help of the corpus. We also apply several state-of-the-art analysis techniques to analyze HappyDB. Our results demonstrate the need for deeper NLP techniques to be developed which makes HappyDB an exciting resource for follow-on research.", "year": 2018, "authors": [{"authorId": "35584853", "name": "Akari Asai"}, {"authorId": "35412470", "name": "Sara Evensen"}, {"authorId": "3047778", "name": "Behzad Golshan"}, {"authorId": "1770962", "name": "A. Halevy"}, {"authorId": "2052674361", "name": "Vivian Li"}, {"authorId": "2888751", "name": "A. Lopatenko"}, {"authorId": "46552353", "name": "Daniela Stepanov"}, {"authorId": "38844482", "name": "Yoshihiko Suhara"}, {"authorId": "34582619", "name": "W. Tan"}, {"authorId": "10032840", "name": "Yinzhan Xu"}], "cluster": 4, "position": {"x": -72.46634674072266, "y": 115.08003234863281}}, {"paperId": "c151f144c2c0e8d3b176edaf2ce5369c7707bd31", "url": "https://www.semanticscholar.org/paper/c151f144c2c0e8d3b176edaf2ce5369c7707bd31", "title": "Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health", "abstract": "Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.", "year": 2016, "authors": [{"authorId": "1745524", "name": "Tim Althoff"}, {"authorId": "144358401", "name": "Kevin Clark"}, {"authorId": "1702139", "name": "J. Leskovec"}], "cluster": 6, "position": {"x": -55.986671447753906, "y": -60.08910369873047}}, {"paperId": "c2fd13dab7a581f2f875826b204d063f42b63a8b", "url": "https://www.semanticscholar.org/paper/c2fd13dab7a581f2f875826b204d063f42b63a8b", "title": "Happiness Entailment: Automating Suggestions for Well-Being", "abstract": "Understanding what makes people happy is a central topic in psychology. Prior work has mostly focused on developing self-reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments. One of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well-being. In this paper, we outline a complementary approach; on the assumption that the user journals her happy moments as short texts, a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well-being. We prototype one necessary component of such a system, the Happiness Entailment Recognition (HER)module, which takes as input a short text describing an event, a candidate suggestion, and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described. This component is implemented as a neural network model with two encoders, one for the user input and one for the candidate actionable suggestion, with additional layers to capture psychologically significant features in the happy moment and suggestion. Our model achieves an AU-ROC of 0.831 and outperforms our baseline as well as the current state-of-the-art Textual Entailment model from AllenNLP by more than 48% of improvements, confirming the uniqueness and complexity of the HER task.", "year": 2019, "authors": [{"authorId": "35412470", "name": "Sara Evensen"}, {"authorId": "38844482", "name": "Yoshihiko Suhara"}, {"authorId": "1770962", "name": "A. Halevy"}, {"authorId": "2052674361", "name": "Vivian Li"}, {"authorId": "34582619", "name": "W. Tan"}, {"authorId": "91456334", "name": "Saran Mumick"}], "cluster": 4, "position": {"x": -78.68013000488281, "y": 115.10941314697266}}, {"paperId": "7cb5bc9f8bc023f1f2c94e2e83012f1d35e4e01d", "url": "https://www.semanticscholar.org/paper/7cb5bc9f8bc023f1f2c94e2e83012f1d35e4e01d", "title": "Ingredients for Happiness: Modeling constructs via semi-supervised content driven inductive transfer", "abstract": "Modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios. In the CL-Aff Shared Task (part of Affective Content Analysis workshop @ AAAI 2019), the organizers released a dataset of \u2018happy\u2019 moments, called the HappyDB corpus. The task is to detect two social constructs: the agency (i.e., whether the author is in control of the happy moment) and the social characteristics (i.e., whether anyone else other than the author was also involved in the happy moment). We employ an inductive transfer learning technique where we utilize a pre-trained language model and fine-tune it on the target task for both the binary classification tasks. At first, we use a language model pre-trained on the huge WikiText-103 corpus. This step utilizes an AWDLSTM with three hidden layers for training the language model. In the second step, we fine-tune the pre-trained language model on both the labeled and unlabeled instances from the HappyDB dataset. Finally, we train a classifier on top of the language model for each of the identification tasks. Our experiments using 10-fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author, showing significant gains over other baselines. We also show that using the unlabeled dataset for fine-tuning the language model in the second step improves our accuracy by 1-2% across detection of both the constructs.", "year": 2019, "authors": [{"authorId": "50708394", "name": "B. Syed"}, {"authorId": "7259599", "name": "Vijayasaradhi Indurthi"}, {"authorId": "2053918106", "name": "Kulin Shah"}, {"authorId": "46722320", "name": "Manish Gupta"}, {"authorId": "1704709", "name": "Vasudeva Varma"}], "cluster": 4, "position": {"x": -75.31620788574219, "y": 120.99507904052734}}, {"paperId": "1b41d919b912795927109fde8383d5bc25467b3c", "url": "https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c", "title": "HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion", "abstract": "Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.", "year": 2020, "authors": [{"authorId": "117018834", "name": "Bharathi Raja Chakravarthi"}], "cluster": 8, "position": {"x": -16.9995174407959, "y": 35.058685302734375}}, {"paperId": "32bf7ad3fdda71036b48f7dc85cad407674277b7", "url": "https://www.semanticscholar.org/paper/32bf7ad3fdda71036b48f7dc85cad407674277b7", "title": "Women worry about family, men about the economy: Gender differences in emotional responses to COVID-19", "abstract": "Among the critical challenges around the COVID-19 pandemic is dealing with the potentially detrimental effects on people's mental health. Designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries, concerns and emotional responses from text data. We examine gender differences and the effect of document length on worries about the ongoing COVID-19 situation. Our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts. We further find ii) marked gender differences in topics concerning emotional responses. Women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society. This paper adds to the understanding of general gender differences in language found elsewhere, and shows that the current unique circumstances likely amplified these effects. We close this paper with a call for more high-quality datasets due to the limitations of Tweet-sized data.", "year": 2020, "authors": [{"authorId": "51241310", "name": "Isabelle van der Vegt"}, {"authorId": "6032930", "name": "Bennett Kleinberg"}], "cluster": 10, "position": {"x": -15.145685195922852, "y": -42.14733123779297}}, {"paperId": "64c68fa52491c4a815f21917e068e4c19dd404b0", "url": "https://www.semanticscholar.org/paper/64c68fa52491c4a815f21917e068e4c19dd404b0", "title": "FERMI at SemEval-2019 Task 5: Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in Twitter", "abstract": "This paper describes our system (Fermi) for Task 5 of SemEval-2019: HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter. We participated in the subtask A for English and ranked first in the evaluation on the test set. We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team - Fermi\u2019s model achieved an accuracy of 65.00% for English language in task A. Our models, which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task.", "year": 2019, "authors": [{"authorId": "7259599", "name": "Vijayasaradhi Indurthi"}, {"authorId": "50708394", "name": "B. Syed"}, {"authorId": "2045067", "name": "Manish Shrivastava"}, {"authorId": "138436346", "name": "Nikhil Chakravartula"}, {"authorId": "46722320", "name": "Manish Gupta"}, {"authorId": "1704709", "name": "Vasudeva Varma"}], "cluster": 8, "position": {"x": -10.598173141479492, "y": 39.76396942138672}}, {"paperId": "52ef2bea83eba1575fcfd02c82b4228f7aab0bd6", "url": "https://www.semanticscholar.org/paper/52ef2bea83eba1575fcfd02c82b4228f7aab0bd6", "title": "Cheap Talk and Cherry-Picking: What ClimateBert has to say on Corporate Climate Risk Disclosures", "abstract": "Disclosure of climate-related financial risks greatly helps investors assess companies' preparedness for climate change. Voluntary disclosures such as those based on the recommendations of the Task Force for Climate-related Financial Disclosures (TCFD) are being hailed as an effective measure for better climate risk management. We ask whether this expectation is justified. We do so with the help of a deep neural language model, which we christen ClimateBert. We train ClimateBert on thousands of sentences related to climate-risk disclosures aligned with the TCFD recommendations. In analyzing the disclosures of TCFD-supporting firms, ClimateBert comes to the sobering conclusion that the firms' TCFD support is mostly cheap talk and that firms cherry-pick to report primarily non-material climate risk information. From our analysis, we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures.", "year": 2021, "authors": [{"authorId": "2006205621", "name": "Julia Anna Bingler"}, {"authorId": "10743797", "name": "Mathias Kraus"}, {"authorId": "3073566", "name": "Markus Leippold"}], "cluster": 1, "position": {"x": 42.97530746459961, "y": -135.5842742919922}}, {"paperId": "a9d09338a3a0d9102c1e623e6ad434a446c0bbfe", "url": "https://www.semanticscholar.org/paper/a9d09338a3a0d9102c1e623e6ad434a446c0bbfe", "title": "Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism", "abstract": "Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models.", "year": 2021, "authors": [{"authorId": "1984990", "name": "Shraey Bhatia"}, {"authorId": "1800564", "name": "Jey Han Lau"}, {"authorId": "123917295", "name": "Tim Baldwin"}], "cluster": 1, "position": {"x": 53.48640060424805, "y": -98.95967102050781}}, {"paperId": "998039a4876edc440e0cabb0bc42239b0eb29644", "url": "https://www.semanticscholar.org/paper/998039a4876edc440e0cabb0bc42239b0eb29644", "title": "Tackling Climate Change with Machine Learning", "abstract": "Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.", "year": 2019, "authors": [{"authorId": "2381187", "name": "D. Rolnick"}, {"authorId": "49698491", "name": "P. Donti"}, {"authorId": "12736608", "name": "L. Kaack"}, {"authorId": "80536011", "name": "K. Kochanski"}, {"authorId": "8651990", "name": "Alexandre Lacoste"}, {"authorId": "46769963", "name": "K. Sankaran"}, {"authorId": "2068943123", "name": "Andrew Slavin Ross"}, {"authorId": "1417437895", "name": "Nikola Milojevic-Dupont"}, {"authorId": "3106683", "name": "Natasha Jaques"}, {"authorId": "1419467725", "name": "Anna Waldman-Brown"}, {"authorId": "2993731", "name": "Alexandra Luccioni"}, {"authorId": "3422058", "name": "Tegan Maharaj"}, {"authorId": "74936246", "name": "Evan D. Sherwin"}, {"authorId": "103485736", "name": "S. Mukkavilli"}, {"authorId": "36121677", "name": "K. K\u00f6rding"}, {"authorId": "2064532325", "name": "Carla P. Gomes"}, {"authorId": "2067948334", "name": "Andrew Y. Ng"}, {"authorId": "48987704", "name": "D. Hassabis"}, {"authorId": "144189092", "name": "John C. Platt"}, {"authorId": "47628266", "name": "F. Creutzig"}, {"authorId": "1695997", "name": "J. Chayes"}, {"authorId": "1751762", "name": "Yoshua Bengio"}], "cluster": 1, "position": {"x": 26.998981475830078, "y": -119.36385345458984}}, {"paperId": "1d37460baded22f488085e82985419178679dce0", "url": "https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0", "title": "The Climate Change Debate and Natural Language Processing", "abstract": "The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.", "year": 2021, "authors": [{"authorId": "2574304", "name": "Manfred Stede"}, {"authorId": "80953245", "name": "R. Patz"}], "cluster": 1, "position": {"x": 36.42068099975586, "y": -102.12275695800781}}, {"paperId": "9de154d3c886177380062be7c8d50304a335752f", "url": "https://www.semanticscholar.org/paper/9de154d3c886177380062be7c8d50304a335752f", "title": "Fermi at SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media using Sentence Embeddings", "abstract": "This paper describes our system (Fermi) for Task 6: OffensEval: Identifying and Categorizing Offensive Language in Social Media of SemEval-2019. We participated in all the three sub-tasks within Task 6. We evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team Fermi\u2019s model achieved an F1-score of 64.40%, 62.00% and 62.60% for sub-task A, B and C respectively on the official leaderboard. Our model for sub-task C which uses pre-trained ELMo embeddings for transforming the input and uses SVM (RBF kernel) for training, scored third position on the official leaderboard. Through the paper we provide a detailed description of the approach, as well as the results obtained for the task.", "year": 2019, "authors": [{"authorId": "7259599", "name": "Vijayasaradhi Indurthi"}, {"authorId": "50708394", "name": "B. Syed"}, {"authorId": "2045067", "name": "Manish Shrivastava"}, {"authorId": "46722320", "name": "Manish Gupta"}, {"authorId": "1704709", "name": "Vasudeva Varma"}], "cluster": 8, "position": {"x": -12.368562698364258, "y": 44.55643081665039}}, {"paperId": "bbc6486ad37365b77b9d0de8894f595d70af49ac", "url": "https://www.semanticscholar.org/paper/bbc6486ad37365b77b9d0de8894f595d70af49ac", "title": "Ask BERT: How Regulatory Disclosure of Transition and Physical Climate Risks affects the CDS Term Structure", "abstract": "We use BERT, an AI-based algorithm for language understanding, to decipher regulatory climate-risk disclosures and measure their impact on the credit default swap (CDS) market. Risk disclosures can either increase or decrease credit spreads, depending on whether disclosure reveals new risks or sharpens the signal and decreases the uncertainty. Training BERT to differentiate between transition and physical climate risks, we find that disclosing transition risks increases CDS spreads, especially after the Paris Climate Agreement of 2015, while disclosing physical climate risks leads to a decrease in CDS spreads. These impacts are statistically and economically highly significant.", "year": 2020, "authors": [{"authorId": "104846221", "name": "Julian F. K\u00f6lbel"}, {"authorId": "3073566", "name": "Markus Leippold"}, {"authorId": "2050613648", "name": "J. Rillaerts"}, {"authorId": "2048787257", "name": "Q. Wang"}], "cluster": 1, "position": {"x": 48.06892395019531, "y": -134.6238250732422}}, {"paperId": "134c8486ae58b421681656c85bbc48dc862f6f98", "url": "https://www.semanticscholar.org/paper/134c8486ae58b421681656c85bbc48dc862f6f98", "title": "ClimaText: A Dataset for Climate Change Topic Detection", "abstract": "Climate change communication in the mass media and other textual sources may affect and shape public perception. Extracting climate change information from these sources is an important task, e.g., for filtering content and e-discovery, sentiment analysis, automatic summarization, question-answering, and fact-checking. However, automating this process is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular text-based AI tasks. In this paper, we introduce \\textsc{ClimaText}, a dataset for sentence-based climate change topic detection, which we make publicly available. We explore different approaches to identify the climate change topic in various text sources. We find that popular keyword-based models are not adequate for such a complex and evolving task. Context-based algorithms like BERT \\cite{devlin2018bert} can detect, in addition to many trivial cases, a variety of complex and implicit topic patterns. Nevertheless, our analysis reveals a great potential for improvement in several directions, such as, e.g., capturing the discussion on indirect effects of climate change. Hence, we hope this work can serve as a good starting point for further research on this topic.", "year": 2020, "authors": [{"authorId": "2029887098", "name": "Francesco S. Varini"}, {"authorId": "1389036863", "name": "Jordan L. Boyd-Graber"}, {"authorId": "2754495", "name": "Massimiliano Ciaramita"}, {"authorId": "3073566", "name": "Markus Leippold"}], "cluster": 1, "position": {"x": 38.56993865966797, "y": -96.8475570678711}}, {"paperId": "086e6733e5fda70bbce0c7545bd06d5634918a60", "url": "https://www.semanticscholar.org/paper/086e6733e5fda70bbce0c7545bd06d5634918a60", "title": "CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims", "abstract": "We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER [1], the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \\textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community.", "year": 2020, "authors": [{"authorId": "2106073554", "name": "Thomas Diggelmann"}, {"authorId": "1389036863", "name": "Jordan L. Boyd-Graber"}, {"authorId": "2362210", "name": "Jannis Bulian"}, {"authorId": "2754495", "name": "Massimiliano Ciaramita"}, {"authorId": "3073566", "name": "Markus Leippold"}], "cluster": 1, "position": {"x": 23.287260055541992, "y": -108.30899810791016}}, {"paperId": "f071e7c19fc18fc4d4ab69ce13f0c4e92cfb08c4", "url": "https://www.semanticscholar.org/paper/f071e7c19fc18fc4d4ab69ce13f0c4e92cfb08c4", "title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled Data", "abstract": "While it is well-documented that climate change accepters and deniers have become increasingly polarized in the United States over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. On the sub-population of Twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U.S. in 2018. We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. We then apply RNNs to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. However, this effect does not hold for the 2018 blizzard and wildfires studied, implying that Twitter users' opinions on climate change are fairly ingrained on this subset of natural disasters.", "year": 2019, "authors": [{"authorId": "101883616", "name": "Allison Koenecke"}, {"authorId": "1411879443", "name": "Jordi Feliu-Fab\u00e0"}], "cluster": 1, "position": {"x": 33.43470764160156, "y": -112.95494079589844}}, {"paperId": "1d7a0e8be2bbe7d148bb48a1f9de218beaea4ed5", "url": "https://www.semanticscholar.org/paper/1d7a0e8be2bbe7d148bb48a1f9de218beaea4ed5", "title": "DeSMOG: Detecting Stance in Media On Global Warming", "abstract": "Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, \u201cLeading scientists agree that global warming is a serious concern,\u201d framing a clause which affirms their own stance (\u201cthat global warming is serious\u201d) as an opinion endorsed (\"[scientists] agree\u201d) by a reputable source (\u201cleading\u201d). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: \u201cMistaken scientists claim [...].\" Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce DeSMOG, a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other\u2019s opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author\u2019s own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.", "year": 2020, "authors": [{"authorId": "18160260", "name": "Yiwei Luo"}, {"authorId": "35540755", "name": "D. Card"}, {"authorId": "1746807", "name": "Dan Jurafsky"}], "cluster": 1, "position": {"x": 49.610774993896484, "y": -94.65247344970703}}, {"paperId": "418b9f37b07a05b38798e97f57e17a6cc1048b92", "url": "https://www.semanticscholar.org/paper/418b9f37b07a05b38798e97f57e17a6cc1048b92", "title": "You are right. I am ALARMED - But by Climate Change Counter Movement", "abstract": "The world is facing the challenge of climate crisis. Despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP. Despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. We try to bridge this gap by scraping and releasing articles with known climate change misinformation.", "year": 2020, "authors": [{"authorId": "1984990", "name": "Shraey Bhatia"}, {"authorId": "1800564", "name": "Jey Han Lau"}, {"authorId": "145465286", "name": "Timothy Baldwin"}], "cluster": 1, "position": {"x": 17.603477478027344, "y": -101.61248016357422}}, {"paperId": "3475876c3f5e624587181744bc5acf4223aecd00", "url": "https://www.semanticscholar.org/paper/3475876c3f5e624587181744bc5acf4223aecd00", "title": "Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings", "abstract": "We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our methods to study 4.4M tweets on 21 mass shootings. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. We identify framing devices, such as grounding and the contrasting use of the terms \u201cterrorist\u201d and \u201ccrazy\u201d, that contribute to polarization. Results pertaining to topic choice, affect and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.", "year": 2019, "authors": [{"authorId": "81551000", "name": "Dorottya Demszky"}, {"authorId": "2779427", "name": "Nikhil Garg"}, {"authorId": "35248702", "name": "R. Voigt"}, {"authorId": "145085305", "name": "James Y. Zou"}, {"authorId": "8731644", "name": "Matthew Gentzkow"}, {"authorId": "122795272", "name": "Jesse M. Shapiro"}, {"authorId": "1746807", "name": "Dan Jurafsky"}], "cluster": 12, "position": {"x": 79.38226318359375, "y": -27.99814796447754}}, {"paperId": "171847842d161743f2b52dc4f85c121087ceb426", "url": "https://www.semanticscholar.org/paper/171847842d161743f2b52dc4f85c121087ceb426", "title": "Comparing Attitudes to Climate Change in the Media using sentiment analysis based on Latent Dirichlet Allocation", "abstract": "News media typically present biased accounts of news stories, and different publications present different angles on the same event. In this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. To understand these attitudes, we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet, a general sentiment lexicon. Using LDA, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity. Preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. Ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.", "year": 2017, "authors": [{"authorId": "2051229488", "name": "Ye Jiang"}, {"authorId": "3456762", "name": "Xingyi Song"}, {"authorId": "48276492", "name": "J. Harrison"}, {"authorId": "1776008", "name": "S. Quegan"}, {"authorId": "2144272", "name": "D. Maynard"}], "cluster": 1, "position": {"x": 36.97146987915039, "y": -89.27102661132812}}, {"paperId": "9b1653e3b57016958d10ff8531475eb0483d156c", "url": "https://www.semanticscholar.org/paper/9b1653e3b57016958d10ff8531475eb0483d156c", "title": "Social Privacy in Networked Publics: Teens\u2019 Attitudes, Practices, and Strategies", "abstract": "This paper examines how teens understand privacy in highly public networked environments like Facebook and Twitter. We describe both teens\u2019 practices, their privacy strategies, and the structural conditions in which they are embedded, highlighting the ways in which privacy, as it plays out in everyday life, is related more to agency and the ability to control a social situation than particular properties of information. Finally, we discuss the implications of teens\u2019 practices and strategies, revealing the importance of social norms as a regulatory force.(This paper was presented at Oxford Internet Institute\u2019s \u201cA Decade in Internet Time: Symposium on the Dynamics of the Internet and Society\u201d on September 22, 2011.)", "year": 2011, "authors": [{"authorId": "38818867", "name": "D. Boyd"}, {"authorId": "2929988", "name": "Alice E. Marwick"}], "cluster": -1, "position": {"x": 105.2887954711914, "y": 37.87517166137695}}, {"paperId": "29584ed6d68a06fdf91440a018f6bc83a44fd177", "url": "https://www.semanticscholar.org/paper/29584ed6d68a06fdf91440a018f6bc83a44fd177", "title": "Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases", "abstract": "Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.", "year": 2021, "authors": [{"authorId": "10783142", "name": "Ilias Chalkidis"}, {"authorId": "121316873", "name": "Manos Fergadiotis"}, {"authorId": "8245526", "name": "D. Tsarapatsanis"}, {"authorId": "2042831965", "name": "Nikolaos Aletras"}, {"authorId": "1752430", "name": "Ion Androutsopoulos"}, {"authorId": "1950133", "name": "Prodromos Malakasiotis"}], "cluster": 7, "position": {"x": 52.38268280029297, "y": 59.1190299987793}}, {"paperId": "2547836827e0423f198320977d393f574e0fb3d6", "url": "https://www.semanticscholar.org/paper/2547836827e0423f198320977d393f574e0fb3d6", "title": "Framing and Agenda-setting in Russian News: a Computational Analysis of Intricate Political Strategies", "abstract": "Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and \u201cfake news\u201d. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.", "year": 2018, "authors": [{"authorId": "49713890", "name": "Anjalie Field"}, {"authorId": "38317249", "name": "Doron Kliger"}, {"authorId": "2524073", "name": "S. Wintner"}, {"authorId": "50617806", "name": "Jennifer Pan"}, {"authorId": "1746807", "name": "Dan Jurafsky"}, {"authorId": "145317727", "name": "Yulia Tsvetkov"}], "cluster": 11, "position": {"x": 107.90266418457031, "y": -32.94938278198242}}, {"paperId": "546c87abc26137359c43898559884ba9d6c5ae64", "url": "https://www.semanticscholar.org/paper/546c87abc26137359c43898559884ba9d6c5ae64", "title": "Predicting the Role of Political Trolls in Social Media", "abstract": "We investigate the political roles of \"Internet trolls\" in social media. Political trolls, such as the ones linked to the Russian Internet Research Agency (IRA), have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this analysis is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. In this paper, we show how to automate this analysis by using machine learning in a realistic setting. In particular, we show how to classify trolls according to their political role ---left, news feed, right--- by using features extracted from social media, i.e., Twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e.,~embeddings, for the trolls. Experiments on the \"IRA Russian Troll\" dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.", "year": 2019, "authors": [{"authorId": "2063953501", "name": "Atanas Atanasov"}, {"authorId": "35168570", "name": "G. D. F. Morales"}, {"authorId": "1683562", "name": "Preslav Nakov"}], "cluster": 13, "position": {"x": 17.214366912841797, "y": 13.887959480285645}}, {"paperId": "9c2cab0e72f2588e857671c1fbb13b5fba8f2586", "url": "https://www.semanticscholar.org/paper/9c2cab0e72f2588e857671c1fbb13b5fba8f2586", "title": "Cross-Platform Disinformation Campaigns: Lessons Learned and Next Steps", "abstract": "We conducted a mixed-method, interpretative analysis of an online, cross-platform disinformation campaign targeting the White Helmets, a rescue group operating in rebel-held areas of Syria that has become the subject of a persistent effort of delegitimization. This research helps to conceptualize what a disinformation campaign is and how it works. Based on what we learned from this case study, we conclude that a comprehensive understanding of disinformation requires accounting for the spread of content across platforms and that social media platforms should increase collaboration to detect and characterize disinformation campaigns.", "year": 2020, "authors": [{"authorId": "3181624", "name": "Kate Starbird"}, {"authorId": "50503504", "name": "T. Wilson"}], "cluster": 7, "position": {"x": 92.34263610839844, "y": 105.2764892578125}}, {"paperId": "c0a2ee56fc80d971e920fba6179f81b515533592", "url": "https://www.semanticscholar.org/paper/c0a2ee56fc80d971e920fba6179f81b515533592", "title": "Historical Change in the Moral Foundations of Political Persuasion", "abstract": "How have attempts at political persuasion changed over time? Using nine corpora dating back through 1789, containing over 7 million words of speech (1,666 documents in total), covering three different countries, plus the entire Google nGram corpus, we find that language relating to togetherness permanently crowded out language relating to duties and obligations in the persuasive speeches of politicians during the early 20th century. This shift is temporally predicted by a rise in Western nationalism and the mass movement of people from more rural to more urban areas and is unexplained by changes in language, private political speech, or nonmoral persuasion. We theorize that the emergence of the modern state in the 1920s had psychopolitical consequences for the ways that people understood and communicated their relationships with their government, which was then reflected in the levers of persuasion chosen by political elites.", "year": 2020, "authors": [{"authorId": "51217405", "name": "N. Buttrick"}, {"authorId": "145855924", "name": "R. Moulder"}, {"authorId": "33364770", "name": "S. Oishi"}], "cluster": 12, "position": {"x": 77.16404724121094, "y": -50.8026237487793}}, {"paperId": "a12d22ff91ce159a0d3558ed5aaed115115beabd", "url": "https://www.semanticscholar.org/paper/a12d22ff91ce159a0d3558ed5aaed115115beabd", "title": "Fine-Grained Analysis of Propaganda in News Articles", "abstract": "Propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.", "year": 2019, "authors": [{"authorId": "34086979", "name": "Giovanni Da San Martino"}, {"authorId": "1885974", "name": "Seunghak Yu"}, {"authorId": "1397442049", "name": "Alberto Barr\u00f3n-Cede\u00f1o"}, {"authorId": "2065869784", "name": "R. Petrov"}, {"authorId": "1683562", "name": "Preslav Nakov"}], "cluster": 7, "position": {"x": 74.0443344116211, "y": 119.93367767333984}}, {"paperId": "c0472b63d7d45d948dea42c4b5efffb6859541f9", "url": "https://www.semanticscholar.org/paper/c0472b63d7d45d948dea42c4b5efffb6859541f9", "title": "Technology, Autonomy, and Manipulation", "abstract": "Since 2016, when the Facebook/Cambridge Analytica scandal began to emerge, public concern has grown around the threat of \u201conline manipulation\u201d. While these worries are familiar to privacy researchers, this paper aims to make them more salient to policymakers \u2014 first, by defining \u201conline manipulation\u201d, thus enabling identification of manipulative practices; and second, by drawing attention to the specific harms online manipulation threatens. We argue that online manipulation is the use of information technology to covertly influence another person\u2019s decision-making, by targeting and exploiting their decision-making vulnerabilities. Engaging in such practices can harm individuals by diminishing their economic interests, but its deeper, more insidious harm is its challenge to individual autonomy. We explore this autonomy harm, emphasising its implications for both individuals and society, and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world.", "year": 2019, "authors": [{"authorId": "2585895", "name": "Daniel Susser"}, {"authorId": "33174950", "name": "B. Roessler"}, {"authorId": "2994505", "name": "H. Nissenbaum"}], "cluster": -1, "position": {"x": 105.81392669677734, "y": 43.04277801513672}}, {"paperId": "08e3ce4454012d928470efa96a54aa08dd8a96b0", "url": "https://www.semanticscholar.org/paper/08e3ce4454012d928470efa96a54aa08dd8a96b0", "title": "Automatically Characterizing Targeted Information Operations Through Biases Present in Discourse on Twitter", "abstract": "This paper considers the problem of automatically characterizing biases that may be associated with emerging information operations via artificial intelligence. Accurate analysis of these emerging topics usually requires laborious, manual analysis by experts to annotate millions of tweets to identify biases in new topics. We introduce adaptations of the Word Embedding Association Test [1] to a new domain: information operations. We validate our method using known information operation-related tweets from Twitter's Transparency Reports, and we perform a case study on the COVID-19 pandemic to evaluate our method's performance on non-labeled Twitter data, demonstrating its usability in emerging domains.", "year": 2021, "authors": [{"authorId": "1643684431", "name": "Autumn Toney"}, {"authorId": "2070539062", "name": "Akshat Pandey"}, {"authorId": "28735341", "name": "W. Guo"}, {"authorId": "3119101", "name": "David A. Broniatowski"}, {"authorId": "144537437", "name": "Aylin Caliskan"}], "cluster": 12, "position": {"x": 72.12188720703125, "y": -24.223602294921875}}, {"paperId": "7de1c0db3b3848d1a51dbfefce8b73e89a682177", "url": "https://www.semanticscholar.org/paper/7de1c0db3b3848d1a51dbfefce8b73e89a682177", "title": "Classification of Moral Foundations in Microblog Political Discourse", "abstract": "Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.", "year": 2018, "authors": [{"authorId": "49808204", "name": "Kristen Marie Johnson"}, {"authorId": "2877164", "name": "Dan Goldwasser"}], "cluster": 12, "position": {"x": 76.21260833740234, "y": -42.95819091796875}}, {"paperId": "d89e89eac7882790b48ad73376742e7a93fea4fd", "url": "https://www.semanticscholar.org/paper/d89e89eac7882790b48ad73376742e7a93fea4fd", "title": "Red Bots Do It Better:Comparative Analysis of Social Bot Partisan Behavior", "abstract": "Recent research brought awareness of the issue of bots on social media and the significant risks of mass manipulation of public opinion in the context of political discussion. In this work, we leverage Twitter to study the discourse during the 2018 US midterm elections and analyze social bot activity and interactions with humans. We collected 2.6 million tweets for 42 days around the election day from nearly 1 million users. We use the collected tweets to answer three research questions: (i) Do social bots lean and behave according to a political ideology? (ii) Can we observe different strategies among liberal and conservative bots? (iii) How effective are bot strategies in engaging humans? We show that social bots can be accurately classified according to their political leaning and behave accordingly. Conservative bots share most of the topics of discussion with their human counterparts, while liberal bots show less overlap and a more inflammatory attitude. We studied bot interactions with humans and observed different strategies. Finally, we measured bots embeddedness in the social network and the extent of human engagement with each group of bots. Results show that conservative bots are more deeply embedded in the social network and more effective than liberal bots at exerting influence on humans.", "year": 2019, "authors": [{"authorId": "3349623", "name": "Luca Luceri"}, {"authorId": "144003355", "name": "A. Deb"}, {"authorId": "9552744", "name": "Adam Badawy"}, {"authorId": "2082804692", "name": "Emilio Ferrara"}], "cluster": 13, "position": {"x": 21.111560821533203, "y": 6.14729642868042}}, {"paperId": "16981cc4ddefd3ea7655754fd83a2a8ff2203a8b", "url": "https://www.semanticscholar.org/paper/16981cc4ddefd3ea7655754fd83a2a8ff2203a8b", "title": "Automatically Neutralizing Subjective Bias in Text", "abstract": "Texts like news, encyclopedias, and some social media strive for objectivity. Yet bias in the form of inappropriate subjectivity - introducing attitudes via framing, presupposing truth, and casting doubt - remains ubiquitous. This kind of bias erodes our collective trust and fuels social conflict. To address this issue, we introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (\"neutralizing\" biased text). We also offer the first parallel corpus of biased language. The corpus contains 180,000 sentence pairs and originates from Wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. Last, we propose two strong encoder-decoder baselines for the task. A straightforward yet opaque CONCURRENT system uses a BERT encoder to identify subjective words as part of the generation process. An interpretable and controllable MODULAR algorithm separates these steps, using (1) a BERT-based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder. Large-scale human evaluation across four domains (encyclopedias, news headlines, books, and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias.", "year": 2020, "authors": [{"authorId": "4099006", "name": "Reid Pryzant"}, {"authorId": "144584594", "name": "Richard Diehl Martinez"}, {"authorId": "40556444", "name": "Nathan Dass"}, {"authorId": "1795664", "name": "S. Kurohashi"}, {"authorId": "1746807", "name": "Dan Jurafsky"}, {"authorId": "2022168", "name": "Diyi Yang"}], "cluster": 8, "position": {"x": -33.32257080078125, "y": 37.85023498535156}}, {"paperId": "9e82aba279807a6c8541c8811f049a40ebca36c1", "url": "https://www.semanticscholar.org/paper/9e82aba279807a6c8541c8811f049a40ebca36c1", "title": "Issue Framing in Online Discussion Fora", "abstract": "In online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. In social science, this is referred to as issue framing. In this paper, we introduce a new issue frame annotated corpus of online discussions. We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.", "year": 2019, "authors": [{"authorId": "34604611", "name": "Mareike Hartmann"}, {"authorId": "93663917", "name": "Tallulah Jansen"}, {"authorId": "1736067", "name": "Isabelle Augenstein"}, {"authorId": "1700187", "name": "Anders S\u00f8gaard"}], "cluster": 11, "position": {"x": 131.88290405273438, "y": -13.370356559753418}}, {"paperId": "a0a5669b310ceb3840a599bf4f27a077a643d613", "url": "https://www.semanticscholar.org/paper/a0a5669b310ceb3840a599bf4f27a077a643d613", "title": "A Systematic Media Frame Analysis of 1.5 Million New York Times Articles from 2000 to 2017", "abstract": "Framing is an indispensable narrative device for news media because even the same facts may lead to conflicting understandings if deliberate framing is employed. Therefore, identifying media framing is a crucial step to understanding how news media influence the public. Framing is, however, difficult to operationalize and detect, and thus traditional media framing studies had to rely on manual annotation, which is challenging to scale up to massive news datasets. Here, by developing a media frame classifier that achieves state-of-the-art performance, we systematically analyze the media frames of 1.5 million New York Times articles published from 2000 to 2017. By examining the ebb and flow of media frames over almost two decades, we show that short-term frame abundance fluctuation closely corresponds to major events, while there also exist several long-term trends, such as the gradually increasing prevalence of the \u201cCultural identity\u201d frame. By examining specific topics and sentiments, we identify characteristics and dynamics of each frame. Finally, as a case study, we delve into the framing of mass shootings, revealing three major framing patterns. Our scalable, computational approach to massive news datasets opens up new pathways for systematic media framing studies.", "year": 2020, "authors": [{"authorId": "2113451145", "name": "Haewoon Kwak"}, {"authorId": "40660541", "name": "Jisun An"}, {"authorId": "36663090", "name": "Yong-Yeol Ahn"}], "cluster": 11, "position": {"x": 115.50447082519531, "y": -29.40366554260254}}, {"paperId": "20a37289d0b7c90065850eb7bf96d5ac3c8a21e4", "url": "https://www.semanticscholar.org/paper/20a37289d0b7c90065850eb7bf96d5ac3c8a21e4", "title": "Modeling Frames in Argumentation", "abstract": "In argumentation, framing is used to emphasize a specific aspect of a controversial topic while concealing others. When talking about legalizing drugs, for instance, its economical aspect may be emphasized. In general, we call a set of arguments that focus on the same aspect a frame. An argumentative text has to serve the \u201cright\u201d frame(s) to convince the audience to adopt the author\u2019s stance (e.g., being pro or con legalizing drugs). More specifically, an author has to choose frames that fit the audience\u2019s cultural background and interests. This paper introduces frame identification, which is the task of splitting a set of arguments into non-overlapping frames. We present a fully unsupervised approach to this task, which first removes topical information and then identifies frames using clustering. For evaluation purposes, we provide a corpus with 12, 326 debate-portal arguments, organized along the frames of the debates\u2019 topics. On this corpus, our approach outperforms different strong baselines, achieving an F1-score of 0.28.", "year": 2019, "authors": [{"authorId": "22312473", "name": "Yamen Ajjour"}, {"authorId": "2300829", "name": "Milad Alshomary"}, {"authorId": "2626599", "name": "Henning Wachsmuth"}, {"authorId": "144146081", "name": "Benno Stein"}], "cluster": 11, "position": {"x": 128.0576934814453, "y": -18.595783233642578}}, {"paperId": "92408cc19033cc4af29accef3793014ab79355c2", "url": "https://www.semanticscholar.org/paper/92408cc19033cc4af29accef3793014ab79355c2", "title": "The Media Frames Corpus: Annotations of Frames Across Issues", "abstract": "We describe the first version of the Media Frames Corpus: several thousand news articles on three policy issues, annotated in terms of media framing. We motivate framing as a phenomenon of study for computational linguistics and describe our annotation process.", "year": 2015, "authors": [{"authorId": "35540755", "name": "D. Card"}, {"authorId": "2641095", "name": "Amber E. Boydstun"}, {"authorId": "2603663", "name": "J. Gross"}, {"authorId": "1680292", "name": "P. Resnik"}, {"authorId": "144365875", "name": "Noah A. Smith"}], "cluster": 11, "position": {"x": 122.0289306640625, "y": -28.324430465698242}}, {"paperId": "2a501b074261e81b9126e80a0a308cfa5e76f8c1", "url": "https://www.semanticscholar.org/paper/2a501b074261e81b9126e80a0a308cfa5e76f8c1", "title": "Linguistic Models for Analyzing and Detecting Biased Language", "abstract": "Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective intensifiers. These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task.", "year": 2013, "authors": [{"authorId": "144409897", "name": "Marta Recasens"}, {"authorId": "1388368997", "name": "Cristian Danescu-Niculescu-Mizil"}, {"authorId": "1746807", "name": "Dan Jurafsky"}], "cluster": 8, "position": {"x": -38.44752883911133, "y": 37.787071228027344}}, {"paperId": "245bcbdfff19721c226059a5d5b1ae71b67e0572", "url": "https://www.semanticscholar.org/paper/245bcbdfff19721c226059a5d5b1ae71b67e0572", "title": "FrameAxis: Characterizing Framing Bias and Intensity with Word Embedding", "abstract": "We propose FrameAxis, a method of characterizing the framing of a given text by identifying the most relevant semantic axes (\"microframes\") defined by antonym word pairs. In contrast to the traditional framing analysis, which has been constrained by a small number of manually annotated general frames, our unsupervised approach provides much more detailed insights, by considering a host of semantic axes. Our method is capable of quantitatively teasing out framing bias -- how biased a text is in each microframe -- and framing intensity -- how much each microframe is used -- from the text, offering a nuanced characterization of framing. We evaluate our approach using SemEval datasets as well as three other datasets and human evaluations, demonstrating that FrameAxis can reliably characterize documents with relevant microframes. Our method may allow scalable and nuanced computational analyses of framing across disciplines.", "year": 2020, "authors": [{"authorId": "2113451145", "name": "Haewoon Kwak"}, {"authorId": "40660541", "name": "Jisun An"}, {"authorId": "36663090", "name": "Yong-Yeol Ahn"}], "cluster": 11, "position": {"x": 130.87002563476562, "y": -28.33321189880371}}, {"paperId": "c899c6048a1bd99590972c5fb2f9fd00db8d4e18", "url": "https://www.semanticscholar.org/paper/c899c6048a1bd99590972c5fb2f9fd00db8d4e18", "title": "Who Falls for Online Political Manipulation?", "abstract": "Social media, once hailed as a vehicle for democratization and the promotion of positive social change across the globe, are under attack for becoming a tool of political manipulation and spread of disinformation. A case in point is the alleged use of trolls by Russia to spread malicious content in Western elections. This paper examines the Russian interference campaign in the 2016 US presidential election on Twitter. Our aim is twofold: first, we test whether predicting users who spread trolls\u2019 content is feasible in order to gain insight on how to contain their influence in the future; second, we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content. We collected a dataset with over 43 million election-related posts shared on Twitter between September 16 and November 9, 2016, by about 5.7 million users. This dataset includes accounts associated with the Russian trolls identified by the US Congress. Proposed models are able to very accurately identify users who spread the trolls\u2019 content (average AUC score of 96%, using 10-fold validation). We show that political ideology, bot likelihood scores, and some activity-related account meta data are the most predictive features of whether a user spreads trolls\u2019 content or not.", "year": 2019, "authors": [{"authorId": "9552744", "name": "Adam Badawy"}, {"authorId": "1782658", "name": "Kristina Lerman"}, {"authorId": "48898287", "name": "Emilio Ferrara"}], "cluster": 13, "position": {"x": 22.75714683532715, "y": 12.749972343444824}}, {"paperId": "2cd74561109e2675634e0f4f0129fcfbc238a1d8", "url": "https://www.semanticscholar.org/paper/2cd74561109e2675634e0f4f0129fcfbc238a1d8", "title": "Connotation Frames of Power and Agency in Modern Films", "abstract": "The framing of an action influences how we perceive its actor. We introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of power and agency are implicitly projected on actors through their actions. We use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known Bechdel test. Our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames.", "year": 2017, "authors": [{"authorId": "2729164", "name": "Maarten Sap"}, {"authorId": "24851412", "name": "Marcella Cindy Prasettio"}, {"authorId": "14487640", "name": "Ari Holtzman"}, {"authorId": "2516777", "name": "Hannah Rashkin"}, {"authorId": "1699545", "name": "Yejin Choi"}], "cluster": 11, "position": {"x": 124.35980224609375, "y": -38.64555358886719}}, {"paperId": "15130cdd46ff7e3a69384e6c62ab1775c4aba96f", "url": "https://www.semanticscholar.org/paper/15130cdd46ff7e3a69384e6c62ab1775c4aba96f", "title": "Analyzing Framing through the Casts of Characters in the News", "abstract": "We present an unsupervised model for the discovery and clustering of latent \u201cpersonas\u201d (characterizations of entities). Our model simultaneously clusters documents featuring similar collections of personas. We evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation.", "year": 2016, "authors": [{"authorId": "35540755", "name": "D. Card"}, {"authorId": "2603663", "name": "J. Gross"}, {"authorId": "2641095", "name": "Amber E. Boydstun"}, {"authorId": "144365875", "name": "Noah A. Smith"}], "cluster": 11, "position": {"x": 118.09510803222656, "y": -21.883319854736328}}, {"paperId": "9129399d6b1599508709e3a72f80defd52ef89c9", "url": "https://www.semanticscholar.org/paper/9129399d6b1599508709e3a72f80defd52ef89c9", "title": "The online competition between pro- and anti-vaccination views", "abstract": "Distrust in scientific expertise 1 \u2013 14 is dangerous. Opposition to vaccination with a future vaccine against SARS-CoV-2, the causal agent of COVID-19, for example, could amplify outbreaks 2 \u2013 4 , as happened for measles in 2019 5 , 6 . Homemade remedies 7 , 8 and falsehoods are being shared widely on the Internet, as well as dismissals of expert advice 9 \u2013 11 . There is a lack of understanding about how this distrust evolves at the system level 13 , 14 . Here we provide a map of the contention surrounding vaccines that has emerged from the global pool of around three billion Facebook users. Its core reveals a multi-sided landscape of unprecedented intricacy that involves nearly 100\u00a0million individuals partitioned into highly dynamic, interconnected clusters across cities, countries, continents and languages. Although smaller in overall size, anti-vaccination clusters manage to become highly entangled with undecided clusters in the main online network, whereas pro-vaccination clusters are more peripheral. Our theoretical framework reproduces the recent explosive growth in anti-vaccination views, and predicts that these views will dominate in a decade. Insights provided by this framework can inform new policies and approaches to interrupt this shift to negative views. Our results challenge the conventional thinking about undecided individuals in issues of contention surrounding health, shed light on other issues of contention such as climate change 11 , and highlight the key role of network cluster dynamics in multi-species ecologies 15 . Insights into the interactions between pro- and anti-vaccination clusters on Facebook can enable policies and approaches that attempt to interrupt the shift to anti-vaccination views and persuade undecided individuals to adopt a pro-vaccination stance.", "year": 2020, "authors": [{"authorId": "2069626187", "name": "N. Johnson"}, {"authorId": "143618115", "name": "N. Vel\u00e1squez"}, {"authorId": "1695773203", "name": "N. J. Restrepo"}, {"authorId": "152152429", "name": "R. Leahy"}, {"authorId": "1720776022", "name": "N. Gabriel"}, {"authorId": "1695503420", "name": "Sara El Oud"}, {"authorId": "37987273", "name": "M. Zheng"}, {"authorId": "1971592", "name": "P. Manrique"}, {"authorId": "3160568", "name": "S. Wuchty"}, {"authorId": "51169219", "name": "Y. Lupu"}], "cluster": 13, "position": {"x": 26.09749984741211, "y": -11.576904296875}}, {"paperId": "1778d1da6264ed4274fb94115ebbc3b4f34c9f7a", "url": "https://www.semanticscholar.org/paper/1778d1da6264ed4274fb94115ebbc3b4f34c9f7a", "title": "Misinfo Belief Frames: A Case Study on Covid & Climate News", "abstract": "Prior beliefs of readers impact the way in which they project meaning onto news headlines. These beliefs can influence their perception of news reliability, as well as their reaction to news, and their likelihood of spreading the misinformation through social networks. However, most prior work focuses on fact-checking veracity of news or stylometry rather than measuring impact of misinformation. We propose Misinfo Belief Frames, a formalism for understanding how readers perceive the reliability of news and the impact of misinformation. We also introduce the Misinfo Belief Frames (MBF) corpus, a dataset of 66k inferences over 23.5k headlines. Misinformation frames use commonsense reasoning to uncover implications of real and fake news headlines focused on global crises: the Covid-19 pandemic and climate change. Our results using large-scale language modeling to predict misinformation frames show that machine-generated inferences can influence readers\u2019 trust in news headlines (readers\u2019 trust in news headlines was affected in 29.3% of cases). This demonstrates the potential effectiveness of using generated frames to counter misinformation.", "year": 2021, "authors": [{"authorId": "119902504", "name": "Saadia Gabriel"}, {"authorId": "1474550731", "name": "Skyler Hallinan"}, {"authorId": "2729164", "name": "Maarten Sap"}, {"authorId": "2078508715", "name": "Pemi Nguyen"}, {"authorId": "3268360", "name": "Franziska Roesner"}, {"authorId": "2890423", "name": "Eunsol Choi"}, {"authorId": "2111165011", "name": "Yejin Choi"}], "cluster": 15, "position": {"x": 59.766597747802734, "y": 0.49576786160469055}}, {"paperId": "ed1c17451a23471afde91c109ecadc6aab8b2ba6", "url": "https://www.semanticscholar.org/paper/ed1c17451a23471afde91c109ecadc6aab8b2ba6", "title": "A Survey on Multimodal Disinformation Detection", "abstract": "Recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. As a result, researchers started targeting different modalities and combinations thereof. As different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation \u2013 (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions.", "year": 2021, "authors": [{"authorId": "37784060", "name": "Firoj Alam"}, {"authorId": "40598011", "name": "S. Cresci"}, {"authorId": "144054829", "name": "Tanmoy Chakraborty"}, {"authorId": "144925193", "name": "F. Silvestri"}, {"authorId": "2757957", "name": "D. Dimitrov"}, {"authorId": "34086979", "name": "Giovanni Da San Martino"}, {"authorId": "65877664", "name": "Shaden Shaar"}, {"authorId": "22593971", "name": "Hamed Firooz"}, {"authorId": "2026545715", "name": "Preslav Nakov"}], "cluster": 7, "position": {"x": 83.6489486694336, "y": 108.41419982910156}}, {"paperId": "460986c12b9fdedc3bf9a710c429c5a84608055a", "url": "https://www.semanticscholar.org/paper/460986c12b9fdedc3bf9a710c429c5a84608055a", "title": "Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective and a Call to Arms", "abstract": "With the outbreak of the COVID-19 pandemic, people turned to social media to read and to share timely information including statistics, warnings, advice, and inspirational stories. Unfortunately, alongside all this useful information, there was also a new blending of medical and political misinformation and disinformation, which gave rise to the first global infodemic. While fighting this infodemic is typically thought of in terms of factuality, the problem is much broader as malicious content includes not only fake news, rumors, and conspiracy theories, but also promotion of fake cures, panic, racism, xenophobia, and mistrust in the authorities, among others. This is a complex problem that needs a holistic approach combining the perspectives of journalists, fact-checkers, policymakers, government entities, social media platforms, and society as a whole. Taking them into account we define an annotation schema and detailed annotation instructions, which reflect these perspectives. We performed initial annotations using this schema, and our initial experiments demonstrated sizable improvements over the baselines. Now, we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts.", "year": 2021, "authors": [{"authorId": "37784060", "name": "Firoj Alam"}, {"authorId": "6415321", "name": "Fahim Dalvi"}, {"authorId": "65877664", "name": "Shaden Shaar"}, {"authorId": "145938140", "name": "Nadir Durrani"}, {"authorId": "143779235", "name": "Hamdy Mubarak"}, {"authorId": "47195288", "name": "Alex Nikolov"}, {"authorId": "34086979", "name": "Giovanni Da San Martino"}, {"authorId": "1683403", "name": "Ahmed Abdelali"}, {"authorId": "50433033", "name": "Hassan Sajjad"}, {"authorId": "143758717", "name": "Kareem Darwish"}, {"authorId": "1683562", "name": "Preslav Nakov"}], "cluster": 15, "position": {"x": 48.26222229003906, "y": -4.465073108673096}}, {"paperId": "38d243b9f6e2c786699dbc83513fb190372cde07", "url": "https://www.semanticscholar.org/paper/38d243b9f6e2c786699dbc83513fb190372cde07", "title": "Automated Fact-Checking for Assisting Human Fact-Checkers", "abstract": "The reporting and analysis of current events around the globe has expanded from professional, editorlead journalism all the way to citizen journalism. Politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. However, the multiple advantages of free speech and direct communication are dimmed by the misuse of the media to spread inaccurate or misleading claims. These phenomena have led to the modern incarnation of the fact-checker \u2014 a professional whose main aim is to examine claims using available evidence to assess their veracity. As in other text forensics tasks, the amount of information available makes the work of the fact-checker more difficult. With this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. These include identifying claims worth fact-checking; detecting relevant previously fact-checked claims; retrieving relevant evidence to fact-check a claim; and actually verifying a claim. In each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.", "year": 2021, "authors": [{"authorId": "2026545715", "name": "Preslav Nakov"}, {"authorId": "1801487", "name": "D. Corney"}, {"authorId": "2905745", "name": "Maram Hasanain"}, {"authorId": "37784060", "name": "Firoj Alam"}, {"authorId": "1693370300", "name": "Tamer Elsayed"}, {"authorId": "2063950160", "name": "A. Barr'on-Cedeno"}, {"authorId": "1802817", "name": "Paolo Papotti"}, {"authorId": "65877664", "name": "Shaden Shaar"}, {"authorId": "34086979", "name": "Giovanni Da San Martino"}], "cluster": 7, "position": {"x": 54.74955368041992, "y": 83.21550750732422}}, {"paperId": "29370adbf0674ad72d5e0bcc59582c22035bd0aa", "url": "https://www.semanticscholar.org/paper/29370adbf0674ad72d5e0bcc59582c22035bd0aa", "title": "Coronavirus on Social Media: Analyzing Misinformation in Twitter Conversations", "abstract": "The ongoing Coronavirus Disease (COVID-19) pandemic highlights the interconnected-ness of our present-day globalized world. With social distancing policies in place, virtual communication has become an important source of (mis)information. As increasing number of people rely on social media platforms for news, identifying misinformation has emerged as a critical task in these unprecedented times. In addition to being malicious, the spread of such information poses a serious public health risk. To this end, we design a dashboard to track misinformation on popular social media news sharing platform - Twitter. Our dashboard allows visibility into the social media discussions around Coronavirus and the quality of information shared on the platform as the situation evolves. We collect streaming data using the Twitter API from March 1, 2020 to date and provide analysis of topic clusters and social sentiments related to important emerging policies such as \"#socialdistancing\" and \"#workfromhome\". We track emerging hashtags over time, and provide location and time sensitive analysis of sentiments. In addition, we study the challenging problem of misinformation on social media, and provide a detection method to identify false, misleading and clickbait contents from Twitter information cascades. The dashboard maintains an evolving list of detected misinformation cascades with the corresponding detection scores, accessible online athttps://ksharmar.this http URL.", "year": 2020, "authors": [{"authorId": "2082949", "name": "Karishma Sharma"}, {"authorId": "145260557", "name": "Sungyong Seo"}, {"authorId": "27737939", "name": "Chuizheng Meng"}, {"authorId": "2267664", "name": "Sirisha Rambhatla"}, {"authorId": "146055386", "name": "Aastha Dua"}, {"authorId": "1947397", "name": "Y. Liu"}], "cluster": 15, "position": {"x": 53.22694396972656, "y": -6.533105850219727}}, {"paperId": "76de89ca66f898e8211acba7392ef2d4a7e14125", "url": "https://www.semanticscholar.org/paper/76de89ca66f898e8211acba7392ef2d4a7e14125", "title": "Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection", "abstract": "Fake news has altered society in negative ways in politics and culture. It has adversely affected both online social network systems as well as offline communities and conversations. Using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. However, a lack of effective, comprehensive datasets has been a problem for fake news research and detection model development. Prior fake news datasets do not provide multimodal text and image data, metadata, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. We present Fakeddit, a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. We construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of multimodality and fine-grained classification unique to Fakeddit.", "year": 2020, "authors": [{"authorId": "30347964", "name": "Kai Nakamura"}, {"authorId": "49285370", "name": "Sharon Levy"}, {"authorId": "1682479", "name": "William Yang Wang"}], "cluster": 7, "position": {"x": 76.31981658935547, "y": 110.22539520263672}}, {"paperId": "418b9f37b07a05b38798e97f57e17a6cc1048b92", "url": "https://www.semanticscholar.org/paper/418b9f37b07a05b38798e97f57e17a6cc1048b92", "title": "You are right. I am ALARMED - But by Climate Change Counter Movement", "abstract": "The world is facing the challenge of climate crisis. Despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP. Despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. We try to bridge this gap by scraping and releasing articles with known climate change misinformation.", "year": 2020, "authors": [{"authorId": "1984990", "name": "Shraey Bhatia"}, {"authorId": "1800564", "name": "Jey Han Lau"}, {"authorId": "145465286", "name": "Timothy Baldwin"}], "cluster": 1, "position": {"x": 22.172849655151367, "y": -99.19750213623047}}, {"paperId": "1dad69f1fd4403aed4d3d709ab794113291d625c", "url": "https://www.semanticscholar.org/paper/1dad69f1fd4403aed4d3d709ab794113291d625c", "title": "Fake News: Spread of Misinformation about Urological Conditions on Social Media.", "abstract": "Although there is a large amount of user-generated content about urological health issues on social media, much of this content has not been vetted for information accuracy. In this article, we review the literature on the quality and balance of information on urological health conditions on social networks. Across a wide range of benign and malignant urological conditions, studies show a substantial amount of commercial, biased and/or inaccurate information present on popular social networking sites. The healthcare community should take proactive steps to improve the quality of medical information on social networks. PATIENT SUMMARY: In this review, we examined the spread of misinformation about urological health conditions on social media. We found that a significant amount of the circulating information is commercial, biased or misinformative.", "year": 2019, "authors": [{"authorId": "2235946", "name": "S. Loeb"}, {"authorId": "49187173", "name": "Jacob Taylor"}, {"authorId": "2989346", "name": "J. Borin"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}, {"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}, {"authorId": "40893848", "name": "N. Byrne"}, {"authorId": "32806077", "name": "A. Chiang"}, {"authorId": "38657671", "name": "A. Langford"}], "cluster": 15, "position": {"x": 50.275238037109375, "y": -15.24006175994873}}, {"paperId": "1213d98f3d0a42d575bbd2c2af0309c7f76024a1", "url": "https://www.semanticscholar.org/paper/1213d98f3d0a42d575bbd2c2af0309c7f76024a1", "title": "Combating Fake News: A Survey on Identification and Mitigation Techniques", "abstract": "The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news, and mitigation of its widespread impact on public opinion. While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. In this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. We discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. In addition, research has often been limited by the quality of existing datasets and their specific application contexts. To alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. Furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.", "year": 2019, "authors": [{"authorId": "2082949", "name": "Karishma Sharma"}, {"authorId": "2053324591", "name": "Feng Qian"}, {"authorId": "2116480752", "name": "He Jiang"}, {"authorId": "2820106", "name": "Natali Ruchansky"}, {"authorId": "47474380", "name": "Ming Zhang"}, {"authorId": "46400027", "name": "Y. Liu"}], "cluster": 7, "position": {"x": 72.70661926269531, "y": 96.29442596435547}}, {"paperId": "8415274c8fb370cbab84ad82ab2f469786ddee72", "url": "https://www.semanticscholar.org/paper/8415274c8fb370cbab84ad82ab2f469786ddee72", "title": "Weaponized Health Communication: Twitter Bots and Russian Trolls Amplify the Vaccine Debate", "abstract": "Objectives To understand how Twitter bots and trolls (\u201cbots\u201d) promote online health content. Methods We compared bots\u2019 to average users\u2019 rates of vaccine-relevant messages, which we collected online from July 2014 through September 2017. We estimated the likelihood that users were bots, comparing proportions of polarized and antivaccine tweets across user types. We conducted a content analysis of a Twitter hashtag associated with Russian troll activity. Results Compared with average users, Russian trolls (\u03c72(1)\u2009=\u2009102.0; P\u2009<\u2009.001), sophisticated bots (\u03c72(1)\u2009=\u200928.6; P\u2009<\u2009.001), and \u201ccontent polluters\u201d (\u03c72(1)\u2009=\u20097.0; P\u2009<\u2009.001) tweeted about vaccination at higher rates. Whereas content polluters posted more antivaccine content (\u03c72(1)\u2009=\u200911.18; P\u2009<\u2009.001), Russian trolls amplified both sides. Unidentifiable accounts were more polarized (\u03c72(1)\u2009=\u200912.1; P\u2009<\u2009.001) and antivaccine (\u03c72(1)\u2009=\u200935.9; P\u2009<\u2009.001). Analysis of the Russian troll hashtag showed that its messages were more political and divisive. Conclusions Whereas bots that spread malware and unsolicited content disseminated antivaccine messages, Russian trolls promoted discord. Accounts masquerading as legitimate users create false equivalency, eroding public consensus on vaccination. Public Health Implications. Directly confronting vaccine skeptics enables bots to legitimize the vaccine debate. More research is needed to determine how best to combat bot-driven content.", "year": 2018, "authors": [{"authorId": "3119101", "name": "David A. Broniatowski"}, {"authorId": "26759530", "name": "A. Jamison"}, {"authorId": "2053993324", "name": "SiHua Qi"}, {"authorId": "32335850", "name": "Lulwah Alkulaib"}, {"authorId": "144799987", "name": "Tao Chen"}, {"authorId": "145583569", "name": "Adrian Benton"}, {"authorId": "144673084", "name": "S. Quinn"}, {"authorId": "1782853", "name": "Mark Dredze"}], "cluster": 13, "position": {"x": 27.476150512695312, "y": -5.7868146896362305}}, {"paperId": "bc8c416f821b93795370524247c8a455c373ee6e", "url": "https://www.semanticscholar.org/paper/bc8c416f821b93795370524247c8a455c373ee6e", "title": "Fake News: A Survey of Research, Detection Methods, and Opportunities", "abstract": "The explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news analysis, detection and intervention. This survey comprehensively and systematically reviews fake news research. The survey identifies and specifies fundamental theories across various disciplines, e.g., psychology and social science, to facilitate and enhance the interdisciplinary research of fake news. Current fake news research is reviewed, summarized and evaluated. These studies focus on fake news from four perspective: (1) the false knowledge it carries, (2) its writing style, (3) its propagation patterns, and (4) the credibility of its creators and spreaders. We characterize each perspective with various analyzable and utilizable information provided by news and its spreaders, various strategies and frameworks that are adaptable, and techniques that are applicable. By reviewing the characteristics of fake news and open issues in fake news studies, we highlight some potential research tasks at the end of this survey.", "year": 2018, "authors": [{"authorId": "47155134", "name": "Xinyi Zhou"}, {"authorId": "2281410", "name": "R. Zafarani"}], "cluster": 7, "position": {"x": 70.9822769165039, "y": 89.31475830078125}}, {"paperId": "1e3d1055fc65966bc2c64505785207d7e8d71022", "url": "https://www.semanticscholar.org/paper/1e3d1055fc65966bc2c64505785207d7e8d71022", "title": "Rumor Cascades", "abstract": "Online social networks provide a rich substrate for rumor propagation. Information received via friends tends to be trusted, and online social networks allow individuals to transmit information to many friends at once. By referencing known rumors from Snopes.com, a popular website documenting memes and urban legends, we track the propagation of thousands of rumors appearing on Facebook. From this sample we infer the rates at which rumors from different categories and of varying truth value are uploaded and reshared. We find that rumor cascades run deeper in the social network than reshare cascades in general. We then examine the effect of individual reshares receiving a comment containing a link to a Snopes article on the evolution of the cascade. We find that receiving such a comment increases the likelihood that a reshare of a rumor will be deleted. Furthermore, large cascades are able to accumulate hundreds of Snopes comments while continuing to propagate. Finally, using a dataset of rumors copied and pasted from one status update to another, we show that rumors change over time and that different variants tend to dominate different bursts in popularity.", "year": 2014, "authors": [{"authorId": "34760887", "name": "A. Friggeri"}, {"authorId": "1778398", "name": "Lada A. Adamic"}, {"authorId": "1996878", "name": "Dean Eckles"}, {"authorId": "144500753", "name": "Justin Cheng"}], "cluster": 14, "position": {"x": 75.97649383544922, "y": 17.790348052978516}}, {"paperId": "a1e58f89f57f57fad3c77cd558444ad5ad64b525", "url": "https://www.semanticscholar.org/paper/a1e58f89f57f57fad3c77cd558444ad5ad64b525", "title": "The spread of true and false news online", "abstract": "Lies spread faster than the truth There is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. To understand how false news spreads, Vosoughi et al. used a data set of rumor cascades on Twitter from 2006 to 2017. About 126,000 rumors were spread by \u223c3 million people. False news reached more people than the truth; the top 1% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. Falsehood also diffused faster than the truth. The degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. Science, this issue p. 1146 A large-scale analysis of tweets reveals that false rumors spread further and faster than the truth. We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98% agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.", "year": 2018, "authors": [{"authorId": "1918441", "name": "Soroush Vosoughi"}, {"authorId": "145364504", "name": "D. Roy"}, {"authorId": "2413779", "name": "Sinan Aral"}], "cluster": 14, "position": {"x": 68.38113403320312, "y": 20.010986328125}}, {"paperId": "c6a9ca56c93323c0199dd22631d1cf731bdd7ec1", "url": "https://www.semanticscholar.org/paper/c6a9ca56c93323c0199dd22631d1cf731bdd7ec1", "title": "Automatic Detection of Fake News", "abstract": "The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors, and show that we can achieve accuracies of up to 76%. In addition, we provide comparative analyses of the automatic and manual identification of fake news.", "year": 2018, "authors": [{"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}, {"authorId": "6032930", "name": "Bennett Kleinberg"}, {"authorId": "32809241", "name": "Alexandra Lefevre"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}], "cluster": 7, "position": {"x": 68.45520782470703, "y": 104.75091552734375}}, {"paperId": "6f78b5608fed43f106da192f12e09d9edbd2fce0", "url": "https://www.semanticscholar.org/paper/6f78b5608fed43f106da192f12e09d9edbd2fce0", "title": "Social Media and Fake News in the 2016 Election", "abstract": "Following the 2016 U.S. presidential election, many have expressed concern about the effects of false stories (\u201cfake news\u201d), circulated largely through social media. We discuss the economics of fake news and present new data on its consumption prior to the election. Drawing on web browsing data, archives of fact-checking websites, and results from a new online survey, we find: (i) social media was an important but not dominant source of election news, with 14 percent of Americans calling social media their \u201cmost important\u201d source; (ii) of the known false news stories that appeared in the three months before the election, those favoring Trump were shared a total of 30 million times on Facebook, while those favoring Clinton were shared 8 million times; (iii) the average American adult saw on the order of one or perhaps several fake news stories in the months around the election, with just over half of those who recalled seeing them believing them; and (iv) people are much more likely to believe stories that favor their preferred candidate, especially if they have ideologically segregated social media networks.", "year": 2017, "authors": [{"authorId": "6112526", "name": "H. Allcott"}, {"authorId": "8731644", "name": "Matthew Gentzkow"}], "cluster": 14, "position": {"x": 68.41339111328125, "y": 27.39612579345703}}, {"paperId": "7d3c2ff37d04914836f9cbd9ce54b6c97aa74a22", "url": "https://www.semanticscholar.org/paper/7d3c2ff37d04914836f9cbd9ce54b6c97aa74a22", "title": "Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking", "abstract": "We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.", "year": 2017, "authors": [{"authorId": "2516777", "name": "Hannah Rashkin"}, {"authorId": "2890423", "name": "Eunsol Choi"}, {"authorId": "1835709", "name": "J. Jang"}, {"authorId": "143683394", "name": "Svitlana Volkova"}, {"authorId": "1699545", "name": "Yejin Choi"}], "cluster": 7, "position": {"x": 63.46194839477539, "y": 86.56668090820312}}, {"paperId": "03c294ad75bd1bac92217419ac25358227f6a901", "url": "https://www.semanticscholar.org/paper/03c294ad75bd1bac92217419ac25358227f6a901", "title": "\"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection", "abstract": "Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present liar: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.", "year": 2017, "authors": [{"authorId": "1682479", "name": "William Yang Wang"}], "cluster": 7, "position": {"x": 63.3216552734375, "y": 108.11666870117188}}, {"paperId": "9aefd614e52336151966d8dca2ed0ea62a8f30af", "url": "https://www.semanticscholar.org/paper/9aefd614e52336151966d8dca2ed0ea62a8f30af", "title": "The Limitations of Stylometry for Detecting Machine-Generated Fake News", "abstract": "Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.", "year": 2020, "authors": [{"authorId": "32303439", "name": "Tal Schuster"}, {"authorId": "39347554", "name": "R. Schuster"}, {"authorId": "143896588", "name": "Darsh J. Shah"}, {"authorId": "1741283", "name": "R. Barzilay"}], "cluster": 7, "position": {"x": 56.72988510131836, "y": 124.20216369628906}}, {"paperId": "44ecb9783e43d726fc87b7ffdd580b01e13a85cb", "url": "https://www.semanticscholar.org/paper/44ecb9783e43d726fc87b7ffdd580b01e13a85cb", "title": "Rumors, False Flags, and Digital Vigilantes: Misinformation on Twitter after the 2013 Boston Marathon Bombing", "abstract": "The Boston Marathon bombing story unfolded on every possible carrier of information available in the spring of 2013, including Twitter. As information spread, it was filled with rumors (unsubstantiated information), and many of these rumors contained misinformation. Earlier studies have suggested that crowdsourced information flows can correct misinformation, and our research investigates this proposition. This exploratory research examines three rumors, later demonstrated to be false, that circulated on Twitter in the aftermath of the bombings. Our findings suggest that corrections to the misinformation emerge but are muted compared with the propagation of the misinformation. The similarities and differences we observe in the patterns of the misinformation and corrections contained within the stream over the days that followed the attacks suggest directions for possible research strategies to automatically detect misinformation.", "year": 2014, "authors": [{"authorId": "3181624", "name": "Kate Starbird"}, {"authorId": "3330847", "name": "Jim Maddock"}, {"authorId": "2751519", "name": "M. Orand"}, {"authorId": "72318911", "name": "Peg Achterman"}, {"authorId": "35236169", "name": "R. Mason"}], "cluster": 14, "position": {"x": 66.56043243408203, "y": 14.366697311401367}}, {"paperId": "d8cb11d4be955f9869387a18967dee366eb851d9", "url": "https://www.semanticscholar.org/paper/d8cb11d4be955f9869387a18967dee366eb851d9", "title": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims", "abstract": "We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.", "year": 2019, "authors": [{"authorId": "1736067", "name": "Isabelle Augenstein"}, {"authorId": "1784800", "name": "C. Lioma"}, {"authorId": "2111243103", "name": "Dongsheng Wang"}, {"authorId": "145980035", "name": "Lucas Chaves Lima"}, {"authorId": "144613166", "name": "Casper Hansen"}, {"authorId": "144613163", "name": "Christian Hansen"}, {"authorId": "1707651", "name": "J. Simonsen"}], "cluster": 7, "position": {"x": 34.2087287902832, "y": 82.84903717041016}}, {"paperId": "69116800a8a8195531d29c8e14cefb1c92cbb8a7", "url": "https://www.semanticscholar.org/paper/69116800a8a8195531d29c8e14cefb1c92cbb8a7", "title": "Extractive and Abstractive Explanations for Fact-Checking and Evaluation of News", "abstract": "In this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. We experiment with two methods: (1) an extractive method based on Biased TextRank \u2013 a resource-effective unsupervised graph-based algorithm for content extraction; and (2) an abstractive method based on the GPT-2 language model. We perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise.", "year": 2021, "authors": [{"authorId": "1721683964", "name": "Ashkan Kazemi"}, {"authorId": "2109964406", "name": "Zehua Li"}, {"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}], "cluster": 7, "position": {"x": 39.22578048706055, "y": 63.25115966796875}}, {"paperId": "20b2f18aaf10a9221c5edf3720d4cce7da672104", "url": "https://www.semanticscholar.org/paper/20b2f18aaf10a9221c5edf3720d4cce7da672104", "title": "That is a Known Lie: Detecting Previously Fact-Checked Claims", "abstract": "The recent proliferation of \u201dfake news\u201d has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.", "year": 2020, "authors": [{"authorId": "65877664", "name": "Shaden Shaar"}, {"authorId": "34086979", "name": "Giovanni Da San Martino"}, {"authorId": "1693976811", "name": "Nikolay Babulkov"}, {"authorId": "1683562", "name": "Preslav Nakov"}], "cluster": 7, "position": {"x": 49.91106414794922, "y": 102.78744506835938}}, {"paperId": "3a16c38294ac8899825c488490199b854e05473a", "url": "https://www.semanticscholar.org/paper/3a16c38294ac8899825c488490199b854e05473a", "title": "Evaluating adversarial attacks against multiple fact verification systems", "abstract": "Automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. Due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. We introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. We consider six fact verification systems from the recent Fact Extraction and VERification (FEVER) challenge: the four best-scoring ones and two baselines. We evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. We find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.", "year": 2019, "authors": [{"authorId": "2053211210", "name": "James Thorne"}, {"authorId": "2064056928", "name": "Andreas Vlachos"}, {"authorId": "2718039", "name": "Christos Christodoulopoulos"}, {"authorId": "2008596", "name": "Arpit Mittal"}], "cluster": 7, "position": {"x": 29.52999496459961, "y": 95.4556884765625}}, {"paperId": "9e8ac8df8b46c36cad3f307f85975012479b5a32", "url": "https://www.semanticscholar.org/paper/9e8ac8df8b46c36cad3f307f85975012479b5a32", "title": "Fact or Fiction: Verifying Scientific Claims", "abstract": "We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that supports or refutes a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that these models benefit from combined training on a large dataset of claims about Wikipedia articles, together with the new SciFact data. We show that our claim verification system is able to identify plausible evidence for 23 / 36 claims relevant to COVID-19 on the CORD-19 corpus. Our results and experiments strongly suggest that our new task and data will support significant future research efforts.", "year": 2020, "authors": [{"authorId": "30051202", "name": "David Wadden"}, {"authorId": "46258841", "name": "Kyle Lo"}, {"authorId": "31860505", "name": "Lucy Lu Wang"}, {"authorId": "32370203", "name": "Shanchuan Lin"}, {"authorId": "15292561", "name": "Madeleine van Zuylen"}, {"authorId": "2527954", "name": "Arman Cohan"}, {"authorId": "2548384", "name": "Hannaneh Hajishirzi"}], "cluster": 7, "position": {"x": 22.616626739501953, "y": 76.59632110595703}}, {"paperId": "6f373f4711e1285bdec23069c9503d3bf77bfaef", "url": "https://www.semanticscholar.org/paper/6f373f4711e1285bdec23069c9503d3bf77bfaef", "title": "A Benchmark Dataset of Check-worthy Factual Claims", "abstract": "In this paper we present the ClaimBuster dataset of 23,533 statements extracted from all U.S. general election presidential debates and annotated by human coders. The ClaimBuster dataset can be leveraged in building computational methods to identify claims that are worth fact-checking from the myriad of sources of digital or traditional media. The ClaimBuster dataset is publicly available to the research community, and it can be found at this http URL.", "year": 2020, "authors": [{"authorId": "144285061", "name": "Fatma Arslan"}, {"authorId": "2789540", "name": "Naeemul Hassan"}, {"authorId": "2128664093", "name": "Chengkai Li"}, {"authorId": "2819331", "name": "M. Tremayne"}], "cluster": 7, "position": {"x": 41.3870964050293, "y": 84.76487731933594}}, {"paperId": "5e0daaeceb75ffbbe23be13d34ffae830cb4e8c4", "url": "https://www.semanticscholar.org/paper/5e0daaeceb75ffbbe23be13d34ffae830cb4e8c4", "title": "Generating Fact Checking Explanations", "abstract": "Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.", "year": 2020, "authors": [{"authorId": "145676297", "name": "Pepa Atanasova"}, {"authorId": "1707651", "name": "J. Simonsen"}, {"authorId": "1784800", "name": "C. Lioma"}, {"authorId": "1736067", "name": "Isabelle Augenstein"}], "cluster": 7, "position": {"x": 40.252479553222656, "y": 70.99545288085938}}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "url": "https://www.semanticscholar.org/paper/ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News", "abstract": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.", "year": 2019, "authors": [{"authorId": "2545335", "name": "Rowan Zellers"}, {"authorId": "14487640", "name": "Ari Holtzman"}, {"authorId": "2516777", "name": "Hannah Rashkin"}, {"authorId": "3312309", "name": "Yonatan Bisk"}, {"authorId": "143787583", "name": "Ali Farhadi"}, {"authorId": "3268360", "name": "Franziska Roesner"}, {"authorId": "1699545", "name": "Yejin Choi"}], "cluster": 7, "position": {"x": 61.350807189941406, "y": 127.44141387939453}}, {"paperId": "a4947468ceac25c11a665c4f5f95a49d6dbea3cc", "url": "https://www.semanticscholar.org/paper/a4947468ceac25c11a665c4f5f95a49d6dbea3cc", "title": "Towards Debiasing Fact Verification Models", "abstract": "Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.", "year": 2019, "authors": [{"authorId": "32303439", "name": "Tal Schuster"}, {"authorId": "143896588", "name": "Darsh J. Shah"}, {"authorId": "153466660", "name": "Yun Jie Serene Yeo"}, {"authorId": "67260776", "name": "Daniel Filizzola"}, {"authorId": "150991377", "name": "Enrico Santus"}, {"authorId": "1741283", "name": "R. Barzilay"}], "cluster": 7, "position": {"x": 18.43981170654297, "y": 85.35454559326172}}, {"paperId": "b1d24e8e08435b7c52335485a0d635abf9bc604c", "url": "https://www.semanticscholar.org/paper/b1d24e8e08435b7c52335485a0d635abf9bc604c", "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification", "abstract": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.", "year": 2018, "authors": [{"authorId": "144603330", "name": "James Thorne"}, {"authorId": "2775151", "name": "Andreas Vlachos"}, {"authorId": "2718039", "name": "Christos Christodoulopoulos"}, {"authorId": "2008596", "name": "Arpit Mittal"}], "cluster": 7, "position": {"x": 28.98950958251953, "y": 85.1680679321289}}]